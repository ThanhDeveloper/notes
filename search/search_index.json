{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The repository for sharing the knowledge of computer science","title":"Home"},{"location":"algorithms/articulation_points_and_bridges/","text":"Articulation Points and Bridges 1. Articulation Point In a graph, a vertex is called an articulation point if removing it and all the edges associated with it results in the increase of the number of connected components in the graph Algorithm Steps: Pick an arbitrary vertex of the graph root and run DFS from it. For each node maintain two time values: disc : The time in which the node was first reached low : The low time of a node is the lowest discovery time of all of its adjacent nodes A node is an articulation point if satisfy any of the following properties: If node is root node and node has 2 children If node's low time is lower than the low time of all other adjacent nodes (using lows ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 int n ; // number of nodes vector < vector < int >> adj ; // adjacency list of graph vector < bool > visited ; vector < int > disc , low ; int timer ; void dfs ( int v , int p = -1 ) { visited [ v ] = true ; disc [ v ] = low [ v ] = timer ++ ; int children = 0 ; for ( int to : adj [ v ]) { if ( to == p ) { continue ; } if ( visited [ to ]) { low [ v ] = min ( low [ v ], disc [ to ]); } else { dfs ( to , v ); low [ v ] = min ( low [ v ], low [ to ]); if ( low [ to ] >= disc [ v ] && p != -1 ) { IS_CUTPOINT ( v ); } ++ children ; } } if ( p == -1 && children > 1 ) { IS_CUTPOINT ( v ); } } void find_cutpoints () { timer = 0 ; visited . assign ( n , false ); disc . assign ( n , -1 ); low . assign ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs ( i ); } } } The time complexity of the algorithm is O(E + V). 2. Bridge An edge in a graph between vertices u and v is called a Bridge, if after removing it, there will be no path left between u and v. Algorithm Steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 int n ; // number of nodes vector < vector < int >> adj ; // adjacency list of graph vector < bool > visited ; vector < int > disc , low ; int timer ; void dfs ( int v , int p = -1 ) { visited [ v ] = true ; disc [ v ] = low [ v ] = timer ++ ; for ( int to : adj [ v ]) { if ( to == p ) { continue ; } if ( visited [ to ]) { low [ v ] = min ( low [ v ], disc [ to ]); } else { dfs ( to , v ); low [ v ] = min ( low [ v ], low [ to ]); if ( low [ to ] > disc [ v ]) { IS_BRIDGE ( v , to ); } } } } void find_bridges () { timer = 0 ; visited . assign ( n , false ); disc . assign ( n , -1 ); low . assign ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs ( i ); } } }","title":"Articulation Points and Bridges"},{"location":"algorithms/articulation_points_and_bridges/#articulation-points-and-bridges","text":"","title":"Articulation Points and Bridges"},{"location":"algorithms/articulation_points_and_bridges/#1-articulation-point","text":"In a graph, a vertex is called an articulation point if removing it and all the edges associated with it results in the increase of the number of connected components in the graph Algorithm Steps: Pick an arbitrary vertex of the graph root and run DFS from it. For each node maintain two time values: disc : The time in which the node was first reached low : The low time of a node is the lowest discovery time of all of its adjacent nodes A node is an articulation point if satisfy any of the following properties: If node is root node and node has 2 children If node's low time is lower than the low time of all other adjacent nodes (using lows ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 int n ; // number of nodes vector < vector < int >> adj ; // adjacency list of graph vector < bool > visited ; vector < int > disc , low ; int timer ; void dfs ( int v , int p = -1 ) { visited [ v ] = true ; disc [ v ] = low [ v ] = timer ++ ; int children = 0 ; for ( int to : adj [ v ]) { if ( to == p ) { continue ; } if ( visited [ to ]) { low [ v ] = min ( low [ v ], disc [ to ]); } else { dfs ( to , v ); low [ v ] = min ( low [ v ], low [ to ]); if ( low [ to ] >= disc [ v ] && p != -1 ) { IS_CUTPOINT ( v ); } ++ children ; } } if ( p == -1 && children > 1 ) { IS_CUTPOINT ( v ); } } void find_cutpoints () { timer = 0 ; visited . assign ( n , false ); disc . assign ( n , -1 ); low . assign ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs ( i ); } } } The time complexity of the algorithm is O(E + V).","title":"1. Articulation Point"},{"location":"algorithms/articulation_points_and_bridges/#2-bridge","text":"An edge in a graph between vertices u and v is called a Bridge, if after removing it, there will be no path left between u and v. Algorithm Steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 int n ; // number of nodes vector < vector < int >> adj ; // adjacency list of graph vector < bool > visited ; vector < int > disc , low ; int timer ; void dfs ( int v , int p = -1 ) { visited [ v ] = true ; disc [ v ] = low [ v ] = timer ++ ; for ( int to : adj [ v ]) { if ( to == p ) { continue ; } if ( visited [ to ]) { low [ v ] = min ( low [ v ], disc [ to ]); } else { dfs ( to , v ); low [ v ] = min ( low [ v ], low [ to ]); if ( low [ to ] > disc [ v ]) { IS_BRIDGE ( v , to ); } } } } void find_bridges () { timer = 0 ; visited . assign ( n , false ); disc . assign ( n , -1 ); low . assign ( n , -1 ); for ( int i = 0 ; i < n ; ++ i ) { if ( ! visited [ i ]) { dfs ( i ); } } }","title":"2. Bridge"},{"location":"algorithms/binary_search/","text":"Binary search Binary Search is a searching algorithm for finding an element's position in a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. Algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int binarySearch ( std :: vector < int > array , int x ) { int low = 0 ; int high = array . size () - 1 ; while ( low <= high ) { int mid = ( low + high ) / 2 ; if ( array [ mid ] == x ) { return mid ; } else if ( array [ mid ] < x ) { low = mid + 1 ; } else { high = mid - 1 ; } } // Not found return -1 ; } Examples Find First and Last Position of Element in Sorted Array Problem Approach In this problem, we'll search for x as we are searching normally. When we find the element, then there are 2 solutions: To find the first position, we keep searching for element in the left part of the array To find the last position, we keep searching for element in the right part of the array 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 int findPosition ( std :: vector < int > & nums , int target , bool findFirst ) { int leftIdx = 0 ; int rightIdx = nums . size () - 1 ; int position = NOT_FOUND ; while ( leftIdx <= rightIdx ) { int midIdx = ( leftIdx + rightIdx ) / 2 ; if ( nums [ midIdx ] == target ) { position = midIdx ; if ( findFirst ) { // Continue to find in the left part rightIdx = midIdx - 1 ; } else { // Continue to find in the right part leftIdx = midIdx + 1 ; } } else if ( nums [ midIdx ] > target ) { rightIdx = midIdx - 1 ; } else { leftIdx = midIdx + 1 ; } } return position ; } Find floor/ ceil in a sorted array floor(arr, x): Return the largest integer in array arr that is smaller or equal to x Problem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int findFloor ( vector < int > arr , int target ) { int lowArrIdx = 0 ; int highArrIdx = arr . size () - 1 ; int floorIdx = -1 ; while ( lowArrIdx <= highArrIdx ) { int midArrIdx = ( lowArrIdx + highArrIdx ) / 2 ; if ( arr [ midArrIdx ] <= target ) { floorIdx = midArrIdx ; lowArrIdx = midArrIdx + 1 ; } else { // arr[midArrIdx] > target highArrIdx = midArrIdx - 1 ; } } return floorIdx ; } ceil(arr, x): Return the smallest integer in array arr that is greater or equal to x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int findCeil ( vector < int > arr , int target ) { int lowArrIdx = 0 ; int highArrIdx = arr . size () - 1 ; int ceilIdx = -1 ; while ( lowArrIdx <= highArrIdx ) { int midArrIdx = ( lowArrIdx + highArrIdx ) / 2 ; if ( arr [ midArrIdx ] >= target ) { ceilIdx = midArrIdx ; highArrIdx = midArrIdx - 1 ; } else { // arr[midArrIdx] < target lowArrIdx = midArrIdx + 1 ; } } return ceilIdx ; } Finding the peak of an array Problem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 int findPeakElement ( vector < int >& nums ) { int leftNumsIdx = 0 ; int rightNumsIdx = nums . size () - 1 ; while ( leftNumsIdx <= rightNumsIdx ) { int midNumsIdx = ( leftNumsIdx + rightNumsIdx ) / 2 ; if ( midNumsIdx == 0 || nums [ midNumsIdx - 1 ] < nums [ midNumsIdx ]) { if ( midNumsIdx == nums . size () - 1 || nums [ midNumsIdx + 1 ] < nums [ midNumsIdx ]) { return midNumsIdx ; } // We can find answer in the right part // We have a contrain nums[n] = -INF leftNumsIdx = midNumsIdx + 1 ; } else { // nums[midNumsIdx - 1] > nums[midNumsIdx] rightNumsIdx = midNumsIdx - 1 ; } } // Not reachable return -1 ; }","title":"Binary search"},{"location":"algorithms/binary_search/#binary-search","text":"Binary Search is a searching algorithm for finding an element's position in a sorted array. Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. Algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int binarySearch ( std :: vector < int > array , int x ) { int low = 0 ; int high = array . size () - 1 ; while ( low <= high ) { int mid = ( low + high ) / 2 ; if ( array [ mid ] == x ) { return mid ; } else if ( array [ mid ] < x ) { low = mid + 1 ; } else { high = mid - 1 ; } } // Not found return -1 ; }","title":"Binary search"},{"location":"algorithms/binary_search/#examples","text":"","title":"Examples"},{"location":"algorithms/binary_search/#find-first-and-last-position-of-element-in-sorted-array","text":"Problem Approach In this problem, we'll search for x as we are searching normally. When we find the element, then there are 2 solutions: To find the first position, we keep searching for element in the left part of the array To find the last position, we keep searching for element in the right part of the array 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 int findPosition ( std :: vector < int > & nums , int target , bool findFirst ) { int leftIdx = 0 ; int rightIdx = nums . size () - 1 ; int position = NOT_FOUND ; while ( leftIdx <= rightIdx ) { int midIdx = ( leftIdx + rightIdx ) / 2 ; if ( nums [ midIdx ] == target ) { position = midIdx ; if ( findFirst ) { // Continue to find in the left part rightIdx = midIdx - 1 ; } else { // Continue to find in the right part leftIdx = midIdx + 1 ; } } else if ( nums [ midIdx ] > target ) { rightIdx = midIdx - 1 ; } else { leftIdx = midIdx + 1 ; } } return position ; }","title":"Find First and Last Position of Element in Sorted Array"},{"location":"algorithms/binary_search/#find-floor-ceil-in-a-sorted-array","text":"floor(arr, x): Return the largest integer in array arr that is smaller or equal to x Problem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int findFloor ( vector < int > arr , int target ) { int lowArrIdx = 0 ; int highArrIdx = arr . size () - 1 ; int floorIdx = -1 ; while ( lowArrIdx <= highArrIdx ) { int midArrIdx = ( lowArrIdx + highArrIdx ) / 2 ; if ( arr [ midArrIdx ] <= target ) { floorIdx = midArrIdx ; lowArrIdx = midArrIdx + 1 ; } else { // arr[midArrIdx] > target highArrIdx = midArrIdx - 1 ; } } return floorIdx ; } ceil(arr, x): Return the smallest integer in array arr that is greater or equal to x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int findCeil ( vector < int > arr , int target ) { int lowArrIdx = 0 ; int highArrIdx = arr . size () - 1 ; int ceilIdx = -1 ; while ( lowArrIdx <= highArrIdx ) { int midArrIdx = ( lowArrIdx + highArrIdx ) / 2 ; if ( arr [ midArrIdx ] >= target ) { ceilIdx = midArrIdx ; highArrIdx = midArrIdx - 1 ; } else { // arr[midArrIdx] < target lowArrIdx = midArrIdx + 1 ; } } return ceilIdx ; }","title":"Find floor/ ceil in a sorted array"},{"location":"algorithms/binary_search/#finding-the-peak-of-an-array","text":"Problem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 int findPeakElement ( vector < int >& nums ) { int leftNumsIdx = 0 ; int rightNumsIdx = nums . size () - 1 ; while ( leftNumsIdx <= rightNumsIdx ) { int midNumsIdx = ( leftNumsIdx + rightNumsIdx ) / 2 ; if ( midNumsIdx == 0 || nums [ midNumsIdx - 1 ] < nums [ midNumsIdx ]) { if ( midNumsIdx == nums . size () - 1 || nums [ midNumsIdx + 1 ] < nums [ midNumsIdx ]) { return midNumsIdx ; } // We can find answer in the right part // We have a contrain nums[n] = -INF leftNumsIdx = midNumsIdx + 1 ; } else { // nums[midNumsIdx - 1] > nums[midNumsIdx] rightNumsIdx = midNumsIdx - 1 ; } } // Not reachable return -1 ; }","title":"Finding the peak of an array"},{"location":"algorithms/bloom_filter/","text":"Bloom filter What is Bloom filter A Bloom filter is a data structure designed to tell you, rapidly and memory-efficiently, whether an element is present in a set. The price paid for this efficiency is that a Bloom filter is a probabilistic data structure: it tells us that the element either definitely is not in the set or may be in the set. For example is x in the list? Yes (90% correct) No (100% correct) How does a Bloom filter work? An empty Bloom filter is a Bit Vector with all bits set to zero. In the image below, each cell represents a bit. The number below the bit is its index for a 10-bit vector. In order to add an element, it must be hashed using multiple hash functions. Bits are set at the index of the hashes in the bit vector. For example, let\u2019s assume we need to add the james@gmail.com element using three efficient hash functions: H1(james@gmail.com) = 12021 H2(james@gmail.com) = 23324 H3(james@gmail.com) = 23237 We can take the mod of 10 for all these values to get an index within the bounds of the bit vector: 12021 % 10 = 1 23324 % 10 = 4 23237 % 10 = 7 For an item whose membership needs to be tested, it is also hashed via the same hash functions. If all the bits are already set for this, the element may exist in the set. If any bit is not set, the element is definitely not in the set. Why Bloom filters give false positive results? Let\u2019s assume we have added the two members below to the bloom filter. Monkey with Hash Output H(\u201cMonkey\u201d) = {1,2,5} Lion with Hash Output H(\u201cLion\u201d) = {7,4,3} Now, if we want to check whether or not Tiger exists in the set, we can hash it via the same hash functions. H(\u201cTiger\u201d) = {2,7,3} We have not added \u201cTiger\u201d to the bloom filter, but all the bits at index {2,7,3} have already been set by the previous two elements; thus, Bloom Filter claims that \u201cTiger\u201d is present in the set. This is a false positive result. Bloom filter applications Medium uses Bloom filters in its Recommendation module to avoid showing those posts that have already been seen by the user. Cassandra uses bloom filters to optimize the search of data in an SSTable on the disk. CDNs use bloom filters to avoid caching items that are rarely searched.","title":"Bloom filter"},{"location":"algorithms/bloom_filter/#bloom-filter","text":"","title":"Bloom filter"},{"location":"algorithms/bloom_filter/#what-is-bloom-filter","text":"A Bloom filter is a data structure designed to tell you, rapidly and memory-efficiently, whether an element is present in a set. The price paid for this efficiency is that a Bloom filter is a probabilistic data structure: it tells us that the element either definitely is not in the set or may be in the set. For example is x in the list? Yes (90% correct) No (100% correct)","title":"What is Bloom filter"},{"location":"algorithms/bloom_filter/#how-does-a-bloom-filter-work","text":"An empty Bloom filter is a Bit Vector with all bits set to zero. In the image below, each cell represents a bit. The number below the bit is its index for a 10-bit vector. In order to add an element, it must be hashed using multiple hash functions. Bits are set at the index of the hashes in the bit vector. For example, let\u2019s assume we need to add the james@gmail.com element using three efficient hash functions: H1(james@gmail.com) = 12021 H2(james@gmail.com) = 23324 H3(james@gmail.com) = 23237 We can take the mod of 10 for all these values to get an index within the bounds of the bit vector: 12021 % 10 = 1 23324 % 10 = 4 23237 % 10 = 7 For an item whose membership needs to be tested, it is also hashed via the same hash functions. If all the bits are already set for this, the element may exist in the set. If any bit is not set, the element is definitely not in the set.","title":"How does a Bloom filter work?"},{"location":"algorithms/bloom_filter/#why-bloom-filters-give-false-positive-results","text":"Let\u2019s assume we have added the two members below to the bloom filter. Monkey with Hash Output H(\u201cMonkey\u201d) = {1,2,5} Lion with Hash Output H(\u201cLion\u201d) = {7,4,3} Now, if we want to check whether or not Tiger exists in the set, we can hash it via the same hash functions. H(\u201cTiger\u201d) = {2,7,3} We have not added \u201cTiger\u201d to the bloom filter, but all the bits at index {2,7,3} have already been set by the previous two elements; thus, Bloom Filter claims that \u201cTiger\u201d is present in the set. This is a false positive result.","title":"Why Bloom filters give false positive results?"},{"location":"algorithms/bloom_filter/#bloom-filter-applications","text":"Medium uses Bloom filters in its Recommendation module to avoid showing those posts that have already been seen by the user. Cassandra uses bloom filters to optimize the search of data in an SSTable on the disk. CDNs use bloom filters to avoid caching items that are rarely searched.","title":"Bloom filter applications"},{"location":"algorithms/bucket_sort/","text":"Bucket sort Bucket sort is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using different sorting algorithm. Bucket sort is mainly useful when input is uniformly distributed over a range. 1. How does bucket sort work? 2. Pseudocode Step 1: Create n empty buckets Step 2: Place every element in their corresponding bucket Step 3: Sort individual buckets (Using: quick sort or insertion sort) Step 4: Merge all the bucket 3. Example Sort a large set of floating point numbers which are in range from 0.0 to 1.0 and are uniformly distributed across the range. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 void bucketSort ( std :: vector < float >& arr , int n ) { // Create n empty buckets std :: vector < std :: vector < float >> buckets ; buckets . resize ( n ); // Put array elements in different buckets for ( int i = 0 ; i < arr . size (); ++ i ) { int bucketIndex = arr [ i ] * n ; // the value is in range from 0.0 to 1.0 buckets [ i ]. push_back ( arr [ i ]); } // Sort individual buckets for ( int i = 0 ; i < n ; ++ i ) { std :: sort ( buckets [ i ]. begin (), buckets [ i ]. end ()); } // Concatenate all buckets into arr[] int index = 0 ; for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < buckets [ i ]. size (); ++ j ) { arr [ index ++ ] = bucket [ i ][ j ]; } } } Bucket sort for numbers having integer part: Step 1: Find maximum and minimum elements of the array Step 2: Calculate the range of each bucket 1 auto range = ( max - min ) / n ; // n is the number of buckets Step 3: Create n buckets of calculated range Step 4: Scatter the array elements to these buckets 1 auto bucketIndex = ( arr [ i ] - min ) / range ; Step 5: Sort each bucket individually Step 6: Gather the sorted elements from buckets to original array 4. Time complexity Time Complexity Best O(n + k) Worst O(n 2 ) Average O(n) Space Complexity O(n + k) Where k is the number of buckets","title":"Bucket sort"},{"location":"algorithms/bucket_sort/#bucket-sort","text":"Bucket sort is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using different sorting algorithm. Bucket sort is mainly useful when input is uniformly distributed over a range.","title":"Bucket sort"},{"location":"algorithms/bucket_sort/#1-how-does-bucket-sort-work","text":"","title":"1. How does bucket sort work?"},{"location":"algorithms/bucket_sort/#2-pseudocode","text":"Step 1: Create n empty buckets Step 2: Place every element in their corresponding bucket Step 3: Sort individual buckets (Using: quick sort or insertion sort) Step 4: Merge all the bucket","title":"2. Pseudocode"},{"location":"algorithms/bucket_sort/#3-example","text":"","title":"3. Example"},{"location":"algorithms/bucket_sort/#sort-a-large-set-of-floating-point-numbers-which-are-in-range-from-00-to-10-and-are-uniformly-distributed-across-the-range","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 void bucketSort ( std :: vector < float >& arr , int n ) { // Create n empty buckets std :: vector < std :: vector < float >> buckets ; buckets . resize ( n ); // Put array elements in different buckets for ( int i = 0 ; i < arr . size (); ++ i ) { int bucketIndex = arr [ i ] * n ; // the value is in range from 0.0 to 1.0 buckets [ i ]. push_back ( arr [ i ]); } // Sort individual buckets for ( int i = 0 ; i < n ; ++ i ) { std :: sort ( buckets [ i ]. begin (), buckets [ i ]. end ()); } // Concatenate all buckets into arr[] int index = 0 ; for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < buckets [ i ]. size (); ++ j ) { arr [ index ++ ] = bucket [ i ][ j ]; } } }","title":"Sort a large set of floating point numbers which are in range from 0.0 to 1.0 and are uniformly distributed across the range."},{"location":"algorithms/bucket_sort/#bucket-sort-for-numbers-having-integer-part","text":"Step 1: Find maximum and minimum elements of the array Step 2: Calculate the range of each bucket 1 auto range = ( max - min ) / n ; // n is the number of buckets Step 3: Create n buckets of calculated range Step 4: Scatter the array elements to these buckets 1 auto bucketIndex = ( arr [ i ] - min ) / range ; Step 5: Sort each bucket individually Step 6: Gather the sorted elements from buckets to original array","title":"Bucket sort for numbers having integer part:"},{"location":"algorithms/bucket_sort/#4-time-complexity","text":"Time Complexity Best O(n + k) Worst O(n 2 ) Average O(n) Space Complexity O(n + k) Where k is the number of buckets","title":"4. Time complexity"},{"location":"algorithms/huffman_coding/","text":"Huffman coding Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters. 1. How huffman coding works Suppose the following stringis to be sent over a network: BCAADDDCCACACAC Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding is done with the help of the following steps. Step 1 : Calculate the frequency of each character in the string. B C A D 1 6 5 3 Step 2 : Create a leaf node for each unique character and sort all nodes in increasing order of the frequency. We can store these nodes in a min heap. B D A C 1 3 5 6 1 [B - 1] [D - 3] [A - 5] [C - 6] Step 3 : Extract 2 nodes with the minimum frequency from the min heap, create a new internode with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. * A C 4 5 6 1 2 3 [* - 4] / \\ [B - 1] [D - 3] [A - 5] [C - 6] Step 4 : Repeat step 2 and step 3 util the min heap contains only one node. The remaining node is the root node and the tree is complete. C * 6 9 1 2 3 4 5 [* - 9] / \\ [* - 4] \\ / \\ \\ [B - 1] [D - 3] [A - 5] [C - 6] * 15 1 2 3 4 5 6 7 [* - 15] / \\ [C - 6] [* - 9] / \\ [* - 4] \\ / \\ \\ [B - 1] [D - 3] [A - 5] Step 5 : For each non-leaf node, assign 0 to the left edge and 1 to the right edge 1 2 3 4 5 6 7 8 9 10 11 12 13 [* - 15] / \\ 0 1 / \\ [C - 6] [* - 9] / \\ 0 \\ / \\ [* - 4] 1 / \\ \\ 0 1 \\ / \\ \\ [B - 1] [D - 3] [A - 5] For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below. Character Frequency Code Size A 5 11 5 * 2 = 10 B 1 100 1 * 3 = 3 C 6 0 6 *1 = 6 D 3 101 3 * 3 = 9 4 * 8 = 32 bits 15 bits 28 bits Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75 . 2. Decoding the code For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. 1 2 3 4 5 6 7 8 9 10 11 12 13 [* - 15] / \\ 0 (1) / \\ [C - 6] [* - 9] / \\ (0) \\ / \\ [* - 4] 1 / \\ \\ 0 (1) \\ / \\ \\ [B - 1] [D - 3] [A - 5] 101 ~ D 3. Huffman Coding Complexity The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n).","title":"Huffman coding"},{"location":"algorithms/huffman_coding/#huffman-coding","text":"Huffman Coding is a technique of compressing data to reduce its size without losing any of the details. Huffman Coding is generally useful to compress the data in which there are frequently occurring characters.","title":"Huffman coding"},{"location":"algorithms/huffman_coding/#1-how-huffman-coding-works","text":"Suppose the following stringis to be sent over a network: BCAADDDCCACACAC Each character occupies 8 bits. There are a total of 15 characters in the above string. Thus, a total of 8 * 15 = 120 bits are required to send this string. Using the Huffman Coding technique, we can compress the string to a smaller size. Huffman coding is done with the help of the following steps. Step 1 : Calculate the frequency of each character in the string. B C A D 1 6 5 3 Step 2 : Create a leaf node for each unique character and sort all nodes in increasing order of the frequency. We can store these nodes in a min heap. B D A C 1 3 5 6 1 [B - 1] [D - 3] [A - 5] [C - 6] Step 3 : Extract 2 nodes with the minimum frequency from the min heap, create a new internode with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap. * A C 4 5 6 1 2 3 [* - 4] / \\ [B - 1] [D - 3] [A - 5] [C - 6] Step 4 : Repeat step 2 and step 3 util the min heap contains only one node. The remaining node is the root node and the tree is complete. C * 6 9 1 2 3 4 5 [* - 9] / \\ [* - 4] \\ / \\ \\ [B - 1] [D - 3] [A - 5] [C - 6] * 15 1 2 3 4 5 6 7 [* - 15] / \\ [C - 6] [* - 9] / \\ [* - 4] \\ / \\ \\ [B - 1] [D - 3] [A - 5] Step 5 : For each non-leaf node, assign 0 to the left edge and 1 to the right edge 1 2 3 4 5 6 7 8 9 10 11 12 13 [* - 15] / \\ 0 1 / \\ [C - 6] [* - 9] / \\ 0 \\ / \\ [* - 4] 1 / \\ \\ 0 1 \\ / \\ \\ [B - 1] [D - 3] [A - 5] For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below. Character Frequency Code Size A 5 11 5 * 2 = 10 B 1 100 1 * 3 = 3 C 6 0 6 *1 = 6 D 3 101 3 * 3 = 9 4 * 8 = 32 bits 15 bits 28 bits Without encoding, the total size of the string was 120 bits. After encoding the size is reduced to 32 + 15 + 28 = 75 .","title":"1. How huffman coding works"},{"location":"algorithms/huffman_coding/#2-decoding-the-code","text":"For decoding the code, we can take the code and traverse through the tree to find the character. Let 101 is to be decoded, we can traverse from the root as in the figure below. 1 2 3 4 5 6 7 8 9 10 11 12 13 [* - 15] / \\ 0 (1) / \\ [C - 6] [* - 9] / \\ (0) \\ / \\ [* - 4] 1 / \\ \\ 0 (1) \\ / \\ \\ [B - 1] [D - 3] [A - 5] 101 ~ D","title":"2. Decoding the code"},{"location":"algorithms/huffman_coding/#3-huffman-coding-complexity","text":"The time complexity for encoding each unique character based on its frequency is O(nlog n). Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n).","title":"3. Huffman Coding Complexity"},{"location":"algorithms/kmp/","text":"Knuth\u2013Morris\u2013Pratt (KMP) KMP (Knuth\u2013Morris\u2013Pratt algorithm) is a nice algorithm for solving some string matching problems (match a pattern in a text). Naive solution A naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again. Example: One naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again. But we can do it better by using KMP algorithm. KMP As you can see in the above image, there is a mismatch at index 3. According to naive approach next step would be to shift W by 1. Since all letters in W are different, we can actually shift W by the index where mismatch occurred (3 in this case). We can say for sure there won\u2019t be any match in between. Let take another example: In the above image the green cells in the left substring is equal to the green cells in the right substring. It is actually the largest prefix which is also equal to the suffix of the substring till index 4 of the word \u201cdeadeye\u201d Now let\u2019s see how it works by taking an abstract example: str1 = str2 (green cells) and str2 = str3. When there is a mismatch after str2, we can directly shift the word till after str1 as you can see in the image. Green cells actually tell us the index from where it should start comparing next, if there is a mismatch. Calculate LPS (Longest Proper Prefix which is also Suffix) table Prefix table (also known as LPS/ Longest Prefix Suffix) is an array data structure which captures the longest prefix which is also a suffix for every substring starting at index 0. Source code Example code to compute LPS table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 std :: vector < int > computeLPS ( std :: string pat ) { int patSize = pat . size (); std :: vector < int > lsp ( patSize , 0 ); int left = 0 ; int right = 1 ; lsp [ 0 ] = 0 ; while ( right < patSize ) { if ( patt [ left ] == pat [ right ]) { lsp [ right ] = left + 1 ; ++ left ; ++ right ; } else { if ( left != 0 ) { left = lsp [ left - 1 ]; } else { lsp [ right ] = 0 ; ++ right ; } } } } Example code to do KMS search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 int kmpSearch ( std :: string str , std :: string pat ) { auto strSize = str . size (); auto patSize = pat . size (); auto lsp = computeLPS ( pat ); int strIdx = 0 ; int patIdx = 0 ; while ( strIdx < strSize ) { if ( str [ strIdx ] == pat [ patIdx ]) { ++ strIdx ; ++ patIdx ; } else { if ( patIdx != 0 ) { patIdx = lsp [ patIdx - 1 ]; } else { ++ strIdx ; } } if ( patIdx == patSize - 1 ) { return strIdx - patIdx ; } } return -1 ; } Time/space complexity Time complexity: O(n + m) Space complexity: O(m) where: n is the size of the string m is the size of the pattern","title":"KMP"},{"location":"algorithms/kmp/#knuthmorrispratt-kmp","text":"KMP (Knuth\u2013Morris\u2013Pratt algorithm) is a nice algorithm for solving some string matching problems (match a pattern in a text).","title":"Knuth\u2013Morris\u2013Pratt (KMP)"},{"location":"algorithms/kmp/#naive-solution","text":"A naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again. Example: One naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again. But we can do it better by using KMP algorithm.","title":"Naive solution"},{"location":"algorithms/kmp/#kmp","text":"As you can see in the above image, there is a mismatch at index 3. According to naive approach next step would be to shift W by 1. Since all letters in W are different, we can actually shift W by the index where mismatch occurred (3 in this case). We can say for sure there won\u2019t be any match in between. Let take another example: In the above image the green cells in the left substring is equal to the green cells in the right substring. It is actually the largest prefix which is also equal to the suffix of the substring till index 4 of the word \u201cdeadeye\u201d Now let\u2019s see how it works by taking an abstract example: str1 = str2 (green cells) and str2 = str3. When there is a mismatch after str2, we can directly shift the word till after str1 as you can see in the image. Green cells actually tell us the index from where it should start comparing next, if there is a mismatch.","title":"KMP"},{"location":"algorithms/kmp/#calculate-lps-longest-proper-prefix-which-is-also-suffix-table","text":"Prefix table (also known as LPS/ Longest Prefix Suffix) is an array data structure which captures the longest prefix which is also a suffix for every substring starting at index 0.","title":"Calculate LPS (Longest Proper Prefix which is also Suffix) table"},{"location":"algorithms/kmp/#source-code","text":"Example code to compute LPS table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 std :: vector < int > computeLPS ( std :: string pat ) { int patSize = pat . size (); std :: vector < int > lsp ( patSize , 0 ); int left = 0 ; int right = 1 ; lsp [ 0 ] = 0 ; while ( right < patSize ) { if ( patt [ left ] == pat [ right ]) { lsp [ right ] = left + 1 ; ++ left ; ++ right ; } else { if ( left != 0 ) { left = lsp [ left - 1 ]; } else { lsp [ right ] = 0 ; ++ right ; } } } } Example code to do KMS search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 int kmpSearch ( std :: string str , std :: string pat ) { auto strSize = str . size (); auto patSize = pat . size (); auto lsp = computeLPS ( pat ); int strIdx = 0 ; int patIdx = 0 ; while ( strIdx < strSize ) { if ( str [ strIdx ] == pat [ patIdx ]) { ++ strIdx ; ++ patIdx ; } else { if ( patIdx != 0 ) { patIdx = lsp [ patIdx - 1 ]; } else { ++ strIdx ; } } if ( patIdx == patSize - 1 ) { return strIdx - patIdx ; } } return -1 ; }","title":"Source code"},{"location":"algorithms/kmp/#timespace-complexity","text":"Time complexity: O(n + m) Space complexity: O(m) where: n is the size of the string m is the size of the pattern","title":"Time/space complexity"},{"location":"algorithms/minimum_spanning_tree/","text":"Minimum Spanning Tree 1. What is a Spanning Tree? In an undirected and connected graph G=(V,E), a spanning tree is a subgraph that is a tree which includes all of the vertices of G, with minimum possible number of edges. A graph may have several spanning trees. The cost of the spanning tree is the sum of the weights of all the edges in the tree 2. What is a Minimum Spanning Tree? A minimum spanning tree (M- ST) is the spanning tree where the cost is minimum among all the spanning trees. 3. Prim\u2019s Algorithm Prim\u2019s algorithm is a greedy algorithm that works well on dense graphs . It finds a minimum spanning tree for a weighted UNDIRECTED graph. Algorithm Steps: Choose any arbitrary node s as root node Enqueues all edges incident to s into a Priority Queue (PQ) Repeatedly do the following greedy steps until PQ is empty: If the vertex v with edge e (w -> v) in the PQ has not been visited then add e to MST and enqueue all edges connected to v into the PQ. Visualising 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 #include <iostream> #include <vector> #include <queue> std :: vector < std :: vector < std :: pair < int , int >>> adj ; std :: vector < bool > visited ; std :: priority_queue < std :: pair < int , int > , std :: vector < std :: pair < int , int >> , std :: greater < std :: pair < int , int >>> queue ; void addEdges ( int s ) { visited [ s ] = true ; for ( int i = 0 ; i < adj [ s ]. size (); ++ i ) { if ( visited [ adj [ s ][ i ]. second ]) { continue ; } queue . push ( adj [ s ][ i ]); } } int main () { int n , m , a , b , w ; std :: cin >> n >> m ; adj = std :: vector < std :: vector < std :: pair < int , int >>> ( n + 1 , std :: vector < std :: pair < int , int >> {}); visited = std :: vector < bool > ( n + 1 , false ); for ( int i = 0 ; i < m ; ++ i ) { std :: cin >> a >> b >> w ; adj [ a ]. push_back ( std :: make_pair ( w , b )); adj [ b ]. push_back ( std :: make_pair ( w , a )); } int edgeCount = 0 ; int mstCost = 0 ; addEdges ( 1 ); while ( ! queue . empty () && edgeCount != n - 1 ) { auto cost = queue . top (). first ; auto des = queue . top (). second ; queue . pop (); if ( visited [ des ]) { continue ; } mstCost += cost ; ++ edgeCount ; addEdges ( des ); } if ( edgeCount != n - 1 ) { // No MST found std :: cout << \"0\" ; } else { std :: cout << mstCost ; } } The time complexity of Prim's algorithm is O(E log V). 4. Kruskal\u2019s Algorithm Kruskal\u2019s algorithm is a greedy algorithm that works well on dense graphs . Algorithm Steps: Sort the set of edges E in increasing order Start adding edges to the MST from the edge with the smallest weight until the edge of the largest weight. Only add edges which doesn't form a cycle , edges which connect only disconnected components. So now the question is how to check if vertices are connected or not ? This could be done using DFS but DFS will make time complexity large. So the best solution is Disjoint Set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #include <iostream> #include <vector> #include <algorithm> class DisjointSet { private : std :: vector < int > parents ; public : DisjointSet ( int n ) { parents = std :: vector < int > ( n + 1 , -1 ); for ( int i = 1 ; i <= n ; ++ i ) { parents [ i ] = i ; } } int find ( int x ) { while ( parents [ x ] != x ) { parents [ x ] = parents [ parents [ x ]]; x = parents [ x ]; } return parents [ x ]; } void unionSet ( int x , int y ) { auto parentX = find ( x ); auto parentY = find ( y ); parents [ parentY ] = parentX ; } }; int main () { int n , m , a , b , w ; std :: cin >> n >> m ; std :: vector < std :: pair < int , std :: pair < int , int >>> edges ; for ( int i = 0 ; i < m ; ++ i ) { std :: cin >> a >> b >> w ; edges . push_back ( std :: make_pair ( w , std :: make_pair ( a , b ))); } DisjointSet disjointSet ( n ); std :: sort ( edges . begin (), edges . end ()); int mst = 0 ; for ( int i = 0 ; i < edges . size (); ++ i ) { auto cost = edges [ i ]. first ; auto s = edges [ i ]. second . first ; auto d = edges [ i ]. second . second ; if ( disjointSet . find ( s ) != disjointSet . find ( d )) { disjointSet . unionSet ( s , d ); mst += cost ; } } std :: cout << mst << std :: endl ; }","title":"Minimum spanning tree"},{"location":"algorithms/minimum_spanning_tree/#minimum-spanning-tree","text":"","title":"Minimum Spanning Tree"},{"location":"algorithms/minimum_spanning_tree/#1-what-is-a-spanning-tree","text":"In an undirected and connected graph G=(V,E), a spanning tree is a subgraph that is a tree which includes all of the vertices of G, with minimum possible number of edges. A graph may have several spanning trees. The cost of the spanning tree is the sum of the weights of all the edges in the tree","title":"1. What is a Spanning Tree?"},{"location":"algorithms/minimum_spanning_tree/#2-what-is-a-minimum-spanning-tree","text":"A minimum spanning tree (M- ST) is the spanning tree where the cost is minimum among all the spanning trees.","title":"2. What is a Minimum Spanning Tree?"},{"location":"algorithms/minimum_spanning_tree/#3-prims-algorithm","text":"Prim\u2019s algorithm is a greedy algorithm that works well on dense graphs . It finds a minimum spanning tree for a weighted UNDIRECTED graph. Algorithm Steps: Choose any arbitrary node s as root node Enqueues all edges incident to s into a Priority Queue (PQ) Repeatedly do the following greedy steps until PQ is empty: If the vertex v with edge e (w -> v) in the PQ has not been visited then add e to MST and enqueue all edges connected to v into the PQ. Visualising 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 #include <iostream> #include <vector> #include <queue> std :: vector < std :: vector < std :: pair < int , int >>> adj ; std :: vector < bool > visited ; std :: priority_queue < std :: pair < int , int > , std :: vector < std :: pair < int , int >> , std :: greater < std :: pair < int , int >>> queue ; void addEdges ( int s ) { visited [ s ] = true ; for ( int i = 0 ; i < adj [ s ]. size (); ++ i ) { if ( visited [ adj [ s ][ i ]. second ]) { continue ; } queue . push ( adj [ s ][ i ]); } } int main () { int n , m , a , b , w ; std :: cin >> n >> m ; adj = std :: vector < std :: vector < std :: pair < int , int >>> ( n + 1 , std :: vector < std :: pair < int , int >> {}); visited = std :: vector < bool > ( n + 1 , false ); for ( int i = 0 ; i < m ; ++ i ) { std :: cin >> a >> b >> w ; adj [ a ]. push_back ( std :: make_pair ( w , b )); adj [ b ]. push_back ( std :: make_pair ( w , a )); } int edgeCount = 0 ; int mstCost = 0 ; addEdges ( 1 ); while ( ! queue . empty () && edgeCount != n - 1 ) { auto cost = queue . top (). first ; auto des = queue . top (). second ; queue . pop (); if ( visited [ des ]) { continue ; } mstCost += cost ; ++ edgeCount ; addEdges ( des ); } if ( edgeCount != n - 1 ) { // No MST found std :: cout << \"0\" ; } else { std :: cout << mstCost ; } } The time complexity of Prim's algorithm is O(E log V).","title":"3. Prim\u2019s Algorithm"},{"location":"algorithms/minimum_spanning_tree/#4-kruskals-algorithm","text":"Kruskal\u2019s algorithm is a greedy algorithm that works well on dense graphs . Algorithm Steps: Sort the set of edges E in increasing order Start adding edges to the MST from the edge with the smallest weight until the edge of the largest weight. Only add edges which doesn't form a cycle , edges which connect only disconnected components. So now the question is how to check if vertices are connected or not ? This could be done using DFS but DFS will make time complexity large. So the best solution is Disjoint Set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #include <iostream> #include <vector> #include <algorithm> class DisjointSet { private : std :: vector < int > parents ; public : DisjointSet ( int n ) { parents = std :: vector < int > ( n + 1 , -1 ); for ( int i = 1 ; i <= n ; ++ i ) { parents [ i ] = i ; } } int find ( int x ) { while ( parents [ x ] != x ) { parents [ x ] = parents [ parents [ x ]]; x = parents [ x ]; } return parents [ x ]; } void unionSet ( int x , int y ) { auto parentX = find ( x ); auto parentY = find ( y ); parents [ parentY ] = parentX ; } }; int main () { int n , m , a , b , w ; std :: cin >> n >> m ; std :: vector < std :: pair < int , std :: pair < int , int >>> edges ; for ( int i = 0 ; i < m ; ++ i ) { std :: cin >> a >> b >> w ; edges . push_back ( std :: make_pair ( w , std :: make_pair ( a , b ))); } DisjointSet disjointSet ( n ); std :: sort ( edges . begin (), edges . end ()); int mst = 0 ; for ( int i = 0 ; i < edges . size (); ++ i ) { auto cost = edges [ i ]. first ; auto s = edges [ i ]. second . first ; auto d = edges [ i ]. second . second ; if ( disjointSet . find ( s ) != disjointSet . find ( d )) { disjointSet . unionSet ( s , d ); mst += cost ; } } std :: cout << mst << std :: endl ; }","title":"4. Kruskal\u2019s Algorithm"},{"location":"algorithms/monotonic_stack/","text":"Monotonic stack Stack is a very simple data structure. The logical sequence of first in and last out conforms to the some characteristics of some problems, such as function call stack. Monotonic stack is actually a stack. It just uses some ingenious logic to keep the elements in the stack orderly (monotone increasing or monotone decreasing) after each new element putting into the stack. Well,sounds like a heap? No, monotonic stack is not widely used. It only deals with one typical problem, which is called Next Greater Element If a problem is suitable to use monotonic stack, it must has at least three characters: It is a range queries in an array problem. The minima/maxima element or the monotonic order of elements in a range is useful to get answer of every range query. When a element is popped from the monotonic stack, it will never be used again Example Next greater element Give you an array,and return an array of equal length.The corresponding index stores the next larger element, if there is no larger element, store -1. It's not easy to explain clearly in words. Let's take a direct example: Give you an array [2,1,2,4,3],and you return an array [4,2,4,-1,-1]. This problem can be thought abstractly: think of the elements in the array as people standing side by side, and the size of the elements as the height of an adult. These people stand in line before you. How to find the Next Greater Number of element \"2\"? Very simply, if you can see the element \"2\", then the first person you can see behind him is the Next Greater Number of \"2\". Because the element smaller than \"2\" is not tall enough and it is blocked by \"2\",the first one not being blocked is the answer. This is a very understandable situation,huh? With this abstract scenario in mind, let's look at the code first. 1 2 3 4 5 6 7 8 9 10 11 12 vector < int > nextGreaterElement ( vector < int >& nums ) { vector < int > ans ( nums . size ()); // array to store answer stack < int > s ; for ( int i = nums . size () - 1 ; i >= 0 ; i -- ) { // put it into the stack back to front while ( ! s . empty () && s . top () <= nums [ i ]) { // determine by height s . pop (); // short one go away while blocked } ans [ i ] = s . empty () ? -1 : s . top (); // the first tall behind this element s . push ( nums [ i ]); // get into the queue and wait for later height determination } return ans ; }","title":"Monotonic stack"},{"location":"algorithms/monotonic_stack/#monotonic-stack","text":"Stack is a very simple data structure. The logical sequence of first in and last out conforms to the some characteristics of some problems, such as function call stack. Monotonic stack is actually a stack. It just uses some ingenious logic to keep the elements in the stack orderly (monotone increasing or monotone decreasing) after each new element putting into the stack. Well,sounds like a heap? No, monotonic stack is not widely used. It only deals with one typical problem, which is called Next Greater Element If a problem is suitable to use monotonic stack, it must has at least three characters: It is a range queries in an array problem. The minima/maxima element or the monotonic order of elements in a range is useful to get answer of every range query. When a element is popped from the monotonic stack, it will never be used again","title":"Monotonic stack"},{"location":"algorithms/monotonic_stack/#example","text":"","title":"Example"},{"location":"algorithms/monotonic_stack/#next-greater-element","text":"Give you an array,and return an array of equal length.The corresponding index stores the next larger element, if there is no larger element, store -1. It's not easy to explain clearly in words. Let's take a direct example: Give you an array [2,1,2,4,3],and you return an array [4,2,4,-1,-1]. This problem can be thought abstractly: think of the elements in the array as people standing side by side, and the size of the elements as the height of an adult. These people stand in line before you. How to find the Next Greater Number of element \"2\"? Very simply, if you can see the element \"2\", then the first person you can see behind him is the Next Greater Number of \"2\". Because the element smaller than \"2\" is not tall enough and it is blocked by \"2\",the first one not being blocked is the answer. This is a very understandable situation,huh? With this abstract scenario in mind, let's look at the code first. 1 2 3 4 5 6 7 8 9 10 11 12 vector < int > nextGreaterElement ( vector < int >& nums ) { vector < int > ans ( nums . size ()); // array to store answer stack < int > s ; for ( int i = nums . size () - 1 ; i >= 0 ; i -- ) { // put it into the stack back to front while ( ! s . empty () && s . top () <= nums [ i ]) { // determine by height s . pop (); // short one go away while blocked } ans [ i ] = s . empty () ? -1 : s . top (); // the first tall behind this element s . push ( nums [ i ]); // get into the queue and wait for later height determination } return ans ; }","title":"Next greater element"},{"location":"algorithms/rolling_hash/","text":"Rolling hash Hashing is used for efficient comparison of strings by converting them into integers and then comparing those strings on the basis of their integer values. Rolling hash is used to prevent rehashing the whole string while calculating hash values of the substrings of a given string. In rolling hash,the new hash value is rapidly calculated given only the old hash value.Using it, two strings can be compared in constant time. Formula In general,the hash H can be defined as:- H = (c 1 a k-1 + c 2 a k-2 + c 3 a k-3 +...+ c k a 0 ) % m where: a is a constant c 1 , c 2 , ... c k are the input characters m is a large prime number (eg: 1e9 + 7) The probability of two random strings colliding is about \u2248 1/m. The hash value of next substring H nxt using rolling hash can be defined as:- H nxt = ((H - c 1 a k-1 ) * a + c k+1 a 0 ) % m Example Consider the string abcd and we have to find the hash values of substrings of this string having length 3 ,i.e., abc and bcd . For simplicity let us take 5 as the base but in actual scenarios we should mod it with a large prime number to avoid overflow. So,the hash value of first substring,H(abc) :- 1 H(abc) => a*(5^2) + b*(5^1) + c*(5^0) = 97*25 + 98*5 + 99*1 = 3014 And the hash value of the second substring,H(bcd) :- 1 2 H(bcd) => b*(5^2) + c*(5^1) + d*(5^0) = 98*25 + 99*5 + 100*1 = 3045 H(bcd) =(H(abc)-a*(5^2))*5 + d*(5^0)=(3014-97*25)*5 + 100*1 = 3045 Time Complexity The rolling hash takes O(n) time complexity to find hash values of all substrings of a length k of a string of length n. Computing hash value for the first substring will take O(k) as we have to visit every character of the substring and then for each of the n-k characters we can find the hash in O(1) so the total time complexity would be O(k+n-k)","title":"Rolling hash"},{"location":"algorithms/rolling_hash/#rolling-hash","text":"Hashing is used for efficient comparison of strings by converting them into integers and then comparing those strings on the basis of their integer values. Rolling hash is used to prevent rehashing the whole string while calculating hash values of the substrings of a given string. In rolling hash,the new hash value is rapidly calculated given only the old hash value.Using it, two strings can be compared in constant time.","title":"Rolling hash"},{"location":"algorithms/rolling_hash/#formula","text":"In general,the hash H can be defined as:- H = (c 1 a k-1 + c 2 a k-2 + c 3 a k-3 +...+ c k a 0 ) % m where: a is a constant c 1 , c 2 , ... c k are the input characters m is a large prime number (eg: 1e9 + 7) The probability of two random strings colliding is about \u2248 1/m. The hash value of next substring H nxt using rolling hash can be defined as:- H nxt = ((H - c 1 a k-1 ) * a + c k+1 a 0 ) % m","title":"Formula"},{"location":"algorithms/rolling_hash/#example","text":"Consider the string abcd and we have to find the hash values of substrings of this string having length 3 ,i.e., abc and bcd . For simplicity let us take 5 as the base but in actual scenarios we should mod it with a large prime number to avoid overflow. So,the hash value of first substring,H(abc) :- 1 H(abc) => a*(5^2) + b*(5^1) + c*(5^0) = 97*25 + 98*5 + 99*1 = 3014 And the hash value of the second substring,H(bcd) :- 1 2 H(bcd) => b*(5^2) + c*(5^1) + d*(5^0) = 98*25 + 99*5 + 100*1 = 3045 H(bcd) =(H(abc)-a*(5^2))*5 + d*(5^0)=(3014-97*25)*5 + 100*1 = 3045","title":"Example"},{"location":"algorithms/rolling_hash/#time-complexity","text":"The rolling hash takes O(n) time complexity to find hash values of all substrings of a length k of a string of length n. Computing hash value for the first substring will take O(k) as we have to visit every character of the substring and then for each of the n-k characters we can find the hash in O(1) so the total time complexity would be O(k+n-k)","title":"Time Complexity"},{"location":"algorithms/shortest_path_algorithms/","text":"Shortest Path Algorithms The shortest path problem is about finding a path between 2 vertices in a graph such that the total sum of the edges weights is minimum. BFS Dijkstra Bellman Ford Floyd Warshall Complexity O(V+E) O(V + E log(V)) O(VE) O(V3) Recommended grapth size Large Large/Medium Medium/Small Small Good for APSP Only works on unweighted graphs Ok Bad Yes Can detect negative cycles No No Yes Yes SP on graph with weighted edges Incorrect SP answer Best algorithm Works Bad in general SP on graph with unweighted edges Best algorithm Ok Bad Bad in general 1. Bellman Ford's Algorithm Bellman Ford's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph. Algorithm Steps: Initialize an array D to keep track of the shortest path from s to all of the nodes Set every entry in D to +\u221e The outer loop traverses from: 0 -> n - 1 . Loop over all edges, check if the D[next] > D[current] + weight, in this case update D[next] = D[current] + weight. Repeat the step 4 to find nodes caught in a negative cycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 std :: vector < int > dist ( n , std :: numeric_limits < int >:: max ()); dist [ 0 ] = 0 ; for ( int i = 0 ; i < n - 1 ; ++ i ) { for ( int j = 0 ; j < m ; ++ j ){ auto from = edges [ j ][ 0 ]; auto to = edges [ j ][ 1 ]; auto w = edges [ j ][ 2 ]; if ( dist [ from ] != std :: numeric_limits < int >:: max () && dist [ from ] + w < dist [ to ]) { dist [ to ] = dist [ from ] + w ; } } } // Repeat to find nodes caught in a negative cycle for ( int i = 0 ; i < n - 1 ; ++ i ) { for ( int j = 0 ; j < m ; ++ j ){ auto from = edges [ j ][ 0 ]; auto to = edges [ j ][ 1 ]; auto w = edges [ j ][ 2 ]; if ( dist [ from ] != std :: numeric_limits < int >:: max () && dist [ from ] + w < dist [ to ]) { dist [ to ] = std :: numeric_limits < int >:: min (); } } } The time complexity of the algorithm is O(VE). 2. Dijkstra's Algorithm Dijkstra's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph. One contraint for Dijkstra's algorithm is that the graph must only contain non-negative edge weight . This contraint is imposed to ensure that once a node has been visited its optimal distance can not be improved. Algorithm Steps: Maintain a dist array where the distance to every node is positive infinity, mark the distance to the start node s to be 0 Maintain a PQ of key-value pairs of (node_idx, distance) pairs and insert (s, 0) in to the PQ Pull out the pair (node_idx, distance) from PQ Update the distance of the connected vertices to node in node_idx in case of current vertex distance + edge weight < next vertex distance , then push them to PQ If the popped vertex is visited before, just continue without using it. Apply the same algorithm again until the PQ is empty. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 std :: vector < int > dist ( n , std :: numeric_limits < int >:: max ()); std :: vector < bool > visited ( n , false ); std :: priority_queue < std :: pair < int , int > , std :: vector < std :: pair < int , int >> , std :: greater < std :: pair < int , int >>> queue ; dist [ 0 ] = 0 ; queue . push ( std :: make_pair ( 0 , 0 )); while ( ! queue . empty ()) { auto w = queue . top (). first ; auto node_idx = queue . top (). second ; queue . pop (); if ( visited [ node_idx ]) { continue ; } visited [ node_idx ] = true ; if ( dist [ node_idx ] < w ) { continue ; } for ( int i = 0 ; i < adj [ node_idx ]. size (); ++ i ) { auto to = adj [ node_idx ][ i ]. first ; auto cost = adj [ node_idx ][ i ]. second ; if ( dist [ node_idx ] + cost < dist [ to ]) { dist [ to ] = dist [ node_idx ] + cost ; queue . push ( std :: make_pair ( dist [ to ], to )); } } } The time complexity of the algorithm is O(V + E log(V)). 3. Floyd Warshall's Algorithm Floyd Warshall's Algorithm is used to find the shortest paths between between all pairs of vertices in a graph, where each edge in the graph has a weight which is positive or negative. Algorithm Steps: Initialize the shortest paths between any 2 vertices with Infinity. Find all pair shortest paths that use 0 intermediate vertices, then find the shortest paths that use 1 intermediate vertex and so on.. until using all V vertices as intermediate nodes. Minimize the shortest paths between any 2 pairs in the previous operation. For any 2 vertices (i,j), one should actually minimize the distances between this pair using the first K nodes, so the shortest path will be: min(dist[i][k]+dist[k][j], dist[i][j]) Repeat to find nodes caught in a negative cycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for ( int k = 1 ; k <= n ; k ++ ){ for ( int i = 1 ; i <= n ; i ++ ){ for ( int j = 1 ; j <= n ; j ++ ){ dist [ i ][ j ] = min ( dist [ i ][ j ], dist [ i ][ k ] + dist [ k ][ j ]); } } } // Detect negative cycle for ( int k = 1 ; k <= n ; k ++ ){ for ( int i = 1 ; i <= n ; i ++ ){ for ( int j = 1 ; j <= n ; j ++ ){ dist [ i ][ j ] = std :: numeric_limits < int >:: min (); } } } The time complexity of the algorithm is O(V^3).","title":"Shortest Path Algorithms"},{"location":"algorithms/shortest_path_algorithms/#shortest-path-algorithms","text":"The shortest path problem is about finding a path between 2 vertices in a graph such that the total sum of the edges weights is minimum. BFS Dijkstra Bellman Ford Floyd Warshall Complexity O(V+E) O(V + E log(V)) O(VE) O(V3) Recommended grapth size Large Large/Medium Medium/Small Small Good for APSP Only works on unweighted graphs Ok Bad Yes Can detect negative cycles No No Yes Yes SP on graph with weighted edges Incorrect SP answer Best algorithm Works Bad in general SP on graph with unweighted edges Best algorithm Ok Bad Bad in general","title":"Shortest Path Algorithms"},{"location":"algorithms/shortest_path_algorithms/#1-bellman-fords-algorithm","text":"Bellman Ford's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph. Algorithm Steps: Initialize an array D to keep track of the shortest path from s to all of the nodes Set every entry in D to +\u221e The outer loop traverses from: 0 -> n - 1 . Loop over all edges, check if the D[next] > D[current] + weight, in this case update D[next] = D[current] + weight. Repeat the step 4 to find nodes caught in a negative cycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 std :: vector < int > dist ( n , std :: numeric_limits < int >:: max ()); dist [ 0 ] = 0 ; for ( int i = 0 ; i < n - 1 ; ++ i ) { for ( int j = 0 ; j < m ; ++ j ){ auto from = edges [ j ][ 0 ]; auto to = edges [ j ][ 1 ]; auto w = edges [ j ][ 2 ]; if ( dist [ from ] != std :: numeric_limits < int >:: max () && dist [ from ] + w < dist [ to ]) { dist [ to ] = dist [ from ] + w ; } } } // Repeat to find nodes caught in a negative cycle for ( int i = 0 ; i < n - 1 ; ++ i ) { for ( int j = 0 ; j < m ; ++ j ){ auto from = edges [ j ][ 0 ]; auto to = edges [ j ][ 1 ]; auto w = edges [ j ][ 2 ]; if ( dist [ from ] != std :: numeric_limits < int >:: max () && dist [ from ] + w < dist [ to ]) { dist [ to ] = std :: numeric_limits < int >:: min (); } } } The time complexity of the algorithm is O(VE).","title":"1. Bellman Ford's Algorithm"},{"location":"algorithms/shortest_path_algorithms/#2-dijkstras-algorithm","text":"Dijkstra's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph. One contraint for Dijkstra's algorithm is that the graph must only contain non-negative edge weight . This contraint is imposed to ensure that once a node has been visited its optimal distance can not be improved. Algorithm Steps: Maintain a dist array where the distance to every node is positive infinity, mark the distance to the start node s to be 0 Maintain a PQ of key-value pairs of (node_idx, distance) pairs and insert (s, 0) in to the PQ Pull out the pair (node_idx, distance) from PQ Update the distance of the connected vertices to node in node_idx in case of current vertex distance + edge weight < next vertex distance , then push them to PQ If the popped vertex is visited before, just continue without using it. Apply the same algorithm again until the PQ is empty. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 std :: vector < int > dist ( n , std :: numeric_limits < int >:: max ()); std :: vector < bool > visited ( n , false ); std :: priority_queue < std :: pair < int , int > , std :: vector < std :: pair < int , int >> , std :: greater < std :: pair < int , int >>> queue ; dist [ 0 ] = 0 ; queue . push ( std :: make_pair ( 0 , 0 )); while ( ! queue . empty ()) { auto w = queue . top (). first ; auto node_idx = queue . top (). second ; queue . pop (); if ( visited [ node_idx ]) { continue ; } visited [ node_idx ] = true ; if ( dist [ node_idx ] < w ) { continue ; } for ( int i = 0 ; i < adj [ node_idx ]. size (); ++ i ) { auto to = adj [ node_idx ][ i ]. first ; auto cost = adj [ node_idx ][ i ]. second ; if ( dist [ node_idx ] + cost < dist [ to ]) { dist [ to ] = dist [ node_idx ] + cost ; queue . push ( std :: make_pair ( dist [ to ], to )); } } } The time complexity of the algorithm is O(V + E log(V)).","title":"2. Dijkstra's Algorithm"},{"location":"algorithms/shortest_path_algorithms/#3-floyd-warshalls-algorithm","text":"Floyd Warshall's Algorithm is used to find the shortest paths between between all pairs of vertices in a graph, where each edge in the graph has a weight which is positive or negative. Algorithm Steps: Initialize the shortest paths between any 2 vertices with Infinity. Find all pair shortest paths that use 0 intermediate vertices, then find the shortest paths that use 1 intermediate vertex and so on.. until using all V vertices as intermediate nodes. Minimize the shortest paths between any 2 pairs in the previous operation. For any 2 vertices (i,j), one should actually minimize the distances between this pair using the first K nodes, so the shortest path will be: min(dist[i][k]+dist[k][j], dist[i][j]) Repeat to find nodes caught in a negative cycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for ( int k = 1 ; k <= n ; k ++ ){ for ( int i = 1 ; i <= n ; i ++ ){ for ( int j = 1 ; j <= n ; j ++ ){ dist [ i ][ j ] = min ( dist [ i ][ j ], dist [ i ][ k ] + dist [ k ][ j ]); } } } // Detect negative cycle for ( int k = 1 ; k <= n ; k ++ ){ for ( int i = 1 ; i <= n ; i ++ ){ for ( int j = 1 ; j <= n ; j ++ ){ dist [ i ][ j ] = std :: numeric_limits < int >:: min (); } } } The time complexity of the algorithm is O(V^3).","title":"3. Floyd Warshall's Algorithm"},{"location":"algorithms/sieve-of-eratosthenes/","text":"Sieve of Eratosthenes Sieve of Eratosthenes is an algorithm for finding all the prime numbers in a segment [1..n] using O(nloglogn). Algorithm Steps: 1 2 3 4 5 6 7 8 9 10 11 std :: vector < bool > primes ( n , true ); primes [ 1 ] = false ; for ( int i = 2 ; i * i < n ; ++ i ) { if ( ! primes [ i ]) { continue ; } for ( int j = i * i ; j < n ; j = j + i ) { primes [ j ] = false ; } } Time Complexity: O(nloglogn)","title":"Sieve of Eratosthenes"},{"location":"algorithms/sieve-of-eratosthenes/#sieve-of-eratosthenes","text":"Sieve of Eratosthenes is an algorithm for finding all the prime numbers in a segment [1..n] using O(nloglogn). Algorithm Steps: 1 2 3 4 5 6 7 8 9 10 11 std :: vector < bool > primes ( n , true ); primes [ 1 ] = false ; for ( int i = 2 ; i * i < n ; ++ i ) { if ( ! primes [ i ]) { continue ; } for ( int j = i * i ; j < n ; j = j + i ) { primes [ j ] = false ; } } Time Complexity: O(nloglogn)","title":"Sieve of Eratosthenes"},{"location":"algorithms/topological_sort/","text":"Topological Sort A topological ordering is an ordering of the nodes in a directed graph where for each directed edge from node A to node B, node A appears before node B in the ordering. For example: A topological sorting of this graph is: 1 2 3 4 5 Topological ordering are NOT unique. For the graph given above one another topological sorting is: 1 2 3 4 5 Note: Not every graph can have a topological ordering. A graph which contains a cycle can not have a valid ordering. 1. Kahn's algorithm Algorithm Steps: Compute in-degree (number of incoming edges) for each of the vertex present in the DAG and initialize the count of visited nodes as 0. Pick all the vertices with in-degree as 0 and add them into a queue (Enqueue operation) Remove a vertex from the queue (Dequeue operation) and then. Push node to topological array. Decrease in-degree by 1 for all its neighbouring nodes. If in-degree of a neighbouring nodes is reduced to zero, then add it to the queue. Repeat Step 3 until the queue is empty. If size of topological array is not equal to the number of nodes in the graph then the topological sort is not possible for the given graph. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 std :: vector < int > inDegree ( n , 0 ); for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < adj [ i ]. size (); ++ j ) { ++ inDegree [ adj [ i ][ j ]]; } } std :: queue < int > queue ; for ( int i = 0 ; i < n ; ++ i ) { if ( inDegree [ i ] == 0 ) { queue . push ( i ); } } std :: vector < int > topologicalOrdering ; while ( ! queue . empty ()) { auto node = queue . front (); queue . pop (); topologicalOrdering . push_back ( node ); for ( int i = 0 ; i < adj [ node ]. size (); ++ i ) { -- inDegree [ adj [ node ][ i ]]; if ( inDegree [ adj [ node ][ i ]] == 0 ) { queue . push ( adj [ node ][ i ]); } } } if ( topologicalOrdering . size () != n ) { // Graph contains a cycle } else { // The topological ordering is in this vector `topologicalOrdering` } 2. DFS Algorithm Steps: Pick an unvisited node Begin with the selected node, do a DFS exploring only unvisited nodes On the recursive callback of the DFS, add the current node to the topological ordering in the reverse order 1 2 3 4 5 6 7 8 9 10 T = [] visited = [] topological_sort ( cur_vert , N , adj [][]){ visited [ cur_vert ] = true for i = 0 to N if adj [ cur_vert ][ i ] is true and visited [ i ] is false topological_sort ( i ) T . insert_in_beginning ( cur_vert ) }","title":"Topological Sort"},{"location":"algorithms/topological_sort/#topological-sort","text":"A topological ordering is an ordering of the nodes in a directed graph where for each directed edge from node A to node B, node A appears before node B in the ordering. For example: A topological sorting of this graph is: 1 2 3 4 5 Topological ordering are NOT unique. For the graph given above one another topological sorting is: 1 2 3 4 5 Note: Not every graph can have a topological ordering. A graph which contains a cycle can not have a valid ordering.","title":"Topological Sort"},{"location":"algorithms/topological_sort/#1-kahns-algorithm","text":"Algorithm Steps: Compute in-degree (number of incoming edges) for each of the vertex present in the DAG and initialize the count of visited nodes as 0. Pick all the vertices with in-degree as 0 and add them into a queue (Enqueue operation) Remove a vertex from the queue (Dequeue operation) and then. Push node to topological array. Decrease in-degree by 1 for all its neighbouring nodes. If in-degree of a neighbouring nodes is reduced to zero, then add it to the queue. Repeat Step 3 until the queue is empty. If size of topological array is not equal to the number of nodes in the graph then the topological sort is not possible for the given graph. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 std :: vector < int > inDegree ( n , 0 ); for ( int i = 0 ; i < n ; ++ i ) { for ( int j = 0 ; j < adj [ i ]. size (); ++ j ) { ++ inDegree [ adj [ i ][ j ]]; } } std :: queue < int > queue ; for ( int i = 0 ; i < n ; ++ i ) { if ( inDegree [ i ] == 0 ) { queue . push ( i ); } } std :: vector < int > topologicalOrdering ; while ( ! queue . empty ()) { auto node = queue . front (); queue . pop (); topologicalOrdering . push_back ( node ); for ( int i = 0 ; i < adj [ node ]. size (); ++ i ) { -- inDegree [ adj [ node ][ i ]]; if ( inDegree [ adj [ node ][ i ]] == 0 ) { queue . push ( adj [ node ][ i ]); } } } if ( topologicalOrdering . size () != n ) { // Graph contains a cycle } else { // The topological ordering is in this vector `topologicalOrdering` }","title":"1. Kahn's algorithm"},{"location":"algorithms/topological_sort/#2-dfs","text":"Algorithm Steps: Pick an unvisited node Begin with the selected node, do a DFS exploring only unvisited nodes On the recursive callback of the DFS, add the current node to the topological ordering in the reverse order 1 2 3 4 5 6 7 8 9 10 T = [] visited = [] topological_sort ( cur_vert , N , adj [][]){ visited [ cur_vert ] = true for i = 0 to N if adj [ cur_vert ][ i ] is true and visited [ i ] is false topological_sort ( i ) T . insert_in_beginning ( cur_vert ) }","title":"2. DFS"},{"location":"algorithms/union_find/","text":"Union find (Disjoint set) 1. Disjoint set in math In mathematics, two sets are said to be disjoint sets if they have no element in common. Equivalently, disjoint sets are sets whose intersection is the empty set. For example, {1, 2, 3} and {4, 5, 6} are disjoint sets, while {1, 2, 3} and {3, 4, 5} are not. 2. Disjoint set data structure Union find(Disjoint set) is a data structure that stores a collection of disjoint (non-overlapping) sets. Its has 2 primary operations: Find : Determine which subset a particular element is in. This can be used for determining if two elements are in the same subset. Union : Join two subsets into a single subset Below is the sample code which implements union-find algorithms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class DisjointSet { private : std :: vector < int > parents ; public : DisjointSet ( int n ) { parents . resize ( n ); for ( int i = 0 ; i < n ; ++ i ) { parents [ i ] = i ; } } int find ( int x ) { if ( parents [ x ] == x ) { return x ; } return find ( parents [ x ]); } void unionSet ( int x , int y ) { int parentX = find ( x ); int parentY = find ( y ); parents [ parentY ] = parentX ; } }; 3. Optimization Path compression This optimization is designed for speeding up find method. The naive find() method is read-only , when find() is called for an element i, root of the tree is returned, the find() operation traverses up from i to find root. The idea of path compression is to make the found root as parent of i so that we don\u2019t have to traverse all intermediate nodes again. If i is root of a subtree, then path (to root) from all nodes under i also compresses. Below is the optimized find() method with Path Compression. 1 2 3 4 5 6 7 int find ( int x ) { if ( parents [ x ] != x ) { parents [ x ] = find ( parents [ x ]); } return parents [ x ]; } We can also implement find without recursion. 1 2 3 4 5 6 7 8 int find ( int x ) { while ( parents [ x ] != x ) { parents [ x ] = parents [ parents [ x ]]; x = parents [ x ]; } return parents [ x ]; } Union by rank In this optimization we will change the unionSet method. In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length O(n). The solution is to always attach smaller depth tree under the root of the deeper tree. Below is the optimized unionSet() method with Union by rank. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 void unionSet ( int x , int y ) { int parentX = find ( x ); int parentY = find ( y ); if ( parentX == parentY ) { return ; } if ( ranks [ parentX ] < ranks [ parentY ]) { parents [ parentX ] = parentY ; } else if ( ranks [ parentX ] > ranks [ parentY ]) { parents [ parentY ] = parentX ; } else { // If ranks are the same parents [ parentX ] = parentY ; ++ ranks [ parentY ]; } }","title":"Union find"},{"location":"algorithms/union_find/#union-find-disjoint-set","text":"","title":"Union find (Disjoint set)"},{"location":"algorithms/union_find/#1-disjoint-set-in-math","text":"In mathematics, two sets are said to be disjoint sets if they have no element in common. Equivalently, disjoint sets are sets whose intersection is the empty set. For example, {1, 2, 3} and {4, 5, 6} are disjoint sets, while {1, 2, 3} and {3, 4, 5} are not.","title":"1. Disjoint set in math"},{"location":"algorithms/union_find/#2-disjoint-set-data-structure","text":"Union find(Disjoint set) is a data structure that stores a collection of disjoint (non-overlapping) sets. Its has 2 primary operations: Find : Determine which subset a particular element is in. This can be used for determining if two elements are in the same subset. Union : Join two subsets into a single subset Below is the sample code which implements union-find algorithms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class DisjointSet { private : std :: vector < int > parents ; public : DisjointSet ( int n ) { parents . resize ( n ); for ( int i = 0 ; i < n ; ++ i ) { parents [ i ] = i ; } } int find ( int x ) { if ( parents [ x ] == x ) { return x ; } return find ( parents [ x ]); } void unionSet ( int x , int y ) { int parentX = find ( x ); int parentY = find ( y ); parents [ parentY ] = parentX ; } };","title":"2. Disjoint set data structure"},{"location":"algorithms/union_find/#3-optimization","text":"","title":"3. Optimization"},{"location":"algorithms/union_find/#path-compression","text":"This optimization is designed for speeding up find method. The naive find() method is read-only , when find() is called for an element i, root of the tree is returned, the find() operation traverses up from i to find root. The idea of path compression is to make the found root as parent of i so that we don\u2019t have to traverse all intermediate nodes again. If i is root of a subtree, then path (to root) from all nodes under i also compresses. Below is the optimized find() method with Path Compression. 1 2 3 4 5 6 7 int find ( int x ) { if ( parents [ x ] != x ) { parents [ x ] = find ( parents [ x ]); } return parents [ x ]; } We can also implement find without recursion. 1 2 3 4 5 6 7 8 int find ( int x ) { while ( parents [ x ] != x ) { parents [ x ] = parents [ parents [ x ]]; x = parents [ x ]; } return parents [ x ]; }","title":"Path compression"},{"location":"algorithms/union_find/#union-by-rank","text":"In this optimization we will change the unionSet method. In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length O(n). The solution is to always attach smaller depth tree under the root of the deeper tree. Below is the optimized unionSet() method with Union by rank. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 void unionSet ( int x , int y ) { int parentX = find ( x ); int parentY = find ( y ); if ( parentX == parentY ) { return ; } if ( ranks [ parentX ] < ranks [ parentY ]) { parents [ parentX ] = parentY ; } else if ( ranks [ parentX ] > ranks [ parentY ]) { parents [ parentY ] = parentX ; } else { // If ranks are the same parents [ parentX ] = parentY ; ++ ranks [ parentY ]; } }","title":"Union by rank"},{"location":"devops/kubernetes/config_maps/","text":"ConfigMaps 1. Description A ConfigMap is an API object used to store non-credential data in key-value pairs. Kubernetes pods can use the created ConfigMaps as a: Configuration files Environment variable Command-line argument A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. Importantly, ConfigMaps are not suitalbe for storing a confidental data. They don't provide any kind of encryption, and all the data in them are visible to anyone who has access to the file. 2. Define a ConfigMap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : ConfigMap metadata : name : game-demo data : # property-like keys; each key maps to a simple value player_initial_lives : \"3\" ui_properties_file_name : \"user-interface.properties\" # file-like keys game.properties : | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties : | color.good=purple color.bad=yellow allow.textmode=true prometheus.yaml : | global: scrape_interval: 15s scrape_configs: - job_name: prometheus metrics_path: /prometheus/metrics static_configs: - targets: - localhost:9090 In a ConfigMap, the required information can store in the data field. We can store values as two ways: As individual key-value pair properties In a granular format where they are fragments of a configuration format. (File Like Keys) 3. Utilizing ConfigMaps in Pod There are four ways that you can use a ConfigMap to configure a container inside a Pod: Inside a container command and args Environment variables for a containers Add a file in read-only volumne, for application to read Write code inside the Pod that uses the K8s API to read a ConfigMap Here's an example Pod that that uses values from the above ConfigMap: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : alpine command : [ \"sleep\" , \"3600\" ] env : # Define the environment variable - name : PLAYER_INITIAL_LIVES # Notice that the case is different here # from the key name in the ConfigMap. valueFrom : configMapKeyRef : name : game-demo # The ConfigMap this value comes from. key : player_initial_lives # The key to fetch. - name : UI_PROPERTIES_FILE_NAME valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name volumeMounts : - name : config mountPath : \"/config\" readOnly : true volumes : # You set volumes at the Pod level, then mount them into containers inside that Pod - name : config configMap : # Provide the name of the ConfigMap you want to mount. name : game-demo # An array of keys from the ConfigMap to create as files items : - key : \"game.properties\" path : \"game.properties\" - key : \"user-interface.properties\" path : \"user-interface.properties\" - key : \"prometheus.yaml\" path : \"prometheus.yaml\" For this example, defining a volume and mounting it inside the demo container as /config creates three files, /config/game.properties , /config/user-interface.properties and prometheus.yaml , even though there are four keys in the ConfigMap. This is because the Pod definition specifies an items array in the volumes section. If you omit the items array entirely, every key in the ConfigMap becomes a file with the same name as the key, and you get 5 files Pods use ConfigMaps through environment variables and configMap volumes Pass ConfigMap entries to a pod as files in a volume Pass a ConfigMap entry as a command-line argument 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : fortune-args-from-configmap spec : containers : - image : luksa/fortune:args env : - name : INTERVAL valueFrom : configMapKeyRef : name : fortune-config key : sleep-interval args : [ \"$(INTERVAL)\" ]","title":"ConfigMaps"},{"location":"devops/kubernetes/config_maps/#configmaps","text":"","title":"ConfigMaps"},{"location":"devops/kubernetes/config_maps/#1-description","text":"A ConfigMap is an API object used to store non-credential data in key-value pairs. Kubernetes pods can use the created ConfigMaps as a: Configuration files Environment variable Command-line argument A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. Importantly, ConfigMaps are not suitalbe for storing a confidental data. They don't provide any kind of encryption, and all the data in them are visible to anyone who has access to the file.","title":"1. Description"},{"location":"devops/kubernetes/config_maps/#2-define-a-configmap","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : ConfigMap metadata : name : game-demo data : # property-like keys; each key maps to a simple value player_initial_lives : \"3\" ui_properties_file_name : \"user-interface.properties\" # file-like keys game.properties : | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties : | color.good=purple color.bad=yellow allow.textmode=true prometheus.yaml : | global: scrape_interval: 15s scrape_configs: - job_name: prometheus metrics_path: /prometheus/metrics static_configs: - targets: - localhost:9090 In a ConfigMap, the required information can store in the data field. We can store values as two ways: As individual key-value pair properties In a granular format where they are fragments of a configuration format. (File Like Keys)","title":"2. Define a ConfigMap"},{"location":"devops/kubernetes/config_maps/#3-utilizing-configmaps-in-pod","text":"There are four ways that you can use a ConfigMap to configure a container inside a Pod: Inside a container command and args Environment variables for a containers Add a file in read-only volumne, for application to read Write code inside the Pod that uses the K8s API to read a ConfigMap Here's an example Pod that that uses values from the above ConfigMap: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : alpine command : [ \"sleep\" , \"3600\" ] env : # Define the environment variable - name : PLAYER_INITIAL_LIVES # Notice that the case is different here # from the key name in the ConfigMap. valueFrom : configMapKeyRef : name : game-demo # The ConfigMap this value comes from. key : player_initial_lives # The key to fetch. - name : UI_PROPERTIES_FILE_NAME valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name volumeMounts : - name : config mountPath : \"/config\" readOnly : true volumes : # You set volumes at the Pod level, then mount them into containers inside that Pod - name : config configMap : # Provide the name of the ConfigMap you want to mount. name : game-demo # An array of keys from the ConfigMap to create as files items : - key : \"game.properties\" path : \"game.properties\" - key : \"user-interface.properties\" path : \"user-interface.properties\" - key : \"prometheus.yaml\" path : \"prometheus.yaml\" For this example, defining a volume and mounting it inside the demo container as /config creates three files, /config/game.properties , /config/user-interface.properties and prometheus.yaml , even though there are four keys in the ConfigMap. This is because the Pod definition specifies an items array in the volumes section. If you omit the items array entirely, every key in the ConfigMap becomes a file with the same name as the key, and you get 5 files","title":"3. Utilizing ConfigMaps in Pod"},{"location":"devops/kubernetes/config_maps/#pods-use-configmaps-through-environment-variables-and-configmap-volumes","text":"","title":"Pods use ConfigMaps through environment variables and configMap volumes"},{"location":"devops/kubernetes/config_maps/#pass-configmap-entries-to-a-pod-as-files-in-a-volume","text":"","title":"Pass ConfigMap entries to a pod as files in a volume"},{"location":"devops/kubernetes/config_maps/#pass-a-configmap-entry-as-a-command-line-argument","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : fortune-args-from-configmap spec : containers : - image : luksa/fortune:args env : - name : INTERVAL valueFrom : configMapKeyRef : name : fortune-config key : sleep-interval args : [ \"$(INTERVAL)\" ]","title":"Pass a ConfigMap entry as a command-line argument"},{"location":"devops/kubernetes/controllers/","text":"Controllers 1. ReplicaSet A ReplicaSet ensures that a specified number of pod replicas are running at any given time. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend spec : replicas : 2 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 The apiVersion , kind , metadata fields are mandatory with all k8s objects. Line 1: We specified that the apiVersion is apps.v1 Line 2-3: The kind is ReplicaSet and metadata has the name key set to frontend Line 5-6: The first field we defined in the spec section is replicas . It sets the desired number of replicas of the pod. In this case the ReplicaSet should ensure that 2 pods should run concurrently. Line 7: The next spec section is the selector . We use it to select which pods should be included in the ReplicaSet. If pods that match the selector exist, ReplicaSet will do nothing. If they don't, it will create as many as pods that match the value of the replicas field. Not only that ReplicaSet creates the pods that are missing, but also monitors the cluster and ensures that the desired number of replicas is always running. Line 8-9: We used spec.selector.matchLabels to specify labels. They must match the labels defined in spec.tempalte . In our case, ReplicaSet will look for Pods with tier set to frontend . If pods with this label does not exist, it will create them using the spec.template section. Line 10-13: The last section of the spec field is the template and it has the same schema as a Pod specification. At a minimum, the labels of the spec.template.metadata.labels section must match those specified in the spec.selector.matchLabels . Line 14-17: Finally the spec.template.spec section the containers definition. The process of creating ReplicaSet K8s client (kubectl) sent a request to the API server requesting the creation of ReplicaSet The controller is watching the API server for new events and it detected that there is a new ReplicaSet objecto The controller creates 2 new pod definitions because we have configured replica value as 2 in the above example The scheduler is watching the API server for new events and it detected that there are 2 unasigned pods The scheduler decided which node to assign the Pod and sent that information to the API server Kublet is also watching the API server. It detected that the 2 pods were assigned to the node it is running on Kublet sent request to Docker requesting the creation of the containers that form the Pod. Finally, Kublet sent a request to the API server notifying it that the pods were created successfully A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, k8s recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all. This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section. 2. Deployments Kubernetes Deployment is the process of providing declarative updates to Pods and ReplicaSets. Define a zero-downtime deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : apps/v1 kind : Deployment metadata : name : go-demo-2-api spec : replicas : 3 selector : matchLabels : type : api service : go-demo-2 minReadySeconds : 1 progressDeadlineSeconds : 60 revisionHistoryLimit : 5 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 1 template : metadata : labels : type : api service : go-demo-2 language : go spec : containers : - name : api image : vfarcic/go-demo-2 env : - name : DB value : go-demo-2-db readinessProbe : httpGet : path : /demo/hello port : 8080 periodSeconds : 1 livenessProbe : httpGet : path : /demo/hello port : 8080 spec.minReadySeconds : defines the number of seconds before k8s starts considering the Pod healthy. The default value is 0 , meaning that the Pods will be considered available as soon as they are ready and, when specified livenessProbe returns OK. spec.revisionHistoryLimit : defines the number of old ReplicaSet we can rollback (default value is 10 ) spec.strategy.type : can be either RollingUpdate or Recreate type. Deployment strategies Recreate The recreate strategy is a dummy deployment which consists of shutting down version A then deploying version B after version A is turned off. This technique implies downtime of the service that depends on both shutdown and boot duration of the application. Pros: Easy to setup Application state entirely renewed. Cons: High impact on the user, expect downtime that depends on both shutdown and boot duration of the application. Rolling update The rolling update deployment strategy consists of slowly rolling out a version of an application by replacing instances one after the other until all the instances are rolled out. Pros: Easy to setup Version is slowly released across instances Convenient for stateful applications that can handle rebalancing of the data. Cons: Rollout/rollback can take time. Supporting multiple APIs is hard. No control over traffic.","title":"Controllers"},{"location":"devops/kubernetes/controllers/#controllers","text":"","title":"Controllers"},{"location":"devops/kubernetes/controllers/#1-replicaset","text":"A ReplicaSet ensures that a specified number of pod replicas are running at any given time. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : apps/v1 kind : ReplicaSet metadata : name : frontend spec : replicas : 2 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : php-redis image : gcr.io/google_samples/gb-frontend:v3 The apiVersion , kind , metadata fields are mandatory with all k8s objects. Line 1: We specified that the apiVersion is apps.v1 Line 2-3: The kind is ReplicaSet and metadata has the name key set to frontend Line 5-6: The first field we defined in the spec section is replicas . It sets the desired number of replicas of the pod. In this case the ReplicaSet should ensure that 2 pods should run concurrently. Line 7: The next spec section is the selector . We use it to select which pods should be included in the ReplicaSet. If pods that match the selector exist, ReplicaSet will do nothing. If they don't, it will create as many as pods that match the value of the replicas field. Not only that ReplicaSet creates the pods that are missing, but also monitors the cluster and ensures that the desired number of replicas is always running. Line 8-9: We used spec.selector.matchLabels to specify labels. They must match the labels defined in spec.tempalte . In our case, ReplicaSet will look for Pods with tier set to frontend . If pods with this label does not exist, it will create them using the spec.template section. Line 10-13: The last section of the spec field is the template and it has the same schema as a Pod specification. At a minimum, the labels of the spec.template.metadata.labels section must match those specified in the spec.selector.matchLabels . Line 14-17: Finally the spec.template.spec section the containers definition.","title":"1. ReplicaSet"},{"location":"devops/kubernetes/controllers/#the-process-of-creating-replicaset","text":"K8s client (kubectl) sent a request to the API server requesting the creation of ReplicaSet The controller is watching the API server for new events and it detected that there is a new ReplicaSet objecto The controller creates 2 new pod definitions because we have configured replica value as 2 in the above example The scheduler is watching the API server for new events and it detected that there are 2 unasigned pods The scheduler decided which node to assign the Pod and sent that information to the API server Kublet is also watching the API server. It detected that the 2 pods were assigned to the node it is running on Kublet sent request to Docker requesting the creation of the containers that form the Pod. Finally, Kublet sent a request to the API server notifying it that the pods were created successfully A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, k8s recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all. This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.","title":"The process of creating ReplicaSet"},{"location":"devops/kubernetes/controllers/#2-deployments","text":"Kubernetes Deployment is the process of providing declarative updates to Pods and ReplicaSets.","title":"2. Deployments"},{"location":"devops/kubernetes/controllers/#define-a-zero-downtime-deployment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : apps/v1 kind : Deployment metadata : name : go-demo-2-api spec : replicas : 3 selector : matchLabels : type : api service : go-demo-2 minReadySeconds : 1 progressDeadlineSeconds : 60 revisionHistoryLimit : 5 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 1 template : metadata : labels : type : api service : go-demo-2 language : go spec : containers : - name : api image : vfarcic/go-demo-2 env : - name : DB value : go-demo-2-db readinessProbe : httpGet : path : /demo/hello port : 8080 periodSeconds : 1 livenessProbe : httpGet : path : /demo/hello port : 8080 spec.minReadySeconds : defines the number of seconds before k8s starts considering the Pod healthy. The default value is 0 , meaning that the Pods will be considered available as soon as they are ready and, when specified livenessProbe returns OK. spec.revisionHistoryLimit : defines the number of old ReplicaSet we can rollback (default value is 10 ) spec.strategy.type : can be either RollingUpdate or Recreate type.","title":"Define a zero-downtime deployment"},{"location":"devops/kubernetes/controllers/#deployment-strategies","text":"","title":"Deployment strategies"},{"location":"devops/kubernetes/controllers/#recreate","text":"The recreate strategy is a dummy deployment which consists of shutting down version A then deploying version B after version A is turned off. This technique implies downtime of the service that depends on both shutdown and boot duration of the application. Pros: Easy to setup Application state entirely renewed. Cons: High impact on the user, expect downtime that depends on both shutdown and boot duration of the application.","title":"Recreate"},{"location":"devops/kubernetes/controllers/#rolling-update","text":"The rolling update deployment strategy consists of slowly rolling out a version of an application by replacing instances one after the other until all the instances are rolled out. Pros: Easy to setup Version is slowly released across instances Convenient for stateful applications that can handle rebalancing of the data. Cons: Rollout/rollback can take time. Supporting multiple APIs is hard. No control over traffic.","title":"Rolling update"},{"location":"devops/kubernetes/ingress/","text":"Ingress 1. What is a ingress? Ingress is an object that allows access to the k8s services within the cluster from outside. Traffic routing is controlled by rules defined on the Ingress Controller. An ingress may be configurated to make services externally-reachable URLs, load balance traffic, terminate SSL/TLS, and offer name-based virtual hosting. You must have an Ingress Controller to satisfy an Ingress. Only creating an Ingress resource has no effect. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : minimal-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /testpath pathType : Prefix backend : service : name : test port : number : 80 2. Ingress rules Each HTTP each contains the following information: An optional host. In the above example, no host is specified, so the rule applies to all inbound HTTP traffic throught the IP adress specified. If a host is provided (eg: dinhhuy258.com) the rules apply to that host. A list of paths (Eg: /testpath ), each of which has an associated backend defined with a service.name and service.port.name or service.port.number . Both the host and path must match the content of an incomming request before the load balancer directs traffic to the referenced Service. 3. Ingress path types Each path in Ingress is required to have a corresponding path type. There are three supported path types: ImplementationSpecific : With this path type, matching is up to the IngressClass. Implementation can treat this as a separate pathType or treat it identically to Prefix or Exact path types. Exact : Match the URL path exactly and with case sensitive. Prefix : Match based on a URL path prefix split by / Note: If the last element of the path is a substring of the last element in the request path, it is not match (Eg: /foo/bar matches /foo/bar/baz but does not match /foo/barbaz ) 4. TLS You can secure an Ingress by specifying a Secret that contains a TLS private key an certificate. Eg: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Secret metadata : name : testsecret-tls namespace : default data : tls.crt : base64 encoded cert tls.key : base64 encoded key type : kubernetes.io/tls 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - https-example.foo.com secretName : testsecret-tls rules : - host : https-example.foo.com http : paths : - path : / pathType : Prefix backend : service : apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - https-example.foo.com secretName : testsecret-tls rules : - host : https-example.foo.com http : paths : - path : / pathType : Prefix backend : service : name : service1 port : number : 80 name : service1 port : number : 80 5. Load balancing An Ingress Controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (eg: persistent session, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.","title":"Ingress"},{"location":"devops/kubernetes/ingress/#ingress","text":"","title":"Ingress"},{"location":"devops/kubernetes/ingress/#1-what-is-a-ingress","text":"Ingress is an object that allows access to the k8s services within the cluster from outside. Traffic routing is controlled by rules defined on the Ingress Controller. An ingress may be configurated to make services externally-reachable URLs, load balance traffic, terminate SSL/TLS, and offer name-based virtual hosting. You must have an Ingress Controller to satisfy an Ingress. Only creating an Ingress resource has no effect. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : minimal-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /testpath pathType : Prefix backend : service : name : test port : number : 80","title":"1. What is a ingress?"},{"location":"devops/kubernetes/ingress/#2-ingress-rules","text":"Each HTTP each contains the following information: An optional host. In the above example, no host is specified, so the rule applies to all inbound HTTP traffic throught the IP adress specified. If a host is provided (eg: dinhhuy258.com) the rules apply to that host. A list of paths (Eg: /testpath ), each of which has an associated backend defined with a service.name and service.port.name or service.port.number . Both the host and path must match the content of an incomming request before the load balancer directs traffic to the referenced Service.","title":"2. Ingress rules"},{"location":"devops/kubernetes/ingress/#3-ingress-path-types","text":"Each path in Ingress is required to have a corresponding path type. There are three supported path types: ImplementationSpecific : With this path type, matching is up to the IngressClass. Implementation can treat this as a separate pathType or treat it identically to Prefix or Exact path types. Exact : Match the URL path exactly and with case sensitive. Prefix : Match based on a URL path prefix split by / Note: If the last element of the path is a substring of the last element in the request path, it is not match (Eg: /foo/bar matches /foo/bar/baz but does not match /foo/barbaz )","title":"3. Ingress path types"},{"location":"devops/kubernetes/ingress/#4-tls","text":"You can secure an Ingress by specifying a Secret that contains a TLS private key an certificate. Eg: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Secret metadata : name : testsecret-tls namespace : default data : tls.crt : base64 encoded cert tls.key : base64 encoded key type : kubernetes.io/tls 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - https-example.foo.com secretName : testsecret-tls rules : - host : https-example.foo.com http : paths : - path : / pathType : Prefix backend : service : apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - https-example.foo.com secretName : testsecret-tls rules : - host : https-example.foo.com http : paths : - path : / pathType : Prefix backend : service : name : service1 port : number : 80 name : service1 port : number : 80","title":"4. TLS"},{"location":"devops/kubernetes/ingress/#5-load-balancing","text":"An Ingress Controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (eg: persistent session, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.","title":"5. Load balancing"},{"location":"devops/kubernetes/pods/","text":"Pods 1. What is a pod? Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container. In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes. 2. Pod networking Each Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports. Inside a Pod (and only then), the containers that belong to the Pod can communicate with one another using localhost . When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports). The containers in a Pod can also communicate with each other using standard inter-process communications Containers in different Pods have distinct IP addresses and can not communicate by IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate. 3. Pod lifetime Pods are created, assigned a unique ID (UID), and scheduled to nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods scheduled to that node are scheduled for deletion after a timeout period. Pods do not, by themselves, self-heal. If a Pod is scheduled to a node that then fails, the Pod is deleted; likewise, a Pod won't survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a higher-level abstraction, called a controller, that handles the work of managing the relatively disposable Pod instances.","title":"Pods"},{"location":"devops/kubernetes/pods/#pods","text":"","title":"Pods"},{"location":"devops/kubernetes/pods/#1-what-is-a-pod","text":"Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container. In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes.","title":"1. What is a pod?"},{"location":"devops/kubernetes/pods/#2-pod-networking","text":"Each Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports. Inside a Pod (and only then), the containers that belong to the Pod can communicate with one another using localhost . When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports). The containers in a Pod can also communicate with each other using standard inter-process communications Containers in different Pods have distinct IP addresses and can not communicate by IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.","title":"2. Pod networking"},{"location":"devops/kubernetes/pods/#3-pod-lifetime","text":"Pods are created, assigned a unique ID (UID), and scheduled to nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods scheduled to that node are scheduled for deletion after a timeout period. Pods do not, by themselves, self-heal. If a Pod is scheduled to a node that then fails, the Pod is deleted; likewise, a Pod won't survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a higher-level abstraction, called a controller, that handles the work of managing the relatively disposable Pod instances.","title":"3. Pod lifetime"},{"location":"devops/kubernetes/service/","text":"Service Kubernetes pods are created and destroyed to match the state of your cluster. If you use Deployment to run your app, it can create and destroy pods dynamically. Each pods get it own IP address, however in Deployment, the set of pods running in one moment in time could be different from the set of pods running that application a moment later. It leads to a problem that if set of pods (backend) provides functionality for other pods (frontend) inside your cluster, how do the frontend can find out and keep track which ip address to connect to? Kubernetes Services provides addresses through which associated pods can be accessed. 1. Define a service For example, suppose you have a set of pods where each listens on TCP port 9376 and contains a label app=MyApp 1 2 3 4 5 6 7 8 9 10 11 apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 This specification creates a new Service object named my-service which targets TCP port 9376 on any Pod with the app=MyApp label. Kubernetes assigns this Service an IP address (sometimes called the \"cluster IP\"), which is used by the Service proxies. The controller for the Service selector continuously scans for Pods that match its selector, and then POSTs any updates to an Endpoint object also named my-service . 2. Services without selectors Services most commonly abstract access to Kubernetes Pods, but they can also abstract other kinds of backends. For example: You want to have an external database cluster in production, but in your test environment you use your own databases. You want to point your Service to a Service in a different Namespace or on another cluster. Eg: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Service metadata : name : my-service spec : ports : - protocol : TCP port : 80 targetPort : 9376 Because this Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to the network address and port where it's running, by adding an Endpoints object manually: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 ports : - port : 9376 3. Type of service Kubernetes Services allow you to specify what kind of Service you want. The default is ClusterIP . Type values and their behavior are: ClusterIP : Exposes the service on cluster internal IP. Choosing this makes the Service only reachable from within the cluster. NodePort : Exposes the service on each Node'IP at a static port. A ClusterIP is automatically created. You will be able to reach the NodePort service from outside the cluster, by requesting : LoadBalancer : Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the content of the externalName field.","title":"Service"},{"location":"devops/kubernetes/service/#service","text":"Kubernetes pods are created and destroyed to match the state of your cluster. If you use Deployment to run your app, it can create and destroy pods dynamically. Each pods get it own IP address, however in Deployment, the set of pods running in one moment in time could be different from the set of pods running that application a moment later. It leads to a problem that if set of pods (backend) provides functionality for other pods (frontend) inside your cluster, how do the frontend can find out and keep track which ip address to connect to? Kubernetes Services provides addresses through which associated pods can be accessed.","title":"Service"},{"location":"devops/kubernetes/service/#1-define-a-service","text":"For example, suppose you have a set of pods where each listens on TCP port 9376 and contains a label app=MyApp 1 2 3 4 5 6 7 8 9 10 11 apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 This specification creates a new Service object named my-service which targets TCP port 9376 on any Pod with the app=MyApp label. Kubernetes assigns this Service an IP address (sometimes called the \"cluster IP\"), which is used by the Service proxies. The controller for the Service selector continuously scans for Pods that match its selector, and then POSTs any updates to an Endpoint object also named my-service .","title":"1. Define a service"},{"location":"devops/kubernetes/service/#2-services-without-selectors","text":"Services most commonly abstract access to Kubernetes Pods, but they can also abstract other kinds of backends. For example: You want to have an external database cluster in production, but in your test environment you use your own databases. You want to point your Service to a Service in a different Namespace or on another cluster. Eg: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Service metadata : name : my-service spec : ports : - protocol : TCP port : 80 targetPort : 9376 Because this Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to the network address and port where it's running, by adding an Endpoints object manually: 1 2 3 4 5 6 7 8 9 apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 ports : - port : 9376","title":"2. Services without selectors"},{"location":"devops/kubernetes/service/#3-type-of-service","text":"Kubernetes Services allow you to specify what kind of Service you want. The default is ClusterIP . Type values and their behavior are: ClusterIP : Exposes the service on cluster internal IP. Choosing this makes the Service only reachable from within the cluster. NodePort : Exposes the service on each Node'IP at a static port. A ClusterIP is automatically created. You will be able to reach the NodePort service from outside the cluster, by requesting : LoadBalancer : Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the content of the externalName field.","title":"3. Type of service"},{"location":"devops/kubernetes/volumes/","text":"Volumes 1. Description On-disk file in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem is the loss of files when a container crashes. The kubelet restarts the container but with a clean state. A second problem occurs when sharing files between containers running together in a Pod . The k8s Volumes solves both of these problems. 2. Types of Volumes K8s supportes several types of volumes. We can categorize the Kubernetes Volumes based on their lifecycle. Considering the lifecycle of the volumes, we can have: Ephemeral Volumes , which are tightly coupled with the lifetime of the Node (for example emptyDir, or hostPath) and they are deleted if the Node goes down. Persistent Volumes , which are meant for long-term storage and are independent of the Pods or Nodes lifecycle. These can be cloud volumes (like gcePersistentDisk, awsElasticBlockStore, azureFile or azureDisk), NFS (Network File Systems) or Persistent Volume Claims (a series of abstraction to connect to the underlying cloud provided storage volumes). 3. Ephemeral Volumes emptyDir An emptyDir volume is first created when a Pod is assigned to a node and exists as long as that Pod is running on that node and the volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently. Depending on your environment, emptyDir volumes are stored on whatever medium that backs the node, such as disk or SSD or RAM. Some use cases for an emptyDir are: Scratch space, for a sort algorithm for example Checkpointing a long computation for recovery from crashes As a cache Holding files that a content-manager container fetches while a webserver Container serves the data Note: emptyDir volume should NOT be used for persisting data (database, application data, etc) Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : test-nginx spec : containers : - image : nginx name : test-nginx volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} Let\u2019s try applying the YAML file and get into the Pod. 1 2 3 4 5 6 7 8 9 10 kubectl apply -f emptydir.yml pod/test-nginx created kubectl get pods NAME READY STATUS RESTARTS AGE test-nginx 1 /1 Running 0 7s kubectl exec -it test-nginx -- /bin/bash root@test-nginx:/# mount | grep -i cache /dev/vda1 on /cache type ext4 ( rw,relatime ) If we see the storage medium used for the emptyDir mounted on the container we just created, it shows up as /dev/vda1 a paravirtualization disk driver. If you set the emptyDir.medium field to Memory . Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Pod metadata : name : test-nginx spec : containers : - image : nginx name : test-nginx volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : medium : Memory hostPath A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. Some use cases for an hostPath are: Running a container that needs access to Docker internals; use a hostPath of /var/lib/docker Persist database data, application data Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory The supported values for field type are: DirectoryOrCreate : If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755 , having the same group and ownership with Kubelet. Directory : A directory must exist at the given path FileOrCreate : If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644 , having the same group and ownership with Kubelet. File : A file must exist at the given path Socket : A UNIX socket must exist at the given path CharDevice : A character device must exist at the given path BlockDevice : A block device must exist at the given path","title":"Volumes"},{"location":"devops/kubernetes/volumes/#volumes","text":"","title":"Volumes"},{"location":"devops/kubernetes/volumes/#1-description","text":"On-disk file in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem is the loss of files when a container crashes. The kubelet restarts the container but with a clean state. A second problem occurs when sharing files between containers running together in a Pod . The k8s Volumes solves both of these problems.","title":"1. Description"},{"location":"devops/kubernetes/volumes/#2-types-of-volumes","text":"K8s supportes several types of volumes. We can categorize the Kubernetes Volumes based on their lifecycle. Considering the lifecycle of the volumes, we can have: Ephemeral Volumes , which are tightly coupled with the lifetime of the Node (for example emptyDir, or hostPath) and they are deleted if the Node goes down. Persistent Volumes , which are meant for long-term storage and are independent of the Pods or Nodes lifecycle. These can be cloud volumes (like gcePersistentDisk, awsElasticBlockStore, azureFile or azureDisk), NFS (Network File Systems) or Persistent Volume Claims (a series of abstraction to connect to the underlying cloud provided storage volumes).","title":"2. Types of Volumes"},{"location":"devops/kubernetes/volumes/#3-ephemeral-volumes","text":"","title":"3. Ephemeral Volumes"},{"location":"devops/kubernetes/volumes/#emptydir","text":"An emptyDir volume is first created when a Pod is assigned to a node and exists as long as that Pod is running on that node and the volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently. Depending on your environment, emptyDir volumes are stored on whatever medium that backs the node, such as disk or SSD or RAM. Some use cases for an emptyDir are: Scratch space, for a sort algorithm for example Checkpointing a long computation for recovery from crashes As a cache Holding files that a content-manager container fetches while a webserver Container serves the data Note: emptyDir volume should NOT be used for persisting data (database, application data, etc) Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Pod metadata : name : test-nginx spec : containers : - image : nginx name : test-nginx volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} Let\u2019s try applying the YAML file and get into the Pod. 1 2 3 4 5 6 7 8 9 10 kubectl apply -f emptydir.yml pod/test-nginx created kubectl get pods NAME READY STATUS RESTARTS AGE test-nginx 1 /1 Running 0 7s kubectl exec -it test-nginx -- /bin/bash root@test-nginx:/# mount | grep -i cache /dev/vda1 on /cache type ext4 ( rw,relatime ) If we see the storage medium used for the emptyDir mounted on the container we just created, it shows up as /dev/vda1 a paravirtualization disk driver. If you set the emptyDir.medium field to Memory . Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : Pod metadata : name : test-nginx spec : containers : - image : nginx name : test-nginx volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : medium : Memory","title":"emptyDir"},{"location":"devops/kubernetes/volumes/#hostpath","text":"A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. Some use cases for an hostPath are: Running a container that needs access to Docker internals; use a hostPath of /var/lib/docker Persist database data, application data Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory The supported values for field type are: DirectoryOrCreate : If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755 , having the same group and ownership with Kubelet. Directory : A directory must exist at the given path FileOrCreate : If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644 , having the same group and ownership with Kubelet. File : A file must exist at the given path Socket : A UNIX socket must exist at the given path CharDevice : A character device must exist at the given path BlockDevice : A block device must exist at the given path","title":"hostPath"},{"location":"distributed-system/api_gateway/","text":"API Gateway An API gateway is a service which is the entry point into the application from the outside world and acts as a single point of entry for a defined group of microservices. In high level It\u2019s responsible for request routing, API composition, and other functions, such as authentication. Functionalities of API Gateway: Authentication and authorization Load balancing Service discovery integration Protocol translation Response caching Retry policies, circuit breaker, and QoS Rate limiting and throttling Logging, tracing, correlation Headers, query strings, and claims transformation IP whitelisting IAM Centralized Logging (transaction ID across the servers, error logging) Identity Provider, Authentication and Authorization List of notable API Gateways: Zuul Amazon API Gateway Kong Gateway Benefits of API gateways Decoupling : If your clients which you have no control over communicated directly with many separate services, renaming or moving those services can be challenging as the client is coupled to the underlying architecture and organization. API gateways enables you to route based on path, hostname, headers, and other key information enabling you to decouple the publicly facing API endpoints from the underlying microservice architecture. Reduce Round Trips : Certain API endpoints may need to join data across multiple services. API gateways can perform this aggregation so that the client doesn\u2019t not need complicated call chaining and reduce number of round trips. Security : API gateways provide a centralized proxy server to manage rate limiting, bot detection, authentication, CORS, among other things. Many API gateways allow setting up a datastore such as Redis to store session information. Cross Cutting Concerns : Logging, Caching, and other cross cutting concerns can be handled in a centralized appliances rather than deployed to every microservice. Backend for Frontend (BFF) pattern It is a variation of the API Gateway pattern. Rather than a single point of entry for the clients, it provides multiple gateways based upon the client. The purpose is to provide tailored APIs according to the needs of the client, removing a lot of bloats caused by making generic APIs for all the clients. Why BFF: Decoupling of Backend and Frontend for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs. The release of new features of one frontend does not affect the other. We can much easier maintain and modify APIs and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately. The BFF can benefit from hiding unnecessary or sensitive data before transferring it to the frontend application interface, so keys and tokens for 3rd party services can be stored and used from BFF. Allows to send formatted data to frontend and because of that can minimalize logic on it. Additionally, give us possibilities for performance improvements and good optimization for mobile.","title":"API Gateway"},{"location":"distributed-system/api_gateway/#api-gateway","text":"An API gateway is a service which is the entry point into the application from the outside world and acts as a single point of entry for a defined group of microservices. In high level It\u2019s responsible for request routing, API composition, and other functions, such as authentication. Functionalities of API Gateway: Authentication and authorization Load balancing Service discovery integration Protocol translation Response caching Retry policies, circuit breaker, and QoS Rate limiting and throttling Logging, tracing, correlation Headers, query strings, and claims transformation IP whitelisting IAM Centralized Logging (transaction ID across the servers, error logging) Identity Provider, Authentication and Authorization List of notable API Gateways: Zuul Amazon API Gateway Kong Gateway","title":"API Gateway"},{"location":"distributed-system/api_gateway/#benefits-of-api-gateways","text":"Decoupling : If your clients which you have no control over communicated directly with many separate services, renaming or moving those services can be challenging as the client is coupled to the underlying architecture and organization. API gateways enables you to route based on path, hostname, headers, and other key information enabling you to decouple the publicly facing API endpoints from the underlying microservice architecture. Reduce Round Trips : Certain API endpoints may need to join data across multiple services. API gateways can perform this aggregation so that the client doesn\u2019t not need complicated call chaining and reduce number of round trips. Security : API gateways provide a centralized proxy server to manage rate limiting, bot detection, authentication, CORS, among other things. Many API gateways allow setting up a datastore such as Redis to store session information. Cross Cutting Concerns : Logging, Caching, and other cross cutting concerns can be handled in a centralized appliances rather than deployed to every microservice.","title":"Benefits of API gateways"},{"location":"distributed-system/api_gateway/#backend-for-frontend-bff-pattern","text":"It is a variation of the API Gateway pattern. Rather than a single point of entry for the clients, it provides multiple gateways based upon the client. The purpose is to provide tailored APIs according to the needs of the client, removing a lot of bloats caused by making generic APIs for all the clients. Why BFF: Decoupling of Backend and Frontend for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs. The release of new features of one frontend does not affect the other. We can much easier maintain and modify APIs and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately. The BFF can benefit from hiding unnecessary or sensitive data before transferring it to the frontend application interface, so keys and tokens for 3rd party services can be stored and used from BFF. Allows to send formatted data to frontend and because of that can minimalize logic on it. Additionally, give us possibilities for performance improvements and good optimization for mobile.","title":"Backend for Frontend (BFF) pattern"},{"location":"distributed-system/cohesion_and_coupling/","text":"Cohesion and coupling Cohesion Cohesion is the degree to which the elements inside a microservice belong together. A microservice with high cohesion contains elements that are tightly related to each other and united in their purpose. Coupling Coupling is the degree of interdependence between microservices. When services are loosely coupled, a change to one service should not require a change to another. Type of coupling Domain coupling Domain coupling describes a situation in which one microservice needs to interact with another microservice, because the first microservice needs to make use of the functionality that the other microservice provides. In microservice, this type of interaction is largely unavoidable. A microservice-based system relies on multiple microservices collaborating in order for it to do its work. We still want to keep this to a minimum, though; whnever you see a single micorservice depending on multiple downstream services in this way, it can be a cause for concern. Pass through coupling Pass through coupling describes a situation in which one micorservice passes data to another microservice purely because the data is needed by some other micorservice further downstream. The major issue with pass-through coupling is that a change to the required data downstream can cause a more significant upstream change. Common coupling Common coupling occurs when 2 or more microservices make use of a common database. Content coupling Content coupling describes a situation in which an upstream service reaches into the internals of a downstream service and changes its internal state. The most common manifestation of this is an external service accessing another microservice's database and changing it directly. The differences between content coupling and common coupling are subtle. In both cases, 2 or more microservices are reading and writing to the same database. With common coupling, you understand that you are making use of a shared database. You know it's not under your control. With content coupling, t he lines of wonership become less clear, and it becomes more difficult for developers to change a system.","title":"Cohesion and coupling"},{"location":"distributed-system/cohesion_and_coupling/#cohesion-and-coupling","text":"","title":"Cohesion and coupling"},{"location":"distributed-system/cohesion_and_coupling/#cohesion","text":"Cohesion is the degree to which the elements inside a microservice belong together. A microservice with high cohesion contains elements that are tightly related to each other and united in their purpose.","title":"Cohesion"},{"location":"distributed-system/cohesion_and_coupling/#coupling","text":"Coupling is the degree of interdependence between microservices. When services are loosely coupled, a change to one service should not require a change to another.","title":"Coupling"},{"location":"distributed-system/cohesion_and_coupling/#type-of-coupling","text":"","title":"Type of coupling"},{"location":"distributed-system/cohesion_and_coupling/#domain-coupling","text":"Domain coupling describes a situation in which one microservice needs to interact with another microservice, because the first microservice needs to make use of the functionality that the other microservice provides. In microservice, this type of interaction is largely unavoidable. A microservice-based system relies on multiple microservices collaborating in order for it to do its work. We still want to keep this to a minimum, though; whnever you see a single micorservice depending on multiple downstream services in this way, it can be a cause for concern.","title":"Domain coupling"},{"location":"distributed-system/cohesion_and_coupling/#pass-through-coupling","text":"Pass through coupling describes a situation in which one micorservice passes data to another microservice purely because the data is needed by some other micorservice further downstream. The major issue with pass-through coupling is that a change to the required data downstream can cause a more significant upstream change.","title":"Pass through coupling"},{"location":"distributed-system/cohesion_and_coupling/#common-coupling","text":"Common coupling occurs when 2 or more microservices make use of a common database.","title":"Common coupling"},{"location":"distributed-system/cohesion_and_coupling/#content-coupling","text":"Content coupling describes a situation in which an upstream service reaches into the internals of a downstream service and changes its internal state. The most common manifestation of this is an external service accessing another microservice's database and changing it directly. The differences between content coupling and common coupling are subtle. In both cases, 2 or more microservices are reading and writing to the same database. With common coupling, you understand that you are making use of a shared database. You know it's not under your control. With content coupling, t he lines of wonership become less clear, and it becomes more difficult for developers to change a system.","title":"Content coupling"},{"location":"distributed-system/distributed_system/","text":"Distributed System 1. CAP Theorem In a distributed computer system, you can only support two of the following guarantees: Consistency: means that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated to all the other nodes in the system before the write is deemed successful Availability: means that every request gets a (non-error) response regardless of the individual state of a node. This does not guarantee that the response contains the most recent write Partition Tolerance: means that the cluster must continue to work despite any number of communication breakdowns between nodes in the system Networks aren't reliable, so you'll need to support partition tolerance (the P of CAP). That leaves a decision between the other two, C and A. When a network failure happens, one can choose to guarantee consistency or availability AP : When availability is chosen over consistency, the system is will always process the client request and try to return the most recent available version of the information even if it cannot guarantee it is up to date due to network partitioning. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. CP : When consistency is chosen over availability, the system will return an error or time-out if particular information cannot be updated to other nodes due to network partition or failures. CP is a good choice if your business needs require atomic reads and writes. Database system designed with ACID guarantees (RDBMS) usually chooses consistency over availability whereas system Designed with BASE guarantees, chooses availability over consistency. 2. Trade-offs Performance vs scalability A service is scalable if it results in increased performance in a manner proportional to the resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load. A system has to sacrifice performance for scalability. For example you have a stock exchange engine running on an API. At first you run it in one location and the performance is fast. But then you start getting customers at the other end of the world and their latency numbers are bad. If you choose to spin up and api instance that is geographically closer to those customers (scale), the latency is shortened for them but now your apis must synchronize between instances. This synchronization will cost you time and extra computation steps, lowering your performance. http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html Latency vs throughput Latency: is the time to perform some action or produce some result Network latency Processing latency Throughput: is the number of such actions executed or results produced per unit of time (number of requests that servers can handle in the certain time) Generally, you should aim for maximal throughput with acceptable latency . For example: An assembly line is manufacturing cars. It takes 8 hours to manufacture a car and that the factory produces 120 cars per day. The latency is: 8 hours. The throughput is: 120 cars / day or 5 cars / hour. Sometimes latency and throughput interfere with each other. Buses might deliver more people per hour than individually hailed cars (higher throughput), but it takes me personally longer to get downtown because I have to walk to a bus stop and wait for the bus (higher latency). Source(s) and further reading: https://medium.com/@kentbeck_7670/inefficient-efficiency-5b3ab5294791 Availability vs consistency Networks aren\u2019t reliable, so you\u2019ll need to support partition tolerance. According to CAP Theorem , you\u2019ll need to make a software tradeoff between consistency and availability. Consistency: Every read receives the most recent write or an error Availability: Every request receives a response, without guarantee that it contains the most recent version of the information 3. Consistency patterns With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error. Weak consistency After a write, reads may or may not see it. A best effort approach is taken. This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss. Eventual consistency After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously . This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems. Strong consistency After a write, reads will see it. Data is replicated synchronously . This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.","title":"Distributed System"},{"location":"distributed-system/distributed_system/#distributed-system","text":"","title":"Distributed System"},{"location":"distributed-system/distributed_system/#1-cap-theorem","text":"In a distributed computer system, you can only support two of the following guarantees: Consistency: means that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated to all the other nodes in the system before the write is deemed successful Availability: means that every request gets a (non-error) response regardless of the individual state of a node. This does not guarantee that the response contains the most recent write Partition Tolerance: means that the cluster must continue to work despite any number of communication breakdowns between nodes in the system Networks aren't reliable, so you'll need to support partition tolerance (the P of CAP). That leaves a decision between the other two, C and A. When a network failure happens, one can choose to guarantee consistency or availability AP : When availability is chosen over consistency, the system is will always process the client request and try to return the most recent available version of the information even if it cannot guarantee it is up to date due to network partitioning. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. CP : When consistency is chosen over availability, the system will return an error or time-out if particular information cannot be updated to other nodes due to network partition or failures. CP is a good choice if your business needs require atomic reads and writes. Database system designed with ACID guarantees (RDBMS) usually chooses consistency over availability whereas system Designed with BASE guarantees, chooses availability over consistency.","title":"1. CAP Theorem"},{"location":"distributed-system/distributed_system/#2-trade-offs","text":"","title":"2. Trade-offs"},{"location":"distributed-system/distributed_system/#performance-vs-scalability","text":"A service is scalable if it results in increased performance in a manner proportional to the resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load. A system has to sacrifice performance for scalability. For example you have a stock exchange engine running on an API. At first you run it in one location and the performance is fast. But then you start getting customers at the other end of the world and their latency numbers are bad. If you choose to spin up and api instance that is geographically closer to those customers (scale), the latency is shortened for them but now your apis must synchronize between instances. This synchronization will cost you time and extra computation steps, lowering your performance. http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html","title":"Performance vs scalability"},{"location":"distributed-system/distributed_system/#latency-vs-throughput","text":"Latency: is the time to perform some action or produce some result Network latency Processing latency Throughput: is the number of such actions executed or results produced per unit of time (number of requests that servers can handle in the certain time) Generally, you should aim for maximal throughput with acceptable latency . For example: An assembly line is manufacturing cars. It takes 8 hours to manufacture a car and that the factory produces 120 cars per day. The latency is: 8 hours. The throughput is: 120 cars / day or 5 cars / hour. Sometimes latency and throughput interfere with each other. Buses might deliver more people per hour than individually hailed cars (higher throughput), but it takes me personally longer to get downtown because I have to walk to a bus stop and wait for the bus (higher latency). Source(s) and further reading: https://medium.com/@kentbeck_7670/inefficient-efficiency-5b3ab5294791","title":"Latency vs throughput"},{"location":"distributed-system/distributed_system/#availability-vs-consistency","text":"Networks aren\u2019t reliable, so you\u2019ll need to support partition tolerance. According to CAP Theorem , you\u2019ll need to make a software tradeoff between consistency and availability. Consistency: Every read receives the most recent write or an error Availability: Every request receives a response, without guarantee that it contains the most recent version of the information","title":"Availability vs consistency"},{"location":"distributed-system/distributed_system/#3-consistency-patterns","text":"With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error.","title":"3. Consistency patterns"},{"location":"distributed-system/distributed_system/#weak-consistency","text":"After a write, reads may or may not see it. A best effort approach is taken. This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.","title":"Weak consistency"},{"location":"distributed-system/distributed_system/#eventual-consistency","text":"After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously . This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.","title":"Eventual consistency"},{"location":"distributed-system/distributed_system/#strong-consistency","text":"After a write, reads will see it. Data is replicated synchronously . This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.","title":"Strong consistency"},{"location":"distributed-system/microservice_communication/","text":"Micro-service communication Communication types Synchronous A microservice makes a call to another microservice and blocks operation waiting for the response. Advantages Simple Disadvantages Temporal coupling may occur Vulnerable to cascading issues caused by downstream outages Asynchronous The act of sending a call out over the network doesn\u2019t block the microservice issuing the call. It is able to carry on with any other processing without having to wait for a response. Advantages Good when dealing with time-consuming tasks Avoid temporal coupling Loosely coupling Disadvantages Complexity Communication styles Request/response communication A microservice sends a request to another microservice asking for something to be done. It expects to receive a response informing it of the result. Event driven Microservices emit events, which other microservices consume and react to accordingly. The microservice emitting the event is unaware of which microservices, if any, consume the events it emits. Common data Not often seen as a communication style, microservices collaborate via some shared data source. Technology choices REST REST (Representational State Transfer) is an architectural style that provides guidelines for designing web APIs. REST itself doesn\u2019t really talk about underlying protocols, although it is most commonly used over HTTP. We can implement REST using different protocols, although this can require a lot of work. Some of the features that HTTP gives us as part of the specification, such as verbs, make implementing REST over HTTP easier, whereas with other protocols you\u2019ll have to handle these features yourself. Where to use: REST-over-HTTP-based API is an obvious choice for a synchronous request-response interface if you are looking to allow access from as wide a variety of clients as possible Remote procedure call Remote procedure call (RPC) refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different RPC implementations in use. Most of the technology in this space requires an explicit schema, such as SOAP or gRPC . In this note, we only focus on gRPC gRPC stands for Google Remote Procedure Call and is a variant based on the RPC architecture. This technology follows an RPC API's implementation that uses HTTP 2.0 protocol gRPC uses Protocol Buffer by default to serialize payload data. REST vs gRPC Guidelines vs. Rules REST is a set of guidelines for designing web APIs without enforcing anything. gRPC enforces rules by defining a .proto file that must be adhered to by both client and server for data exchange. Underlying HTTP Protocol REST provides a request-response communication model built on the HTTP 1.1 protocol. Therefore, when multiple requests reach the server, it is bound to handle each of them, one at a time which consequently slows the entire system. gRPC follows a client-response model of communication for designing web APIs that rely on HTTP/2. Hence, gRPC allows streaming communication and serves multiple requests simultaneously. In addition to that, gRPC also supports unary communication similar to REST. Data Exchange Format REST typically uses JSON and XML formats for data transfer gRPC relies on Protobuf for an exchange of data over the HTTP/2 protocol. The gRPC is based on binary protocol-buffer format which is very lightweight compared to text-based formats such as JSON. For example, for below payload JSON encoding takes 81 bytes and the same payload in protocol-buffers takes 33 bytes Serialization vs. Strong Typing REST, in most cases, uses JSON or XML that requires serialization and conversion into the target programming language for both client and server, thereby increasing response time and the possibility of errors while parsing the request/response. gRPC provides strongly typed messages automatically converted using the Protobuf exchange format to the chosen programming language. Latency REST utilizing HTTP 1.1 requires a TCP handshake for each request. Hence, REST APIs with HTTP 1.1 can suffer from latency issues. gRPC relies on HTTP/2 protocol, which uses multiplexed streams. Therefore, several clients can send multiple requests simultaneously without establishing a new TCP connection for each one. Also, the server can send push notifications to clients via the established connection. Browser Support REST APIs on HTTP 1.1 have universal browser support. gRPC has limited browser support because numerous browsers Code Generation Features REST provides no built-in code generation features. However, we can use third-party tools like Swagger or Postman to produce code for API requests. gRPC, using its protoc compiler, comes with native code generation features, compatible with several programming languages. Where to use: Great choice for inter-process communication in microservices applications (Not only the gRPC services are faster compared to RESTful services but also they are strongly typed) IoT systems that require light-weight message transmission Mobile applications with no browser support Applications that need multiplexed streams. GraphQL GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more. With a REST API, you would typically gather the data by accessing multiple endpoints. In the example, these could be /users/<id> endpoint to fetch the initial user data. Secondly, there\u2019s likely to be a /users/<id>/posts endpoint that returns all the posts for a user. The third endpoint will then be the /users/<id>/followers that returns a list of followers per user. In GraphQL on the other hand, you\u2019d simply send a single query to the GraphQL server that includes the concrete data requirements. The server then responds with a JSON object where these requirements are fulfilled. Where to use: Apps for devices such as mobile phones, smartwatches, and IoT devices, where bandwidth usage matters. Applications where nested data needs to be fetched in a single call. (For example, a blog or social networking platform where posts need to be fetched along with nested comments and commenters details.) Composite pattern, where application retrieves data from multiple, different storage APIs (For example, a dashboard that fetches data from multiple sources such as logging services, backends for consumption stats, third-party analytics tools to capture end-user interactions.) Message brokers Message brokers are programs that enables services to communicate with each other and exchange information. Point-to-Point (Queues) Publish and Subscribe (Topics) Where to use: Long-running tasks Data post-processing","title":"Microservice communication"},{"location":"distributed-system/microservice_communication/#micro-service-communication","text":"","title":"Micro-service communication"},{"location":"distributed-system/microservice_communication/#communication-types","text":"","title":"Communication types"},{"location":"distributed-system/microservice_communication/#synchronous","text":"A microservice makes a call to another microservice and blocks operation waiting for the response.","title":"Synchronous"},{"location":"distributed-system/microservice_communication/#advantages","text":"Simple","title":"Advantages"},{"location":"distributed-system/microservice_communication/#disadvantages","text":"Temporal coupling may occur Vulnerable to cascading issues caused by downstream outages","title":"Disadvantages"},{"location":"distributed-system/microservice_communication/#asynchronous","text":"The act of sending a call out over the network doesn\u2019t block the microservice issuing the call. It is able to carry on with any other processing without having to wait for a response.","title":"Asynchronous"},{"location":"distributed-system/microservice_communication/#advantages_1","text":"Good when dealing with time-consuming tasks Avoid temporal coupling Loosely coupling","title":"Advantages"},{"location":"distributed-system/microservice_communication/#disadvantages_1","text":"Complexity","title":"Disadvantages"},{"location":"distributed-system/microservice_communication/#communication-styles","text":"","title":"Communication styles"},{"location":"distributed-system/microservice_communication/#requestresponse-communication","text":"A microservice sends a request to another microservice asking for something to be done. It expects to receive a response informing it of the result.","title":"Request/response communication"},{"location":"distributed-system/microservice_communication/#event-driven","text":"Microservices emit events, which other microservices consume and react to accordingly. The microservice emitting the event is unaware of which microservices, if any, consume the events it emits.","title":"Event driven"},{"location":"distributed-system/microservice_communication/#common-data","text":"Not often seen as a communication style, microservices collaborate via some shared data source.","title":"Common data"},{"location":"distributed-system/microservice_communication/#technology-choices","text":"","title":"Technology choices"},{"location":"distributed-system/microservice_communication/#rest","text":"REST (Representational State Transfer) is an architectural style that provides guidelines for designing web APIs. REST itself doesn\u2019t really talk about underlying protocols, although it is most commonly used over HTTP. We can implement REST using different protocols, although this can require a lot of work. Some of the features that HTTP gives us as part of the specification, such as verbs, make implementing REST over HTTP easier, whereas with other protocols you\u2019ll have to handle these features yourself. Where to use: REST-over-HTTP-based API is an obvious choice for a synchronous request-response interface if you are looking to allow access from as wide a variety of clients as possible","title":"REST"},{"location":"distributed-system/microservice_communication/#remote-procedure-call","text":"Remote procedure call (RPC) refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different RPC implementations in use. Most of the technology in this space requires an explicit schema, such as SOAP or gRPC . In this note, we only focus on gRPC gRPC stands for Google Remote Procedure Call and is a variant based on the RPC architecture. This technology follows an RPC API's implementation that uses HTTP 2.0 protocol gRPC uses Protocol Buffer by default to serialize payload data.","title":"Remote procedure call"},{"location":"distributed-system/microservice_communication/#rest-vs-grpc","text":"Guidelines vs. Rules REST is a set of guidelines for designing web APIs without enforcing anything. gRPC enforces rules by defining a .proto file that must be adhered to by both client and server for data exchange. Underlying HTTP Protocol REST provides a request-response communication model built on the HTTP 1.1 protocol. Therefore, when multiple requests reach the server, it is bound to handle each of them, one at a time which consequently slows the entire system. gRPC follows a client-response model of communication for designing web APIs that rely on HTTP/2. Hence, gRPC allows streaming communication and serves multiple requests simultaneously. In addition to that, gRPC also supports unary communication similar to REST. Data Exchange Format REST typically uses JSON and XML formats for data transfer gRPC relies on Protobuf for an exchange of data over the HTTP/2 protocol. The gRPC is based on binary protocol-buffer format which is very lightweight compared to text-based formats such as JSON. For example, for below payload JSON encoding takes 81 bytes and the same payload in protocol-buffers takes 33 bytes Serialization vs. Strong Typing REST, in most cases, uses JSON or XML that requires serialization and conversion into the target programming language for both client and server, thereby increasing response time and the possibility of errors while parsing the request/response. gRPC provides strongly typed messages automatically converted using the Protobuf exchange format to the chosen programming language. Latency REST utilizing HTTP 1.1 requires a TCP handshake for each request. Hence, REST APIs with HTTP 1.1 can suffer from latency issues. gRPC relies on HTTP/2 protocol, which uses multiplexed streams. Therefore, several clients can send multiple requests simultaneously without establishing a new TCP connection for each one. Also, the server can send push notifications to clients via the established connection. Browser Support REST APIs on HTTP 1.1 have universal browser support. gRPC has limited browser support because numerous browsers Code Generation Features REST provides no built-in code generation features. However, we can use third-party tools like Swagger or Postman to produce code for API requests. gRPC, using its protoc compiler, comes with native code generation features, compatible with several programming languages. Where to use: Great choice for inter-process communication in microservices applications (Not only the gRPC services are faster compared to RESTful services but also they are strongly typed) IoT systems that require light-weight message transmission Mobile applications with no browser support Applications that need multiplexed streams.","title":"REST vs gRPC"},{"location":"distributed-system/microservice_communication/#graphql","text":"GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more. With a REST API, you would typically gather the data by accessing multiple endpoints. In the example, these could be /users/<id> endpoint to fetch the initial user data. Secondly, there\u2019s likely to be a /users/<id>/posts endpoint that returns all the posts for a user. The third endpoint will then be the /users/<id>/followers that returns a list of followers per user. In GraphQL on the other hand, you\u2019d simply send a single query to the GraphQL server that includes the concrete data requirements. The server then responds with a JSON object where these requirements are fulfilled. Where to use: Apps for devices such as mobile phones, smartwatches, and IoT devices, where bandwidth usage matters. Applications where nested data needs to be fetched in a single call. (For example, a blog or social networking platform where posts need to be fetched along with nested comments and commenters details.) Composite pattern, where application retrieves data from multiple, different storage APIs (For example, a dashboard that fetches data from multiple sources such as logging services, backends for consumption stats, third-party analytics tools to capture end-user interactions.)","title":"GraphQL"},{"location":"distributed-system/microservice_communication/#message-brokers","text":"Message brokers are programs that enables services to communicate with each other and exchange information.","title":"Message brokers"},{"location":"distributed-system/microservice_communication/#point-to-point-queues","text":"","title":"Point-to-Point (Queues)"},{"location":"distributed-system/microservice_communication/#publish-and-subscribe-topics","text":"Where to use: Long-running tasks Data post-processing","title":"Publish and Subscribe (Topics)"},{"location":"distributed-system/service_discovery/","text":"Service Discovery Why Use Service Discovery? Let\u2019s imagine that you are writing some code that invokes a service that has a REST API. In order to make a request, your code needs to know the network location (IP address and port) of a service instance. In a modern, cloud\u2011based microservices application, however, this is a much more difficult problem to solve as shown in the following diagram. Service instances have dynamically assigned network locations. Moreover, the set of service instances changes dynamically because of autoscaling, failures, and upgrades. Consequently, your client code needs to use a more elaborate service discovery mechanism. The Client\u2011Side Discovery Pattern When using client\u2011side discovery, the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry, which is a database of available service instances. The client then uses a load\u2011balancing algorithm to select one of the available service instances and makes a request. The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism. Netflix Eureka is a service registry. It provides a REST API for managing service\u2011instance registration and for querying available instances. Netflix Ribbon is an IPC client that works with Eureka to load balance requests across the available service instances Pros: Since the client knows about the available services instances, it can make intelligent, application\u2011specific load\u2011balancing decisions such as using hashing consistently. Cons: It couples the client with the service registry Must implement client\u2011side service discovery logic in each programming language and framework used by your service The Server\u2011Side Discovery Pattern The client makes a request to a service via a load balancer. The load balancer queries the service registry and routes each request to an available service instance. As with client\u2011side discovery, service instances are registered and deregistered with the service registry. The AWS Elastic Load Balancer is an example of a server-side discovery router. An ELB is commonly used to load balance external traffic from the Internet. However, you can also use an ELB to load balance traffic that is internal to a virtual private cloud (VPC). A client makes requests (HTTP or TCP) via the ELB using its DNS name. The ELB load balances the traffic among a set of registered Elastic Compute Cloud (EC2) instances or EC2 Container Service (ECS) containers. There isn\u2019t a separate service registry. Instead, EC2 instances and ECS containers are registered with the ELB itself. Pros: Details of discovery are abstracted away from the client (Clients simply make requests to the load balancer.) Unless the load balancer is provided by the deployment environment, it is yet another highly available system component that you need to set up and manage. The Service Registry The service registry is a key part of service discovery. It is a database containing the network locations of service instances. A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances. Netflix Eureka is good example of a service registry. It provides a REST API for registering and querying service instances. A service instance registers its network location using a POST request. Every 30 seconds it must refresh its registration using a PUT request. A registration is removed by either using an HTTP DELETE request or by the instance registration timing out. A client can retrieve the registered service instances by using an HTTP GET request. Other examples of service registries include: etcd - A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Kubernetes is using etcd Apache Zookeeper - A widely used, high\u2011performance coordination service for distributed applications. consul Service Registration Options The Self\u2011Registration Pattern When using the self\u2011registration pattern, a service instance is responsible for registering and deregistering itself with the service registry. Also, if required, a service instance sends heartbeat requests to prevent its registration from expiring. A good example of this approach is the Netflix OSS Eureka client Pros: Simple and does not require any other system components Cons: It couples the service instances to the service registry Must implement the registration code in each programming language and framework used by your service The Third\u2011Party Registration Pattern When using the third-party registration pattern, service instances aren\u2019t responsible for registering themselves with the service registry. Instead, another system component known as the service registrar handles the registration. The service registrar tracks changes to the set of running instances by either polling the deployment environment or subscribing to events. When it notices a newly available service instance it registers the instance with the service registry. The service registrar also deregisters terminated service instances. The service registrar is a built\u2011in component of deployment environments. The EC2 instances created by an Autoscaling Group can be automatically registered with an ELB. Kubernetes services are automatically registered and made available for discovery. Pros: Services are decoupled from service registry No need to implement service\u2011registration logic for each programming language and framework used by your developers Cons: Unless it\u2019s built into the deployment environment, it is yet another highly available system component that you need to set up and manage.","title":"Service discovery"},{"location":"distributed-system/service_discovery/#service-discovery","text":"","title":"Service Discovery"},{"location":"distributed-system/service_discovery/#why-use-service-discovery","text":"Let\u2019s imagine that you are writing some code that invokes a service that has a REST API. In order to make a request, your code needs to know the network location (IP address and port) of a service instance. In a modern, cloud\u2011based microservices application, however, this is a much more difficult problem to solve as shown in the following diagram. Service instances have dynamically assigned network locations. Moreover, the set of service instances changes dynamically because of autoscaling, failures, and upgrades. Consequently, your client code needs to use a more elaborate service discovery mechanism.","title":"Why Use Service Discovery?"},{"location":"distributed-system/service_discovery/#the-clientside-discovery-pattern","text":"When using client\u2011side discovery, the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry, which is a database of available service instances. The client then uses a load\u2011balancing algorithm to select one of the available service instances and makes a request. The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism. Netflix Eureka is a service registry. It provides a REST API for managing service\u2011instance registration and for querying available instances. Netflix Ribbon is an IPC client that works with Eureka to load balance requests across the available service instances Pros: Since the client knows about the available services instances, it can make intelligent, application\u2011specific load\u2011balancing decisions such as using hashing consistently. Cons: It couples the client with the service registry Must implement client\u2011side service discovery logic in each programming language and framework used by your service","title":"The Client\u2011Side Discovery Pattern"},{"location":"distributed-system/service_discovery/#the-serverside-discovery-pattern","text":"The client makes a request to a service via a load balancer. The load balancer queries the service registry and routes each request to an available service instance. As with client\u2011side discovery, service instances are registered and deregistered with the service registry. The AWS Elastic Load Balancer is an example of a server-side discovery router. An ELB is commonly used to load balance external traffic from the Internet. However, you can also use an ELB to load balance traffic that is internal to a virtual private cloud (VPC). A client makes requests (HTTP or TCP) via the ELB using its DNS name. The ELB load balances the traffic among a set of registered Elastic Compute Cloud (EC2) instances or EC2 Container Service (ECS) containers. There isn\u2019t a separate service registry. Instead, EC2 instances and ECS containers are registered with the ELB itself. Pros: Details of discovery are abstracted away from the client (Clients simply make requests to the load balancer.) Unless the load balancer is provided by the deployment environment, it is yet another highly available system component that you need to set up and manage.","title":"The Server\u2011Side Discovery Pattern"},{"location":"distributed-system/service_discovery/#the-service-registry","text":"The service registry is a key part of service discovery. It is a database containing the network locations of service instances. A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances. Netflix Eureka is good example of a service registry. It provides a REST API for registering and querying service instances. A service instance registers its network location using a POST request. Every 30 seconds it must refresh its registration using a PUT request. A registration is removed by either using an HTTP DELETE request or by the instance registration timing out. A client can retrieve the registered service instances by using an HTTP GET request. Other examples of service registries include: etcd - A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Kubernetes is using etcd Apache Zookeeper - A widely used, high\u2011performance coordination service for distributed applications. consul","title":"The Service Registry"},{"location":"distributed-system/service_discovery/#service-registration-options","text":"","title":"Service Registration Options"},{"location":"distributed-system/service_discovery/#the-selfregistration-pattern","text":"When using the self\u2011registration pattern, a service instance is responsible for registering and deregistering itself with the service registry. Also, if required, a service instance sends heartbeat requests to prevent its registration from expiring. A good example of this approach is the Netflix OSS Eureka client Pros: Simple and does not require any other system components Cons: It couples the service instances to the service registry Must implement the registration code in each programming language and framework used by your service","title":"The Self\u2011Registration Pattern"},{"location":"distributed-system/service_discovery/#the-thirdparty-registration-pattern","text":"When using the third-party registration pattern, service instances aren\u2019t responsible for registering themselves with the service registry. Instead, another system component known as the service registrar handles the registration. The service registrar tracks changes to the set of running instances by either polling the deployment environment or subscribing to events. When it notices a newly available service instance it registers the instance with the service registry. The service registrar also deregisters terminated service instances. The service registrar is a built\u2011in component of deployment environments. The EC2 instances created by an Autoscaling Group can be automatically registered with an ELB. Kubernetes services are automatically registered and made available for discovery. Pros: Services are decoupled from service registry No need to implement service\u2011registration logic for each programming language and framework used by your developers Cons: Unless it\u2019s built into the deployment environment, it is yet another highly available system component that you need to set up and manage.","title":"The Third\u2011Party Registration Pattern"},{"location":"distributed-system/service_mesh/","text":"Service Mesh A service mesh is a dedicated infrastructure layer that adds features to a network between services: - Service to service communication (service discovery and encryption) - Observability (monitoring and tracking) - Resiliency (circuit breakers and retries) Without a service mesh : each microservice implements business logic and cross cutting concerns (CCC) by itself. With a service mesh : many CCCs like traffic metrics, routing, and encryption are moved out of the microservice and into a proxy. Service Mesh Implementations Istio Linkerd Consul","title":"Service Mesh"},{"location":"distributed-system/service_mesh/#service-mesh","text":"A service mesh is a dedicated infrastructure layer that adds features to a network between services: - Service to service communication (service discovery and encryption) - Observability (monitoring and tracking) - Resiliency (circuit breakers and retries) Without a service mesh : each microservice implements business logic and cross cutting concerns (CCC) by itself. With a service mesh : many CCCs like traffic metrics, routing, and encryption are moved out of the microservice and into a proxy.","title":"Service Mesh"},{"location":"distributed-system/service_mesh/#service-mesh-implementations","text":"Istio Linkerd Consul","title":"Service Mesh Implementations"},{"location":"programming/git/git_commands/","text":"Git commands 1. Configuration Check git configuration 1 git config -l Setup git username, email Repository configuration 1 2 git config user.name \"Huy Duong\" git config user.email \"huy.duongdinh@gmail.com\" Global configuration 1 2 git config --global user.name \"Huy Duong\" git config --global user.email \"huy.duongdinh@gmail.com\" 2. Branch List branches Local 1 git branch Remote 1 git branch -r Rename branch Local 1 git branch -m <new_branch_name> Remote 1 2 git push origin --delete <old_branch_name> git push origin -u <new_branch_name> Delete branch Local 1 git branch -d <branch_name> Remote 1 git push origin --delete <branch_name> Create branch 1 git branch -b <branch_name> 3. Revert Revert specific commit 1 git revert [--no-commit] <commit_id> Revert file to specific commit 1 git checkout <commit_id> -- <file> 4. Rebase Rebase interactive mode 1 git rebase -i <branch_name> 5. Submodule Add submodule 1 2 3 git submodule add <remote_url> git submodule add <remote_url> <folder> Update submodule 1 git submodule update --init --recursive","title":"Git commands"},{"location":"programming/git/git_commands/#git-commands","text":"","title":"Git commands"},{"location":"programming/git/git_commands/#1-configuration","text":"","title":"1. Configuration"},{"location":"programming/git/git_commands/#check-git-configuration","text":"1 git config -l","title":"Check git configuration"},{"location":"programming/git/git_commands/#setup-git-username-email","text":"","title":"Setup git username, email"},{"location":"programming/git/git_commands/#repository-configuration","text":"1 2 git config user.name \"Huy Duong\" git config user.email \"huy.duongdinh@gmail.com\"","title":"Repository configuration"},{"location":"programming/git/git_commands/#global-configuration","text":"1 2 git config --global user.name \"Huy Duong\" git config --global user.email \"huy.duongdinh@gmail.com\"","title":"Global configuration"},{"location":"programming/git/git_commands/#2-branch","text":"","title":"2. Branch"},{"location":"programming/git/git_commands/#list-branches","text":"","title":"List branches"},{"location":"programming/git/git_commands/#local","text":"1 git branch","title":"Local"},{"location":"programming/git/git_commands/#remote","text":"1 git branch -r","title":"Remote"},{"location":"programming/git/git_commands/#rename-branch","text":"","title":"Rename branch"},{"location":"programming/git/git_commands/#local_1","text":"1 git branch -m <new_branch_name>","title":"Local"},{"location":"programming/git/git_commands/#remote_1","text":"1 2 git push origin --delete <old_branch_name> git push origin -u <new_branch_name>","title":"Remote"},{"location":"programming/git/git_commands/#delete-branch","text":"","title":"Delete branch"},{"location":"programming/git/git_commands/#local_2","text":"1 git branch -d <branch_name>","title":"Local"},{"location":"programming/git/git_commands/#remote_2","text":"1 git push origin --delete <branch_name>","title":"Remote"},{"location":"programming/git/git_commands/#create-branch","text":"1 git branch -b <branch_name>","title":"Create branch"},{"location":"programming/git/git_commands/#3-revert","text":"","title":"3. Revert"},{"location":"programming/git/git_commands/#revert-specific-commit","text":"1 git revert [--no-commit] <commit_id>","title":"Revert specific commit"},{"location":"programming/git/git_commands/#revert-file-to-specific-commit","text":"1 git checkout <commit_id> -- <file>","title":"Revert file to specific commit"},{"location":"programming/git/git_commands/#4-rebase","text":"","title":"4. Rebase"},{"location":"programming/git/git_commands/#rebase-interactive-mode","text":"1 git rebase -i <branch_name>","title":"Rebase interactive mode"},{"location":"programming/git/git_commands/#5-submodule","text":"","title":"5. Submodule"},{"location":"programming/git/git_commands/#add-submodule","text":"1 2 3 git submodule add <remote_url> git submodule add <remote_url> <folder>","title":"Add submodule"},{"location":"programming/git/git_commands/#update-submodule","text":"1 git submodule update --init --recursive","title":"Update submodule"},{"location":"programming/golang/array_and_slice/","text":"Array and slice 1. Array Arrays are fixed-length sequences of items of the same type. Arrays in Go can be created using the following syntaxes: 1 2 3 [ N ] Type [ N ] Type { value1 , value2 , ... , valueN } [ ... ] Type { value1 , value2 , ... , valueN } For example, the type [4]int represents an array of four integers. 2. Slice What is slice A Slice is a segment of an array. Slices build on arrays and provide more power, flexibility, and convenience compared to arrays. Just like arrays, Slices are indexable and have a length. But unlike arrays, they can be resized. Slices can be created using the following syntaxes: 1 2 3 4 make ([] Type , length , capacity ) make ([] Type , length ) [] Type {} [] Type { value1 , value2 , ... , valueN } A slice of type T is declared using []T . The slice is declared just like an array except that we do not specify any size in the brackets [] . Since a slice is a segment of an array, we can create a slice from an array. To create a slice from an array a, we specify two indices low (lower bound) and high (upper bound) separated by a colon - 1 a [ low : high ] The above expression selects a slice from the array a . The resulting slice includes all the elements starting from index low to high , but excluding the element at index high . The low and high indices in the slice expression are optional. The default value for low is 0 , and high is the length of the slice. A slice can also be created by slicing an existing slice. Modifying the slice Slices are reference types. They refer to an underlying array. Modifying the elements of a slice will modify the corresponding elements in the referenced array. Other slices that refer the same array will also see those modifications. Length and Capacity of a Slice A slice consists of three things - A pointer (reference) to an underlying array. The length of the segment of the array that the slice contains. The capacity (the maximum size up to which the segment can grow). 1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } Let\u2019s consider the following array and the slice obtained from it as an example - 1 2 var a = [ 6 ] int { 10 , 20 , 30 , 40 , 50 , 60 } var s = [ 1 : 4 ] Here is how the slice s in the above example is represented - The length of the slice is the number of elements in the slice, which is 3 in the above example. The capacity is the number of elements in the underlying array starting from the first element in the slice. It is 5 in the above example. You can find the length and capacity of a slice using the built-in functions len() and cap() Zero value of slices The zero value of a slice is nil. A nil slice doesn\u2019t have any underlying array, and has a length and capacity of 0 1 2 3 4 5 6 7 8 9 10 11 package main import \"fmt\" func main () { var s [] int fmt . Printf ( \"s = %v, len = %d, cap = %d\\n\" , s , len ( s ), cap ( s )) if s == nil { fmt . Println ( \"s is nil\" ) } } Output: 1 2 s = [] , len = 0 , cap = 0 s is nil Slice functions copy The copy() function copies elements from one slice to another. Its signature looks like this - 1 func copy ( dst , src [] T ) int It takes two slices - a destination slice, and a source slice. It then copies elements from the source to the destination and returns the number of elements that are copied. The number of elements copied will be the minimum of len(src) and len(dst). append The append() function appends new elements at the end of a given slice. Following is the signature of append function. 1 func append ( s [] T , x ... T ) [] T It takes a slice and a variable number of arguments x \u2026T. It then returns a new slice containing all the elements from the given slice as well as the new elements. If the given slice doesn\u2019t have sufficient capacity to accommodate new elements then a new underlying array is allocated with bigger capacity. All the elements from the underlying array of the existing slice are copied to this new array, and then the new elements are appended. However, if the slice has enough capacity to accommodate new elements, then the append() function re-uses its underlying array and appends new elements to the same array. Iterating over a slice Using for loop 1 2 3 4 5 6 7 8 9 10 package main import \"fmt\" func main () { countries := [] string { \"India\" , \"America\" , \"Russia\" , \"England\" } for i := 0 ; i < len ( countries ); i ++ { fmt . Println ( countries [ i ]) } } Using range 1 2 3 4 5 6 7 8 9 10 package main import \"fmt\" func main () { primeNumbers := [] int { 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 } for index , number := range primeNumbers { fmt . Printf ( \"PrimeNumber(%d) = %d\\n\" , index + 1 , number ) } }","title":"Array and slice"},{"location":"programming/golang/array_and_slice/#array-and-slice","text":"","title":"Array and slice"},{"location":"programming/golang/array_and_slice/#1-array","text":"Arrays are fixed-length sequences of items of the same type. Arrays in Go can be created using the following syntaxes: 1 2 3 [ N ] Type [ N ] Type { value1 , value2 , ... , valueN } [ ... ] Type { value1 , value2 , ... , valueN } For example, the type [4]int represents an array of four integers.","title":"1. Array"},{"location":"programming/golang/array_and_slice/#2-slice","text":"","title":"2. Slice"},{"location":"programming/golang/array_and_slice/#what-is-slice","text":"A Slice is a segment of an array. Slices build on arrays and provide more power, flexibility, and convenience compared to arrays. Just like arrays, Slices are indexable and have a length. But unlike arrays, they can be resized. Slices can be created using the following syntaxes: 1 2 3 4 make ([] Type , length , capacity ) make ([] Type , length ) [] Type {} [] Type { value1 , value2 , ... , valueN } A slice of type T is declared using []T . The slice is declared just like an array except that we do not specify any size in the brackets [] . Since a slice is a segment of an array, we can create a slice from an array. To create a slice from an array a, we specify two indices low (lower bound) and high (upper bound) separated by a colon - 1 a [ low : high ] The above expression selects a slice from the array a . The resulting slice includes all the elements starting from index low to high , but excluding the element at index high . The low and high indices in the slice expression are optional. The default value for low is 0 , and high is the length of the slice. A slice can also be created by slicing an existing slice.","title":"What is slice"},{"location":"programming/golang/array_and_slice/#modifying-the-slice","text":"Slices are reference types. They refer to an underlying array. Modifying the elements of a slice will modify the corresponding elements in the referenced array. Other slices that refer the same array will also see those modifications.","title":"Modifying the slice"},{"location":"programming/golang/array_and_slice/#length-and-capacity-of-a-slice","text":"A slice consists of three things - A pointer (reference) to an underlying array. The length of the segment of the array that the slice contains. The capacity (the maximum size up to which the segment can grow). 1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } Let\u2019s consider the following array and the slice obtained from it as an example - 1 2 var a = [ 6 ] int { 10 , 20 , 30 , 40 , 50 , 60 } var s = [ 1 : 4 ] Here is how the slice s in the above example is represented - The length of the slice is the number of elements in the slice, which is 3 in the above example. The capacity is the number of elements in the underlying array starting from the first element in the slice. It is 5 in the above example. You can find the length and capacity of a slice using the built-in functions len() and cap()","title":"Length and Capacity of a Slice"},{"location":"programming/golang/array_and_slice/#zero-value-of-slices","text":"The zero value of a slice is nil. A nil slice doesn\u2019t have any underlying array, and has a length and capacity of 0 1 2 3 4 5 6 7 8 9 10 11 package main import \"fmt\" func main () { var s [] int fmt . Printf ( \"s = %v, len = %d, cap = %d\\n\" , s , len ( s ), cap ( s )) if s == nil { fmt . Println ( \"s is nil\" ) } } Output: 1 2 s = [] , len = 0 , cap = 0 s is nil","title":"Zero value of slices"},{"location":"programming/golang/array_and_slice/#slice-functions","text":"","title":"Slice functions"},{"location":"programming/golang/array_and_slice/#copy","text":"The copy() function copies elements from one slice to another. Its signature looks like this - 1 func copy ( dst , src [] T ) int It takes two slices - a destination slice, and a source slice. It then copies elements from the source to the destination and returns the number of elements that are copied. The number of elements copied will be the minimum of len(src) and len(dst).","title":"copy"},{"location":"programming/golang/array_and_slice/#append","text":"The append() function appends new elements at the end of a given slice. Following is the signature of append function. 1 func append ( s [] T , x ... T ) [] T It takes a slice and a variable number of arguments x \u2026T. It then returns a new slice containing all the elements from the given slice as well as the new elements. If the given slice doesn\u2019t have sufficient capacity to accommodate new elements then a new underlying array is allocated with bigger capacity. All the elements from the underlying array of the existing slice are copied to this new array, and then the new elements are appended. However, if the slice has enough capacity to accommodate new elements, then the append() function re-uses its underlying array and appends new elements to the same array.","title":"append"},{"location":"programming/golang/array_and_slice/#iterating-over-a-slice","text":"","title":"Iterating over a slice"},{"location":"programming/golang/array_and_slice/#using-for-loop","text":"1 2 3 4 5 6 7 8 9 10 package main import \"fmt\" func main () { countries := [] string { \"India\" , \"America\" , \"Russia\" , \"England\" } for i := 0 ; i < len ( countries ); i ++ { fmt . Println ( countries [ i ]) } }","title":"Using for loop"},{"location":"programming/golang/array_and_slice/#using-range","text":"1 2 3 4 5 6 7 8 9 10 package main import \"fmt\" func main () { primeNumbers := [] int { 2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 } for index , number := range primeNumbers { fmt . Printf ( \"PrimeNumber(%d) = %d\\n\" , index + 1 , number ) } }","title":"Using range"},{"location":"programming/golang/context/","text":"Context 1. Introduction In Go servers, each incoming request is handled in its own goroutine. Request handlers often start additional goroutines to access backends such as databases and RPC services. The set of goroutines working on a request typically needs access to request-specific values such as the identity of the end user, authorization tokens, and the request's deadline. When a request is canceled or times out, all the goroutines working on that request should exit quickly so the system can reclaim any resources they are using. A context package makes it easy to pass request-scoped values, cancelation signals, and deadlines across API boundaries to all the goroutines involved in handling a request. A context package will help you in following problems: Let\u2019s say that you started a function and you need to pass some common parameters to the downstream functions. You cannot pass these common parameters each as an argument to all the downstream functions. You started a goroutine which in turn start more goroutines and so on. Suppose the task that you were doing is no longer needed. Then how to inform all child goroutines to gracefully exit so that resources can be freed up. A task should be finished within a specified timeout of say 2 seconds. If not it should gracefully exit or return. A task should be finished within a deadline eg it should end before 5 pm. If not finished then it should gracefully exit and return. Use cases: To pass data to the downstream. Eg. a HTTP request creates a request_id , request_user which needs to be passed around to all downstream functions for distributed tracing. When you want to halt the operation in the midway \u2013 A HTTP request should be stopped because the client disconnected. When you want to halt the operation within a specified time from start i.e with timeout \u2013 Eg- a HTTP request should be completed in 2 sec or else should be aborted. When you want to halt an operation before a certain time \u2013 Eg. A cron is running that needs to be aborted in 5 mins if not completed. 2. Context The core of the context package is the Context type: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this Context is canceled // or times out. Done () <- chan struct {} // Err indicates why this context was canceled, after the Done channel // is closed. Err () error // Deadline returns the time when this Context will be canceled, if any. Deadline () ( deadline time . Time , ok bool ) // Value returns the value associated with key or nil if none. Value ( key interface {}) interface {} } 3. Derived contexts The context package provides functions to derive new Context values from existing ones. These values form a tree: when a Context is canceled, all Contexts derived from it are also canceled. Background is the root of any Context tree; it is never canceled: 1 2 3 4 // Background returns an empty Context. It is never canceled, has no deadline, // and has no values. Background is typically used in main, init, and tests, // and as the top-level Context for incoming requests. func Background () Context TODO is a non-nil, empty Context. Code should use context.TODO when it's unclear which Context to use or it is not yet available 1 2 3 4 5 // TODO returns a non-nil, empty Context. Code should use context.TODO when // it's unclear which Context to use or it is not yet available (because the // surrounding function has not yet been extended to accept a Context // parameter). func TODO () Context A derived context is can be created in 4 ways: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // A CancelFunc cancels a Context. type CancelFunc func () // WithCancel returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed or cancel is called. func WithCancel ( parent Context ) ( ctx Context , cancel CancelFunc ) // WithTimeout returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed, cancel is called, or timeout elapses. The new // Context's Deadline is the sooner of now+timeout and the parent's deadline, if // any. If the timer is still running, the cancel function releases its // resources. func WithTimeout ( parent Context , timeout time . Duration ) ( Context , CancelFunc ) // WithValue returns a copy of parent whose Value method returns val for key. func WithValue ( parent Context , key interface {}, val interface {}) Context // WithDeadline returns a copy of the parent context with the deadline adjusted to be no later than d func WithDeadline ( parent Context , d time . Time ) ( Context , CancelFunc ) 4. Context examples We need cancellation to prevent our system from doing unnecessary work. Consider the common situation of an HTTP server making a call to a database, and returning the queried data to the client: The timing diagram, if everything worked perfectly, would look like this: But, what would happen if the client cancelled the request in the middle? This could happen if, for example, the client closed their browser mid-request. Without cancellation, the application server and database would continue to do their work, even though the result of that work would be wasted: Ideally, we would want all downstream components of a process to halt, if we know that the process (in this example, the HTTP request) halted: For example, lets consider an HTTP server that takes two seconds to process an event. If the request gets cancelled before that, we want to return immediately: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main () { // Create an HTTP server that listens on port 8000 http . ListenAndServe ( \":8000\" , http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { ctx := r . Context () // This prints to STDOUT to show that processing has started fmt . Fprint ( os . Stdout , \"processing request\\n\" ) // We use `select` to execute a piece of code depending on which // channel receives a message first select { case <- time . After ( 2 * time . Second ): // If we receive a message after 2 seconds // that means the request has been processed // We then write this as the response w . Write ([] byte ( \"request processed\" )) case <- ctx . Done (): // If the request gets cancelled, log it // to STDERR fmt . Fprint ( os . Stderr , \"request cancelled\\n\" ) } })) } An example using WithCancel context 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \"context\" \"fmt\" \"time\" ) func task ( ctx context . Context ) { i := 1 for { select { case <- ctx . Done (): fmt . Println ( \"Gracefully exit\" ) fmt . Println ( ctx . Err ()) return default : fmt . Println ( i ) time . Sleep ( time . Second * 1 ) i ++ } } } func main () { ctx := context . Background () cancelCtx , cancelFunc := context . WithCancel ( ctx ) go task ( cancelCtx ) time . Sleep ( time . Second * 3 ) cancelFunc () time . Sleep ( time . Second * 1 ) } An example using WithValue context 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package main import ( \"context\" \"github.com/google/uuid\" \"net/http\" ) //HelloWorld hellow world handler func HelloWorld ( w http . ResponseWriter , r * http . Request ) { msgID := \"\" if m := r . Context (). Value ( \"msgId\" ); m != nil { if value , ok := m .( string ); ok { msgID = value } } w . Header (). Add ( \"msgId\" , msgID ) w . Write ([] byte ( \"Hello, world\" )) } func inejctMsgID ( next http . Handler ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { msgID := uuid . New (). String () ctx := context . WithValue ( r . Context (), \"msgId\" , msgID ) req := r . WithContext ( ctx ) next . ServeHTTP ( w , req ) }) } func main () { helloWorldHandler := http . HandlerFunc ( HelloWorld ) http . Handle ( \"/welcome\" , inejctMsgID ( helloWorldHandler )) http . ListenAndServe ( \":8080\" , nil ) } 5. BestPractices and Caveats Do not store a context within a struct type Context should flow through your program. For example, in case of an HTTP request, a new context can be created for each incoming request which can be used to hold a request_id or put some common information in the context like currently logged in user which might be useful for that particular request. Always pass context as the first argument to a function. Whenever you are not sure whether to use the context or not, it is better to use the context.ToDo() as a placeholder. Only the parent goroutine or function should the cancel context. Therefore do not pass the cancelFunc to downstream goroutines or functions. Golang will allow you to pass the cancelFunc around to child goroutines but it is not a recommended practice","title":"Context"},{"location":"programming/golang/context/#context","text":"","title":"Context"},{"location":"programming/golang/context/#1-introduction","text":"In Go servers, each incoming request is handled in its own goroutine. Request handlers often start additional goroutines to access backends such as databases and RPC services. The set of goroutines working on a request typically needs access to request-specific values such as the identity of the end user, authorization tokens, and the request's deadline. When a request is canceled or times out, all the goroutines working on that request should exit quickly so the system can reclaim any resources they are using. A context package makes it easy to pass request-scoped values, cancelation signals, and deadlines across API boundaries to all the goroutines involved in handling a request. A context package will help you in following problems: Let\u2019s say that you started a function and you need to pass some common parameters to the downstream functions. You cannot pass these common parameters each as an argument to all the downstream functions. You started a goroutine which in turn start more goroutines and so on. Suppose the task that you were doing is no longer needed. Then how to inform all child goroutines to gracefully exit so that resources can be freed up. A task should be finished within a specified timeout of say 2 seconds. If not it should gracefully exit or return. A task should be finished within a deadline eg it should end before 5 pm. If not finished then it should gracefully exit and return. Use cases: To pass data to the downstream. Eg. a HTTP request creates a request_id , request_user which needs to be passed around to all downstream functions for distributed tracing. When you want to halt the operation in the midway \u2013 A HTTP request should be stopped because the client disconnected. When you want to halt the operation within a specified time from start i.e with timeout \u2013 Eg- a HTTP request should be completed in 2 sec or else should be aborted. When you want to halt an operation before a certain time \u2013 Eg. A cron is running that needs to be aborted in 5 mins if not completed.","title":"1. Introduction"},{"location":"programming/golang/context/#2-context","text":"The core of the context package is the Context type: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this Context is canceled // or times out. Done () <- chan struct {} // Err indicates why this context was canceled, after the Done channel // is closed. Err () error // Deadline returns the time when this Context will be canceled, if any. Deadline () ( deadline time . Time , ok bool ) // Value returns the value associated with key or nil if none. Value ( key interface {}) interface {} }","title":"2. Context"},{"location":"programming/golang/context/#3-derived-contexts","text":"The context package provides functions to derive new Context values from existing ones. These values form a tree: when a Context is canceled, all Contexts derived from it are also canceled. Background is the root of any Context tree; it is never canceled: 1 2 3 4 // Background returns an empty Context. It is never canceled, has no deadline, // and has no values. Background is typically used in main, init, and tests, // and as the top-level Context for incoming requests. func Background () Context TODO is a non-nil, empty Context. Code should use context.TODO when it's unclear which Context to use or it is not yet available 1 2 3 4 5 // TODO returns a non-nil, empty Context. Code should use context.TODO when // it's unclear which Context to use or it is not yet available (because the // surrounding function has not yet been extended to accept a Context // parameter). func TODO () Context A derived context is can be created in 4 ways: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // A CancelFunc cancels a Context. type CancelFunc func () // WithCancel returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed or cancel is called. func WithCancel ( parent Context ) ( ctx Context , cancel CancelFunc ) // WithTimeout returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed, cancel is called, or timeout elapses. The new // Context's Deadline is the sooner of now+timeout and the parent's deadline, if // any. If the timer is still running, the cancel function releases its // resources. func WithTimeout ( parent Context , timeout time . Duration ) ( Context , CancelFunc ) // WithValue returns a copy of parent whose Value method returns val for key. func WithValue ( parent Context , key interface {}, val interface {}) Context // WithDeadline returns a copy of the parent context with the deadline adjusted to be no later than d func WithDeadline ( parent Context , d time . Time ) ( Context , CancelFunc )","title":"3. Derived contexts"},{"location":"programming/golang/context/#4-context-examples","text":"We need cancellation to prevent our system from doing unnecessary work. Consider the common situation of an HTTP server making a call to a database, and returning the queried data to the client: The timing diagram, if everything worked perfectly, would look like this: But, what would happen if the client cancelled the request in the middle? This could happen if, for example, the client closed their browser mid-request. Without cancellation, the application server and database would continue to do their work, even though the result of that work would be wasted: Ideally, we would want all downstream components of a process to halt, if we know that the process (in this example, the HTTP request) halted: For example, lets consider an HTTP server that takes two seconds to process an event. If the request gets cancelled before that, we want to return immediately: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func main () { // Create an HTTP server that listens on port 8000 http . ListenAndServe ( \":8000\" , http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { ctx := r . Context () // This prints to STDOUT to show that processing has started fmt . Fprint ( os . Stdout , \"processing request\\n\" ) // We use `select` to execute a piece of code depending on which // channel receives a message first select { case <- time . After ( 2 * time . Second ): // If we receive a message after 2 seconds // that means the request has been processed // We then write this as the response w . Write ([] byte ( \"request processed\" )) case <- ctx . Done (): // If the request gets cancelled, log it // to STDERR fmt . Fprint ( os . Stderr , \"request cancelled\\n\" ) } })) } An example using WithCancel context 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \"context\" \"fmt\" \"time\" ) func task ( ctx context . Context ) { i := 1 for { select { case <- ctx . Done (): fmt . Println ( \"Gracefully exit\" ) fmt . Println ( ctx . Err ()) return default : fmt . Println ( i ) time . Sleep ( time . Second * 1 ) i ++ } } } func main () { ctx := context . Background () cancelCtx , cancelFunc := context . WithCancel ( ctx ) go task ( cancelCtx ) time . Sleep ( time . Second * 3 ) cancelFunc () time . Sleep ( time . Second * 1 ) } An example using WithValue context 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package main import ( \"context\" \"github.com/google/uuid\" \"net/http\" ) //HelloWorld hellow world handler func HelloWorld ( w http . ResponseWriter , r * http . Request ) { msgID := \"\" if m := r . Context (). Value ( \"msgId\" ); m != nil { if value , ok := m .( string ); ok { msgID = value } } w . Header (). Add ( \"msgId\" , msgID ) w . Write ([] byte ( \"Hello, world\" )) } func inejctMsgID ( next http . Handler ) http . Handler { return http . HandlerFunc ( func ( w http . ResponseWriter , r * http . Request ) { msgID := uuid . New (). String () ctx := context . WithValue ( r . Context (), \"msgId\" , msgID ) req := r . WithContext ( ctx ) next . ServeHTTP ( w , req ) }) } func main () { helloWorldHandler := http . HandlerFunc ( HelloWorld ) http . Handle ( \"/welcome\" , inejctMsgID ( helloWorldHandler )) http . ListenAndServe ( \":8080\" , nil ) }","title":"4. Context examples"},{"location":"programming/golang/context/#5-bestpractices-and-caveats","text":"Do not store a context within a struct type Context should flow through your program. For example, in case of an HTTP request, a new context can be created for each incoming request which can be used to hold a request_id or put some common information in the context like currently logged in user which might be useful for that particular request. Always pass context as the first argument to a function. Whenever you are not sure whether to use the context or not, it is better to use the context.ToDo() as a placeholder. Only the parent goroutine or function should the cancel context. Therefore do not pass the cancelFunc to downstream goroutines or functions. Golang will allow you to pass the cancelFunc around to child goroutines but it is not a recommended practice","title":"5. BestPractices and Caveats"},{"location":"programming/golang/goroutine/","text":"Goroutine 1. Concurrency vs Parallelism Concurrency is when two or more tasks can start, run, and complete in overlapping time periods. It doesn't necessarily mean they'll ever both be running at the same instant. Parallelism is when tasks literally run at the same time, e.g., on a multicore processor. 2. Data races and race conditions Data races is a situation, in which at least two threads access a shared variable at the same time. At least one thread tries to modify the variable. Race condition is a situation, in which the result of an operation depends on the interleaving of certain individual operations. 3. Deadlocks What is a deadlock? A deadlock occurs when all processes are blocked while waiting for each other and the program cannot proceed further. Coffman conditions There are four conditions, knows as the Coffman conditions that must be present simultaneously for a deadlock to occur: Mutual exclusion: A concurrent process holds at least one resource at any time making it non-sharable Hold and wait: A concurrent process holds a resource and is waiting for an additional resource No preemption: A resource held by a concurrent process cannot be taken away by the system. It can only be freed by the process holding it. Circle wait: A concurrent process must be waiting on a chain of other concurrent processes such that P1 is waiting on P2, P2 on P3 and so on, and there exists a Pn which is waiting for P1. In order to prevent deadlocks, we need to make sure that at least one of the conditions stated above should not hold. 3. Starvation Starvation describes a situation where a thread is unable to gain regular access to shared resources and is unable to make progress. This happens when shared resources are made unavailable for long periods by greedy threads. For example, suppose an object provides a synchronized method that often takes a long time to return. If one thread invokes this method frequently, other threads that also need frequent synchronized access to the same object will often be blocked. 4. Goroutine A goroutine is a lightweight thread managed by the Go runtime. You can create a goroutine by using the following syntax 1 go f ( x , y , z ) The current goroutine evaluates the input parameters to the function which are executed in the new goroutine. main() function is a goroutine which was invoked by the implicity created goroutine managed by Go runtime. 5. Channels Channel is a pipe between goroutines to synchronize excution and communicate by sending/receiving data 1 channelName := make ( chan datatype ) The datatype is the type of data that you will pass on your channel. Eg: 1 channelName := make ( chan int ) Sending on a channel 1 channelName <- data Receiving on a channel 1 data := <- channelName By default, sends and receives block until the other side is ready. This allows goroutines to synchronize without explicit locks or condition variables. Closing a channel 1 close ( channelName ) A sender can close a channel to indicate that no more values will be sent. Receivers can test whether a channel has been closed by assigning a second parameter to the receive expression: 1 v , ok := <- ch ok is false if there are no more values to receive and the channel is closed. The loop for i := range c receives values from the channel repeatedly until it is closed. 6. Buffered channels Buffered channels are channels with a capacity/buffer. They can created with the following syntax: 1 channelName := make ( chan datatype , capacity ) Sends to a buffered channel block only when the buffer is full . Receives block when the buffer is empty . 7. Select The select statement lets a goroutine wait on multiple communication operations. A select blocks until one of its cases can run, then it executes that case. It chooses one at random if multiple are ready. 1 2 3 4 5 6 select { case mess1 := <- channel1 : fmt . Println ( mess1 ) case mess2 := <- channel2 : fmt . Println ( mess2 ) } Default selection The default case in a select is run if no other case is ready. 1 2 3 4 5 6 select { case mess := <- channel : fmt . Println ( mess ) default : time . Sleep ( 50 * time . Millisecond ) } Empty select 1 select {} The empty select will block forever as there is no case statement to execute. It is similar to an empty for {} statement. On most supported Go architectures, the empty select will yield CPU. An empty for-loop won't, i.e. it will \"spin\" on 100% CPU. Select statement with timeout Timeout in select can be achieved by using After() function of time package. Below is the signature of After() function. 1 func After ( d Duration ) <- chan Time The After function waits for d duration to finish and then it returns the current time on a channel. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \"fmt\" \"time\" ) func main () { news := make ( chan string ) go newsFeed ( news ) printAllNews ( news ) } func printAllNews ( news chan string ) { for { select { case n := <- news : fmt . Println ( n ) case <- time . After ( time . Second * 1 ): fmt . Println ( \"Timeout: News feed finished\" ) return } } } func newsFeed ( ch chan string ) { for i := 0 ; i < 2 ; i ++ { time . Sleep ( time . Millisecond * 400 ) ch <- fmt . Sprintf ( \"News: %d\" , i + 1 ) } } 8. WaitGroups A WaitGroup blocks a program an waits for a set of goroutines to finish before moving to the next steps of excutions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \"fmt\" \"sync\" ) func main () { waitgroup := new ( sync . WaitGroup ) waitgroup . Add ( 2 ) go func () { fmt . Println ( \"Hello world 1\" ) waitgroup . Done () }() go func () { fmt . Println ( \"Hello world 2\" ) waitgroup . Done () }() waitgroup . Wait () fmt . Println ( \"Finished Execution\" ) } 9. Mutex Critical section When a program runs concurrently, the parts of code which modify shared resources should not be accessed by multiple Goroutines at the same time. This section of code that modifies shared resources is called critical section What is mutex A mutex prevents other processes from entering a critical section of data while a process occupies it. Go's standard library provides mutual exclusion with sync.Mutex and its two methods: Lock Unlock RWMutex A RWMutex is a reader/writer mutual exclusion lock. The lock can be held by an arbitrary number of readers or a single writer. Lock : locks for writing. If the lock is already locked for reading or writing, Lock blocks until the lock is available. Unlock : unlocks writing lock. RLock : locks for reading. It should not be used for recursive read locking; a blocked Lock call excludes new readers from acquiring the lock. RUnlock : RUnlock undoes a single RLock call; it does not affect other simultaneous readers. 9. Once, Pool, Cond Once Once is an object that performs an action only once. Implement a singleton pattern in Go: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \"fmt\" \"sync\" ) type DbConnection struct {} var ( dbConnOnce sync . Once conn * DbConnection ) func GetConnection () * DbConnection { dbConnOnce . Do ( func () { conn = & DbConnection {} fmt . Println ( \"Inside\" ) }) fmt . Println ( \"Outside\" ) return conn } func main () { for i := 0 ; i < 5 ; i ++ { _ = GetConnection () /* Result is ... Inside Outside Outside Outside Outside Outside */ } } Pool A Pool is a set of temporary objects that may be individually saved and retrieved. Pool's purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. The public methods are: Get() interface{} to retrieve an element Put(interface{}) to add an element 1 2 3 4 5 6 7 pool := & sync . Pool { New : func () interface {} { return NewConnection () }, } connection := pool . Get ().( * Connection ) When shall we use sync.Pool? There are two use-cases: The first one is when we have to reuse shared and long-live objects like a DB connection for example. The second one is to optimize memory allocation . Eg: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func writeFile ( pool * sync . Pool , filename string ) error { // Gets a buffer object buf := pool . Get ().( * bytes . Buffer ) // Returns the buffer into the pool defer pool . Put ( buf ) // Reset buffer otherwise it will contain \"foo\" during the first call // Then \"foofoo\" etc. buf . Reset () buf . WriteString ( \"foo\" ) return ioutil . WriteFile ( filename , buf . Bytes (), 0644 ) } Note: Since a pointer can be put into the interface value returned by Get() without any allocation, it is preferable to put pointers than structures in the pool. Cond Cond implements a condition variable, a rendezvous point for goroutines waiting for or announcing the occurrence of an event. Creating a sync.Cond requires a sync.Locker object (either a sync.Mutex or a sync.RWMutex): 1 cond := sync . NewCond ( & sync . RWMutex {}) 10. Concurrency patterns Generator function Generator Pattern is used to generate a sequence of values which is used to produce some output. This pattern is widely used to introduce parallelism into loops. This allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the next value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import \"fmt\" // Generator func which produces data which might be computationally expensive. func fib ( n int ) chan int { c := make ( chan int ) go func () { for i , j := 0 , 1 ; i < n ; i , j = i + j , i { c <- i } close ( c ) }() return c } func main () { // fib returns the fibonacci numbers lesser than 1000 for i := range fib ( 1000 ) { // Consumer which consumes the data produced by the generator, which further does some extra computations v := i * i fmt . Println ( v ) } } Futures A Future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) type data struct { Body [] byte Error error } func futureData ( url string ) <- chan data { c := make ( chan data , 1 ) go func () { var body [] byte var err error resp , err := http . Get ( url ) defer resp . Body . Close () body , err = ioutil . ReadAll ( resp . Body ) c <- data { Body : body , Error : err } }() return c } func main () { future := futureData ( \"http://test.future.com\" ) // do many other things body := <- future fmt . Printf ( \"response: %#v\" , string ( body . Body )) fmt . Printf ( \"error: %#v\" , body . Error ) } The actual http request is done asynchronously in a goroutine. The main function can continue doing other things. When the result is needed, we read the result from the channel. Fan-in, Fan-out Fan-in Fan-out is a way of Multiplexing and Demultiplexing in golang. Fan-in refers to processing multiple input data and combining into a single entity. Fan-out is the exact opposite, dividing the data into multiple smaller chunks, distributing the work amongst a group of workers to parallelize CPU use and I/O. For example we have a following program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \"fmt\" \"math/rand\" \"time\" ) func updatePosition ( name string ) <- chan string { positionChannel := make ( chan string ) go func () { for i := 0 ; ; i ++ { positionChannel <- fmt . Sprintf ( \"%s %d\" , name , i ) time . Sleep ( time . Duration ( rand . Intn ( 1e3 )) * time . Millisecond ) } }() return positionChannel } func main () { positionChannel1 := updatePosition ( \"Huy\" ) positionChannel2 := updatePosition ( \"Duong\" ) for i := 0 ; i < 5 ; i ++ { fmt . Println ( <- positionChannel1 ) fmt . Println ( <- positionChannel2 ) } } What if the data in positionChannel2 come first? It needs to wait for the data in positionChannel1. What if we want to get position updates as soon as the data is ready? This is where fan-in comes into play. By using this technique, we'll combine the inputs from both channels and send them through a single channel. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package main import ( \"fmt\" \"math/rand\" \"time\" ) func updatePosition(name string) <-chan string { positionChannel := make(chan string) go func() { for i := 0; ; i++ { positionChannel <- fmt.Sprintf(\"%s %d\", name, i) time.Sleep(time.Duration(rand.Intn(1e3)) * time.Millisecond) } }() return positionChannel } func fanIn(chan1, chan2 <-chan string) <-chan string { channel := make(chan string) go func() { for { channel <- <-chan1 } }() go func() { for { channel <- <-chan2 } }() return channel } func main() { positionChannel := fanIn(updatePosition(\"Huy\"), updatePosition(\"Duong\")) for i := 0; i < 10; i++ { fmt.Println(<-positionChannel) } } Another example using generator , fan-in and fan-out pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package main import ( \"fmt\" ) func main () { randomNumbers := [] int { 13 , 44 , 56 , 99 , 9 , 45 , 67 , 90 , 78 , 23 } // generate the common channel with inputs inputChan := generatePipeline ( randomNumbers ) // Fan-out to 2 Go-routine c1 := squareNumber ( inputChan ) c2 := squareNumber ( inputChan ) // Fan-in the resulting squared numbers c := fanIn ( c1 , c2 ) sum := 0 // Do the summation for i := 0 ; i < len ( randomNumbers ); i ++ { sum += <- c } fmt . Printf ( \"Total Sum of Squares: %d\" , sum ) } func generatePipeline ( numbers [] int ) <- chan int { out := make ( chan int ) go func () { for _ , n := range numbers { out <- n } close ( out ) }() return out } func squareNumber ( in <- chan int ) <- chan int { out := make ( chan int ) go func () { for n := range in { out <- n * n } close ( out ) }() return out } func fanIn ( input1 , input2 <- chan int ) <- chan int { c := make ( chan int ) go func () { for { select { case s := <- input1 : c <- s case s := <- input2 : c <- s } } }() return c } Sequencing Imagine a cooking competition. You are participating in it with your partner. The rule of the game are: There are 3 rounds in the competition. In each round both partners will have to to come up with their own dishes. A player can not move to the next round until their partner is done with their dish. The judge will decide the entry to the next round after tasting food from both the team members. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 package main import ( \"fmt\" \"math/rand\" \"time\" ) type CookInfo struct { foodCooked string waitForPartner chan bool } func cookFood ( name string ) <- chan CookInfo { cookChannel := make ( chan CookInfo ) waitForPartner := make ( chan bool ) go func () { for round := 0 ; round < 3 ; round ++ { time . Sleep ( time . Duration ( rand . Intn ( 1e3 )) * time . Millisecond ) cookChannel <- CookInfo { fmt . Sprintf ( \"%s %s\\n\" , name , \"Done\" ), waitForPartner } <- waitForPartner } }() return cookChannel } func fanIn ( input1 , input2 <- chan CookInfo ) <- chan CookInfo { c := make ( chan CookInfo ) go func () { for { select { case s := <- input1 : c <- s case s := <- input2 : c <- s } } }() return c } func main () { channel := fanIn ( cookFood ( \"Player 1\" ), cookFood ( \"Player 2\" )) for round := 0 ; round < 3 ; round ++ { food1 := <- channel fmt . Printf ( food1 . foodCooked ) food2 := <- channel fmt . Printf ( food2 . foodCooked ) food1 . waitForPartner <- true food2 . waitForPartner <- true fmt . Printf ( \"Done with round %d\\n\" , round + 1 ) } fmt . Printf ( \"Done with the competition\\n\" ) }","title":"Goroutine"},{"location":"programming/golang/goroutine/#goroutine","text":"","title":"Goroutine"},{"location":"programming/golang/goroutine/#1-concurrency-vs-parallelism","text":"Concurrency is when two or more tasks can start, run, and complete in overlapping time periods. It doesn't necessarily mean they'll ever both be running at the same instant. Parallelism is when tasks literally run at the same time, e.g., on a multicore processor.","title":"1. Concurrency vs Parallelism"},{"location":"programming/golang/goroutine/#2-data-races-and-race-conditions","text":"Data races is a situation, in which at least two threads access a shared variable at the same time. At least one thread tries to modify the variable. Race condition is a situation, in which the result of an operation depends on the interleaving of certain individual operations.","title":"2. Data races and race conditions"},{"location":"programming/golang/goroutine/#3-deadlocks","text":"","title":"3. Deadlocks"},{"location":"programming/golang/goroutine/#what-is-a-deadlock","text":"A deadlock occurs when all processes are blocked while waiting for each other and the program cannot proceed further.","title":"What is a deadlock?"},{"location":"programming/golang/goroutine/#coffman-conditions","text":"There are four conditions, knows as the Coffman conditions that must be present simultaneously for a deadlock to occur: Mutual exclusion: A concurrent process holds at least one resource at any time making it non-sharable Hold and wait: A concurrent process holds a resource and is waiting for an additional resource No preemption: A resource held by a concurrent process cannot be taken away by the system. It can only be freed by the process holding it. Circle wait: A concurrent process must be waiting on a chain of other concurrent processes such that P1 is waiting on P2, P2 on P3 and so on, and there exists a Pn which is waiting for P1. In order to prevent deadlocks, we need to make sure that at least one of the conditions stated above should not hold.","title":"Coffman conditions"},{"location":"programming/golang/goroutine/#3-starvation","text":"Starvation describes a situation where a thread is unable to gain regular access to shared resources and is unable to make progress. This happens when shared resources are made unavailable for long periods by greedy threads. For example, suppose an object provides a synchronized method that often takes a long time to return. If one thread invokes this method frequently, other threads that also need frequent synchronized access to the same object will often be blocked.","title":"3. Starvation"},{"location":"programming/golang/goroutine/#4-goroutine","text":"A goroutine is a lightweight thread managed by the Go runtime. You can create a goroutine by using the following syntax 1 go f ( x , y , z ) The current goroutine evaluates the input parameters to the function which are executed in the new goroutine. main() function is a goroutine which was invoked by the implicity created goroutine managed by Go runtime.","title":"4. Goroutine"},{"location":"programming/golang/goroutine/#5-channels","text":"Channel is a pipe between goroutines to synchronize excution and communicate by sending/receiving data 1 channelName := make ( chan datatype ) The datatype is the type of data that you will pass on your channel. Eg: 1 channelName := make ( chan int )","title":"5. Channels"},{"location":"programming/golang/goroutine/#sending-on-a-channel","text":"1 channelName <- data","title":"Sending on a channel"},{"location":"programming/golang/goroutine/#receiving-on-a-channel","text":"1 data := <- channelName By default, sends and receives block until the other side is ready. This allows goroutines to synchronize without explicit locks or condition variables.","title":"Receiving on a channel"},{"location":"programming/golang/goroutine/#closing-a-channel","text":"1 close ( channelName ) A sender can close a channel to indicate that no more values will be sent. Receivers can test whether a channel has been closed by assigning a second parameter to the receive expression: 1 v , ok := <- ch ok is false if there are no more values to receive and the channel is closed. The loop for i := range c receives values from the channel repeatedly until it is closed.","title":"Closing a channel"},{"location":"programming/golang/goroutine/#6-buffered-channels","text":"Buffered channels are channels with a capacity/buffer. They can created with the following syntax: 1 channelName := make ( chan datatype , capacity ) Sends to a buffered channel block only when the buffer is full . Receives block when the buffer is empty .","title":"6. Buffered channels"},{"location":"programming/golang/goroutine/#7-select","text":"The select statement lets a goroutine wait on multiple communication operations. A select blocks until one of its cases can run, then it executes that case. It chooses one at random if multiple are ready. 1 2 3 4 5 6 select { case mess1 := <- channel1 : fmt . Println ( mess1 ) case mess2 := <- channel2 : fmt . Println ( mess2 ) }","title":"7. Select"},{"location":"programming/golang/goroutine/#default-selection","text":"The default case in a select is run if no other case is ready. 1 2 3 4 5 6 select { case mess := <- channel : fmt . Println ( mess ) default : time . Sleep ( 50 * time . Millisecond ) }","title":"Default selection"},{"location":"programming/golang/goroutine/#empty-select","text":"1 select {} The empty select will block forever as there is no case statement to execute. It is similar to an empty for {} statement. On most supported Go architectures, the empty select will yield CPU. An empty for-loop won't, i.e. it will \"spin\" on 100% CPU.","title":"Empty select"},{"location":"programming/golang/goroutine/#select-statement-with-timeout","text":"Timeout in select can be achieved by using After() function of time package. Below is the signature of After() function. 1 func After ( d Duration ) <- chan Time The After function waits for d duration to finish and then it returns the current time on a channel. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \"fmt\" \"time\" ) func main () { news := make ( chan string ) go newsFeed ( news ) printAllNews ( news ) } func printAllNews ( news chan string ) { for { select { case n := <- news : fmt . Println ( n ) case <- time . After ( time . Second * 1 ): fmt . Println ( \"Timeout: News feed finished\" ) return } } } func newsFeed ( ch chan string ) { for i := 0 ; i < 2 ; i ++ { time . Sleep ( time . Millisecond * 400 ) ch <- fmt . Sprintf ( \"News: %d\" , i + 1 ) } }","title":"Select statement with timeout"},{"location":"programming/golang/goroutine/#8-waitgroups","text":"A WaitGroup blocks a program an waits for a set of goroutines to finish before moving to the next steps of excutions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \"fmt\" \"sync\" ) func main () { waitgroup := new ( sync . WaitGroup ) waitgroup . Add ( 2 ) go func () { fmt . Println ( \"Hello world 1\" ) waitgroup . Done () }() go func () { fmt . Println ( \"Hello world 2\" ) waitgroup . Done () }() waitgroup . Wait () fmt . Println ( \"Finished Execution\" ) }","title":"8. WaitGroups"},{"location":"programming/golang/goroutine/#9-mutex","text":"","title":"9. Mutex"},{"location":"programming/golang/goroutine/#critical-section","text":"When a program runs concurrently, the parts of code which modify shared resources should not be accessed by multiple Goroutines at the same time. This section of code that modifies shared resources is called critical section","title":"Critical section"},{"location":"programming/golang/goroutine/#what-is-mutex","text":"A mutex prevents other processes from entering a critical section of data while a process occupies it. Go's standard library provides mutual exclusion with sync.Mutex and its two methods: Lock Unlock","title":"What is mutex"},{"location":"programming/golang/goroutine/#rwmutex","text":"A RWMutex is a reader/writer mutual exclusion lock. The lock can be held by an arbitrary number of readers or a single writer. Lock : locks for writing. If the lock is already locked for reading or writing, Lock blocks until the lock is available. Unlock : unlocks writing lock. RLock : locks for reading. It should not be used for recursive read locking; a blocked Lock call excludes new readers from acquiring the lock. RUnlock : RUnlock undoes a single RLock call; it does not affect other simultaneous readers.","title":"RWMutex"},{"location":"programming/golang/goroutine/#9-once-pool-cond","text":"","title":"9. Once, Pool, Cond"},{"location":"programming/golang/goroutine/#once","text":"Once is an object that performs an action only once. Implement a singleton pattern in Go: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \"fmt\" \"sync\" ) type DbConnection struct {} var ( dbConnOnce sync . Once conn * DbConnection ) func GetConnection () * DbConnection { dbConnOnce . Do ( func () { conn = & DbConnection {} fmt . Println ( \"Inside\" ) }) fmt . Println ( \"Outside\" ) return conn } func main () { for i := 0 ; i < 5 ; i ++ { _ = GetConnection () /* Result is ... Inside Outside Outside Outside Outside Outside */ } }","title":"Once"},{"location":"programming/golang/goroutine/#pool","text":"A Pool is a set of temporary objects that may be individually saved and retrieved. Pool's purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. The public methods are: Get() interface{} to retrieve an element Put(interface{}) to add an element 1 2 3 4 5 6 7 pool := & sync . Pool { New : func () interface {} { return NewConnection () }, } connection := pool . Get ().( * Connection ) When shall we use sync.Pool? There are two use-cases: The first one is when we have to reuse shared and long-live objects like a DB connection for example. The second one is to optimize memory allocation . Eg: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func writeFile ( pool * sync . Pool , filename string ) error { // Gets a buffer object buf := pool . Get ().( * bytes . Buffer ) // Returns the buffer into the pool defer pool . Put ( buf ) // Reset buffer otherwise it will contain \"foo\" during the first call // Then \"foofoo\" etc. buf . Reset () buf . WriteString ( \"foo\" ) return ioutil . WriteFile ( filename , buf . Bytes (), 0644 ) } Note: Since a pointer can be put into the interface value returned by Get() without any allocation, it is preferable to put pointers than structures in the pool.","title":"Pool"},{"location":"programming/golang/goroutine/#cond","text":"Cond implements a condition variable, a rendezvous point for goroutines waiting for or announcing the occurrence of an event. Creating a sync.Cond requires a sync.Locker object (either a sync.Mutex or a sync.RWMutex): 1 cond := sync . NewCond ( & sync . RWMutex {})","title":"Cond"},{"location":"programming/golang/goroutine/#10-concurrency-patterns","text":"","title":"10. Concurrency patterns"},{"location":"programming/golang/goroutine/#generator-function","text":"Generator Pattern is used to generate a sequence of values which is used to produce some output. This pattern is widely used to introduce parallelism into loops. This allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the next value. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import \"fmt\" // Generator func which produces data which might be computationally expensive. func fib ( n int ) chan int { c := make ( chan int ) go func () { for i , j := 0 , 1 ; i < n ; i , j = i + j , i { c <- i } close ( c ) }() return c } func main () { // fib returns the fibonacci numbers lesser than 1000 for i := range fib ( 1000 ) { // Consumer which consumes the data produced by the generator, which further does some extra computations v := i * i fmt . Println ( v ) } }","title":"Generator function"},{"location":"programming/golang/goroutine/#futures","text":"A Future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) type data struct { Body [] byte Error error } func futureData ( url string ) <- chan data { c := make ( chan data , 1 ) go func () { var body [] byte var err error resp , err := http . Get ( url ) defer resp . Body . Close () body , err = ioutil . ReadAll ( resp . Body ) c <- data { Body : body , Error : err } }() return c } func main () { future := futureData ( \"http://test.future.com\" ) // do many other things body := <- future fmt . Printf ( \"response: %#v\" , string ( body . Body )) fmt . Printf ( \"error: %#v\" , body . Error ) } The actual http request is done asynchronously in a goroutine. The main function can continue doing other things. When the result is needed, we read the result from the channel.","title":"Futures"},{"location":"programming/golang/goroutine/#fan-in-fan-out","text":"Fan-in Fan-out is a way of Multiplexing and Demultiplexing in golang. Fan-in refers to processing multiple input data and combining into a single entity. Fan-out is the exact opposite, dividing the data into multiple smaller chunks, distributing the work amongst a group of workers to parallelize CPU use and I/O. For example we have a following program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \"fmt\" \"math/rand\" \"time\" ) func updatePosition ( name string ) <- chan string { positionChannel := make ( chan string ) go func () { for i := 0 ; ; i ++ { positionChannel <- fmt . Sprintf ( \"%s %d\" , name , i ) time . Sleep ( time . Duration ( rand . Intn ( 1e3 )) * time . Millisecond ) } }() return positionChannel } func main () { positionChannel1 := updatePosition ( \"Huy\" ) positionChannel2 := updatePosition ( \"Duong\" ) for i := 0 ; i < 5 ; i ++ { fmt . Println ( <- positionChannel1 ) fmt . Println ( <- positionChannel2 ) } } What if the data in positionChannel2 come first? It needs to wait for the data in positionChannel1. What if we want to get position updates as soon as the data is ready? This is where fan-in comes into play. By using this technique, we'll combine the inputs from both channels and send them through a single channel. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package main import ( \"fmt\" \"math/rand\" \"time\" ) func updatePosition(name string) <-chan string { positionChannel := make(chan string) go func() { for i := 0; ; i++ { positionChannel <- fmt.Sprintf(\"%s %d\", name, i) time.Sleep(time.Duration(rand.Intn(1e3)) * time.Millisecond) } }() return positionChannel } func fanIn(chan1, chan2 <-chan string) <-chan string { channel := make(chan string) go func() { for { channel <- <-chan1 } }() go func() { for { channel <- <-chan2 } }() return channel } func main() { positionChannel := fanIn(updatePosition(\"Huy\"), updatePosition(\"Duong\")) for i := 0; i < 10; i++ { fmt.Println(<-positionChannel) } } Another example using generator , fan-in and fan-out pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package main import ( \"fmt\" ) func main () { randomNumbers := [] int { 13 , 44 , 56 , 99 , 9 , 45 , 67 , 90 , 78 , 23 } // generate the common channel with inputs inputChan := generatePipeline ( randomNumbers ) // Fan-out to 2 Go-routine c1 := squareNumber ( inputChan ) c2 := squareNumber ( inputChan ) // Fan-in the resulting squared numbers c := fanIn ( c1 , c2 ) sum := 0 // Do the summation for i := 0 ; i < len ( randomNumbers ); i ++ { sum += <- c } fmt . Printf ( \"Total Sum of Squares: %d\" , sum ) } func generatePipeline ( numbers [] int ) <- chan int { out := make ( chan int ) go func () { for _ , n := range numbers { out <- n } close ( out ) }() return out } func squareNumber ( in <- chan int ) <- chan int { out := make ( chan int ) go func () { for n := range in { out <- n * n } close ( out ) }() return out } func fanIn ( input1 , input2 <- chan int ) <- chan int { c := make ( chan int ) go func () { for { select { case s := <- input1 : c <- s case s := <- input2 : c <- s } } }() return c }","title":"Fan-in, Fan-out"},{"location":"programming/golang/goroutine/#sequencing","text":"Imagine a cooking competition. You are participating in it with your partner. The rule of the game are: There are 3 rounds in the competition. In each round both partners will have to to come up with their own dishes. A player can not move to the next round until their partner is done with their dish. The judge will decide the entry to the next round after tasting food from both the team members. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 package main import ( \"fmt\" \"math/rand\" \"time\" ) type CookInfo struct { foodCooked string waitForPartner chan bool } func cookFood ( name string ) <- chan CookInfo { cookChannel := make ( chan CookInfo ) waitForPartner := make ( chan bool ) go func () { for round := 0 ; round < 3 ; round ++ { time . Sleep ( time . Duration ( rand . Intn ( 1e3 )) * time . Millisecond ) cookChannel <- CookInfo { fmt . Sprintf ( \"%s %s\\n\" , name , \"Done\" ), waitForPartner } <- waitForPartner } }() return cookChannel } func fanIn ( input1 , input2 <- chan CookInfo ) <- chan CookInfo { c := make ( chan CookInfo ) go func () { for { select { case s := <- input1 : c <- s case s := <- input2 : c <- s } } }() return c } func main () { channel := fanIn ( cookFood ( \"Player 1\" ), cookFood ( \"Player 2\" )) for round := 0 ; round < 3 ; round ++ { food1 := <- channel fmt . Printf ( food1 . foodCooked ) food2 := <- channel fmt . Printf ( food2 . foodCooked ) food1 . waitForPartner <- true food2 . waitForPartner <- true fmt . Printf ( \"Done with round %d\\n\" , round + 1 ) } fmt . Printf ( \"Done with the competition\\n\" ) }","title":"Sequencing"},{"location":"programming/kotlin/coroutine/basics/","text":"Coroutine basics A coroutine is an instance of suspendable computation. It is conceptually similar to a thread. However, a coroutine is not bound to any particular thread. It may suspend its execution in one thread and resume in another one. Coroutines can be thought of as light-weight threads, but there is a number of important differences that make their real-life usage very different from threads. Coroutine Builders Coroutine builders are a way of creating coroutines. Since they are not suspending themselves, they can be called from non-suspending code or any other piece of code. They act as a link between the suspending and non-suspending parts of our code. launch launch Launches a new coroutine without blocking the current thread and returns a reference to the coroutine as a Job. The coroutine is cancelled when the resulting job is cancelled. 1 2 3 4 5 6 7 8 fun main() { GlobalScope.launch { delay(200) println(\"World\") } println(\"Hello\") Thread.sleep(400) } The launch function is an extension function on the interface CoroutineScope . This is a part of an important mechanism called structured concurrency, with the purpose of building a relationship between parent coroutine and child coroutine. Coroutines are launched in coroutine builders and are bound by a coroutine scope. A lazy way to launch a coroutine would be to use the GlobalScope . This means that the coroutine would be running for as long as the application is running and, if there is no important reason to do this, I believe that it\u2019s a way to wrongly use resources. 1 2 3 4 5 public fun CoroutineScope.launch( context: CoroutineContext = EmptyCoroutineContext, start: CoroutineStart = CoroutineStart.DEFAULT, block: suspend CoroutineScope.() -> Unit ): Job CoroutineContext is a persistent dataset of contextual information about the current coroutine CoroutineStart is the mode in which you can start a coroutine. DEFAULT: Immediately schedules a coroutine for execution according to its context. LAZY: Starts the coroutine lazily, only when it is needed. ATOMIC: Same as DEFAULT but cannot be cancelled before it starts. UNDISPATCHED: Runs the coroutine until its first suspension point. runblocking runBlocking is a coroutine builder that blocks the current thread until all tasks of the coroutine it creates, finish 1 2 3 4 5 6 7 fun main() = runBlocking { // The method bridges the non-coroutine world and the code with coroutines launch { // launch a new coroutine without blocking the current thread delay(1000L) // non-blocking delay for 1 second (default time unit is ms) println(\"World!\") // print after delay } println(\"Hello\") // main coroutine continues while a previous one is delayed } async async is a coroutine builder which returns some value to the caller. async can be used to perform an asynchronous task which returns a value await is a suspending function that is called upon the async builder to fetch the value of the Deferred object that is returned. 1 2 3 4 5 6 7 fun main() = runBlocking { val res = async { \"Hello world\" } println(res.await()) } withContext Calls the specified suspending block with a given coroutine context, suspends until it completes, and returns the result. Structured Concurrency If a coroutine is started on GlobalScope , the program will not wait for it. For example: 1 2 3 4 5 6 7 8 9 fun main() = runBlocking { GlobalScope.launch { delay(1000L) println(\"World!\") } println(\"Hello,\") } // Hello Everything changes if we get rid of the GlobalScope here. runBlocking creates a CoroutineScope and provides it as its lambda expression receiver. Thus we can call this.launch or simply launch . As a result, launch becomes a child of runBlocking . As parents might recognize, parental responsibility is to wait for all its children, so runBlocking will suspend until all its children are finished. Scope builder CoroutineScope is an interface that has a single abstract property called coroutineContext . 1 2 3 public interface CoroutineScope { public val coroutineContext: CoroutineContext } Whenever a new coroutine scope is created, a new job gets created and gets associated with it. Every coroutine created using this scope becomes the child of this job. GlobalScope This is a singleton and not associated with any Job. This launches top-level coroutines and highly discouraged to use because if you launch coroutines in the global scope, you lose all the benefits you get from structured concurrency. Reference: https://elizarov.medium.com/the-reason-to-avoid-globalscope-835337445abc MainScope This is useful for UI components, it creates SupervisorJob and all the coroutines created with this scope runs on the Main thread. CoroutineScope(ctx) This creates a coroutine scope from provided coroutine context and makes sure that a job is associated with this scope. 1 public fun CoroutineScope(context: CoroutineContext): CoroutineScope = ContextScope(if (context[Job] != null) context else context + Job()) coroutineScope Kotlin avoid thread blocking functions inside coroutine so due to this the use of runBlocking is discouraged from inside coroutines ans allow suspending operation instead of this. The suspending function is equivalent of runBlocking is the coroutineScope builder. CoroutineScope suspends the currently working coroutine until all it\u2019s child coroutines have finished their execution. 1 2 3 4 5 6 7 8 9 10 11 fun main() = runBlocking { doWorld() } suspend fun doWorld() = coroutineScope { // this: CoroutineScope launch { delay(1000L) println(\"World!\") } println(\"Hello\") }","title":"Coroutine basics"},{"location":"programming/kotlin/coroutine/basics/#coroutine-basics","text":"A coroutine is an instance of suspendable computation. It is conceptually similar to a thread. However, a coroutine is not bound to any particular thread. It may suspend its execution in one thread and resume in another one. Coroutines can be thought of as light-weight threads, but there is a number of important differences that make their real-life usage very different from threads.","title":"Coroutine basics"},{"location":"programming/kotlin/coroutine/basics/#coroutine-builders","text":"Coroutine builders are a way of creating coroutines. Since they are not suspending themselves, they can be called from non-suspending code or any other piece of code. They act as a link between the suspending and non-suspending parts of our code.","title":"Coroutine Builders"},{"location":"programming/kotlin/coroutine/basics/#launch","text":"launch Launches a new coroutine without blocking the current thread and returns a reference to the coroutine as a Job. The coroutine is cancelled when the resulting job is cancelled. 1 2 3 4 5 6 7 8 fun main() { GlobalScope.launch { delay(200) println(\"World\") } println(\"Hello\") Thread.sleep(400) } The launch function is an extension function on the interface CoroutineScope . This is a part of an important mechanism called structured concurrency, with the purpose of building a relationship between parent coroutine and child coroutine. Coroutines are launched in coroutine builders and are bound by a coroutine scope. A lazy way to launch a coroutine would be to use the GlobalScope . This means that the coroutine would be running for as long as the application is running and, if there is no important reason to do this, I believe that it\u2019s a way to wrongly use resources. 1 2 3 4 5 public fun CoroutineScope.launch( context: CoroutineContext = EmptyCoroutineContext, start: CoroutineStart = CoroutineStart.DEFAULT, block: suspend CoroutineScope.() -> Unit ): Job CoroutineContext is a persistent dataset of contextual information about the current coroutine CoroutineStart is the mode in which you can start a coroutine. DEFAULT: Immediately schedules a coroutine for execution according to its context. LAZY: Starts the coroutine lazily, only when it is needed. ATOMIC: Same as DEFAULT but cannot be cancelled before it starts. UNDISPATCHED: Runs the coroutine until its first suspension point.","title":"launch"},{"location":"programming/kotlin/coroutine/basics/#runblocking","text":"runBlocking is a coroutine builder that blocks the current thread until all tasks of the coroutine it creates, finish 1 2 3 4 5 6 7 fun main() = runBlocking { // The method bridges the non-coroutine world and the code with coroutines launch { // launch a new coroutine without blocking the current thread delay(1000L) // non-blocking delay for 1 second (default time unit is ms) println(\"World!\") // print after delay } println(\"Hello\") // main coroutine continues while a previous one is delayed }","title":"runblocking"},{"location":"programming/kotlin/coroutine/basics/#async","text":"async is a coroutine builder which returns some value to the caller. async can be used to perform an asynchronous task which returns a value await is a suspending function that is called upon the async builder to fetch the value of the Deferred object that is returned. 1 2 3 4 5 6 7 fun main() = runBlocking { val res = async { \"Hello world\" } println(res.await()) }","title":"async"},{"location":"programming/kotlin/coroutine/basics/#withcontext","text":"Calls the specified suspending block with a given coroutine context, suspends until it completes, and returns the result.","title":"withContext"},{"location":"programming/kotlin/coroutine/basics/#structured-concurrency","text":"If a coroutine is started on GlobalScope , the program will not wait for it. For example: 1 2 3 4 5 6 7 8 9 fun main() = runBlocking { GlobalScope.launch { delay(1000L) println(\"World!\") } println(\"Hello,\") } // Hello Everything changes if we get rid of the GlobalScope here. runBlocking creates a CoroutineScope and provides it as its lambda expression receiver. Thus we can call this.launch or simply launch . As a result, launch becomes a child of runBlocking . As parents might recognize, parental responsibility is to wait for all its children, so runBlocking will suspend until all its children are finished.","title":"Structured Concurrency"},{"location":"programming/kotlin/coroutine/basics/#scope-builder","text":"CoroutineScope is an interface that has a single abstract property called coroutineContext . 1 2 3 public interface CoroutineScope { public val coroutineContext: CoroutineContext } Whenever a new coroutine scope is created, a new job gets created and gets associated with it. Every coroutine created using this scope becomes the child of this job.","title":"Scope builder"},{"location":"programming/kotlin/coroutine/basics/#globalscope","text":"This is a singleton and not associated with any Job. This launches top-level coroutines and highly discouraged to use because if you launch coroutines in the global scope, you lose all the benefits you get from structured concurrency. Reference: https://elizarov.medium.com/the-reason-to-avoid-globalscope-835337445abc","title":"GlobalScope"},{"location":"programming/kotlin/coroutine/basics/#mainscope","text":"This is useful for UI components, it creates SupervisorJob and all the coroutines created with this scope runs on the Main thread.","title":"MainScope"},{"location":"programming/kotlin/coroutine/basics/#coroutinescopectx","text":"This creates a coroutine scope from provided coroutine context and makes sure that a job is associated with this scope. 1 public fun CoroutineScope(context: CoroutineContext): CoroutineScope = ContextScope(if (context[Job] != null) context else context + Job())","title":"CoroutineScope(ctx)"},{"location":"programming/kotlin/coroutine/basics/#coroutinescope","text":"Kotlin avoid thread blocking functions inside coroutine so due to this the use of runBlocking is discouraged from inside coroutines ans allow suspending operation instead of this. The suspending function is equivalent of runBlocking is the coroutineScope builder. CoroutineScope suspends the currently working coroutine until all it\u2019s child coroutines have finished their execution. 1 2 3 4 5 6 7 8 9 10 11 fun main() = runBlocking { doWorld() } suspend fun doWorld() = coroutineScope { // this: CoroutineScope launch { delay(1000L) println(\"World!\") } println(\"Hello\") }","title":"coroutineScope"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/","text":"Coroutine context and dispatchers Coroutine context Coroutines always execute in some context represented by a value of the CoroutineContext type The coroutine context is a set of various elements: Job: A cancellable piece of work, which has a defined lifecycle ContinuationInterceptor: A mechanism which listens to the continuation within a coroutine and intercepts its resumption. CoroutineExceptionHandler: A construct which handles exceptions in coroutines. 1 2 3 4 5 6 7 8 9 fun main() { val defaultDispatcher = Dispatchers.Default val emptyParentJob = Job() val combinedContext = defaultDispatcher + emptyParentJob GlobalScope.launch(context = combinedContext) { println(Thread.currentThread().name) } Thread.sleep(50) } Dispatchers and threads The coroutine context includes a coroutine dispatcher that determines what thread or threads the corresponding coroutine uses for its execution. All coroutine builders like launch and async accept an optional CoroutineContext parameter that can be used to explicitly specify the dispatcher for the new coroutine and other context elements. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /** * When `launch { ... }` is used without parameters, it inherits the context from the `CoroutineScope` it is being launched from. * In this case, it inherits the context of the main `runBlocking` coroutine which runs in the main thread. */ launch { // context of the parent, main runBlocking coroutine println(\"main runBlocking : I'm working in thread ${Thread.currentThread().name}\") } // Dispatchers.Unconfined is a special dispatcher that also appears to run in the main thread. launch(Dispatchers.Unconfined) { // not confined -- will work with main thread println(\"Unconfined : I'm working in thread ${Thread.currentThread().name}\") } launch(Dispatchers.Default) { // will get dispatched to DefaultDispatcher println(\"Default : I'm working in thread ${Thread.currentThread().name}\") } launch(newSingleThreadContext(\"MyOwnThread\")) { // will get its own new thread println(\"newSingleThreadContext: I'm working in thread ${Thread.currentThread().name}\") } It produces the following output (maybe in different order): 1 2 3 4 Unconfined : I'm working in thread main Default : I'm working in thread DefaultDispatcher-worker-1 newSingleThreadContext: I'm working in thread MyOwnThread main runBlocking : I'm working in thread main Thread used Max number of threads Useful when Dispatchers.Default thread pool Number of CPU cores Executing CPU-bound code Dispatchers.IO thread pool Defined in IO_PARALLELISM_PROPERTY_NAME. The default value is max(64, number of cpu cores) Executing IO-heavy code Dispatchers.Main main 1 Working with UI elements executorService.asCoroutineDispatcher() thread pool Defined by ExecuteService config Need to limit number of threads that the coroutine can run in runBlocking {...} Currently thread 1 Bridging blocking and suspending code Unconfined vs confined dispatcher The Dispatchers.Unconfined coroutine dispatcher starts a coroutine in the caller thread, but only until the first suspension point. After suspension it resumes the coroutine in the thread that is fully determined by the suspending function that was invoked. 1 2 3 4 5 6 7 8 9 10 launch(Dispatchers.Unconfined) { // not confined -- will work with main thread println(\"Unconfined : I'm working in thread ${Thread.currentThread().name}\") delay(500) println(\"Unconfined : After delay in thread ${Thread.currentThread().name}\") } launch { // context of the parent, main runBlocking coroutine println(\"main runBlocking: I'm working in thread ${Thread.currentThread().name}\") delay(1000) println(\"main runBlocking: After delay in thread ${Thread.currentThread().name}\") } Produces the output: 1 2 3 4 Unconfined : I'm working in thread main main runBlocking: I'm working in thread main Unconfined : After delay in thread kotlinx.coroutines.DefaultExecutor main runBlocking: After delay in thread main So, the coroutine with the context inherited from runBlocking {...} continues to execute in the main thread, while the unconfined one resumes in the default executor thread that the delay function is using.","title":"Coroutine context and dispatchers"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#coroutine-context-and-dispatchers","text":"","title":"Coroutine context and dispatchers"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#coroutine-context","text":"Coroutines always execute in some context represented by a value of the CoroutineContext type The coroutine context is a set of various elements: Job: A cancellable piece of work, which has a defined lifecycle ContinuationInterceptor: A mechanism which listens to the continuation within a coroutine and intercepts its resumption. CoroutineExceptionHandler: A construct which handles exceptions in coroutines. 1 2 3 4 5 6 7 8 9 fun main() { val defaultDispatcher = Dispatchers.Default val emptyParentJob = Job() val combinedContext = defaultDispatcher + emptyParentJob GlobalScope.launch(context = combinedContext) { println(Thread.currentThread().name) } Thread.sleep(50) }","title":"Coroutine context"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#dispatchers-and-threads","text":"The coroutine context includes a coroutine dispatcher that determines what thread or threads the corresponding coroutine uses for its execution. All coroutine builders like launch and async accept an optional CoroutineContext parameter that can be used to explicitly specify the dispatcher for the new coroutine and other context elements. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /** * When `launch { ... }` is used without parameters, it inherits the context from the `CoroutineScope` it is being launched from. * In this case, it inherits the context of the main `runBlocking` coroutine which runs in the main thread. */ launch { // context of the parent, main runBlocking coroutine println(\"main runBlocking : I'm working in thread ${Thread.currentThread().name}\") } // Dispatchers.Unconfined is a special dispatcher that also appears to run in the main thread. launch(Dispatchers.Unconfined) { // not confined -- will work with main thread println(\"Unconfined : I'm working in thread ${Thread.currentThread().name}\") } launch(Dispatchers.Default) { // will get dispatched to DefaultDispatcher println(\"Default : I'm working in thread ${Thread.currentThread().name}\") } launch(newSingleThreadContext(\"MyOwnThread\")) { // will get its own new thread println(\"newSingleThreadContext: I'm working in thread ${Thread.currentThread().name}\") } It produces the following output (maybe in different order): 1 2 3 4 Unconfined : I'm working in thread main Default : I'm working in thread DefaultDispatcher-worker-1 newSingleThreadContext: I'm working in thread MyOwnThread main runBlocking : I'm working in thread main Thread used Max number of threads Useful when Dispatchers.Default thread pool Number of CPU cores Executing CPU-bound code Dispatchers.IO thread pool Defined in IO_PARALLELISM_PROPERTY_NAME. The default value is max(64, number of cpu cores) Executing IO-heavy code Dispatchers.Main main 1 Working with UI elements executorService.asCoroutineDispatcher() thread pool Defined by ExecuteService config Need to limit number of threads that the coroutine can run in runBlocking {...} Currently thread 1 Bridging blocking and suspending code","title":"Dispatchers and threads"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#unconfined-vs-confined-dispatcher","text":"The Dispatchers.Unconfined coroutine dispatcher starts a coroutine in the caller thread, but only until the first suspension point. After suspension it resumes the coroutine in the thread that is fully determined by the suspending function that was invoked. 1 2 3 4 5 6 7 8 9 10 launch(Dispatchers.Unconfined) { // not confined -- will work with main thread println(\"Unconfined : I'm working in thread ${Thread.currentThread().name}\") delay(500) println(\"Unconfined : After delay in thread ${Thread.currentThread().name}\") } launch { // context of the parent, main runBlocking coroutine println(\"main runBlocking: I'm working in thread ${Thread.currentThread().name}\") delay(1000) println(\"main runBlocking: After delay in thread ${Thread.currentThread().name}\") } Produces the output: 1 2 3 4 Unconfined : I'm working in thread main main runBlocking: I'm working in thread main Unconfined : After delay in thread kotlinx.coroutines.DefaultExecutor main runBlocking: After delay in thread main So, the coroutine with the context inherited from runBlocking {...} continues to execute in the main thread, while the unconfined one resumes in the default executor thread that the delay function is using.","title":"Unconfined vs confined dispatcher"},{"location":"programming/kotlin/coroutine/flow/","text":"Flow In coroutines, a flow is a type that can emit multiple values sequentially, as opposed to suspend functions that return only a single value. For example, you can use a flow to receive live updates from a database. There are three entities involved in streams of data: A producer produces data that is added to the stream. Thanks to coroutines, flows can also produce data asynchronously. (Optional) Intermediaries can modify each value emitted into the stream or the stream itself. A consumer consumes the values from the stream. Flows are cold streams similar to sequences \u2014 the code inside a flow builder does not run until the flow is collected. Creating a flow To create flows, use the flow builder APIs. The flow builder function creates a new flow where you can manually emit new values into the stream of data using the emit function. 1 2 3 4 5 6 fun simple(): Flow<Int> = flow { // flow builder for (i in 1..3) { delay(100) // pretend we are doing something useful here emit(i) // emit next value } } Collecting from a flow 1 2 3 4 fun main() = runBlocking<Unit> { // Collect the flow simple().collect { value -> println(value) } } Modifying the stream Intermediaries can use intermediate operators to modify the stream of data without consuming the value. 1 2 3 4 5 6 7 8 9 10 11 suspend fun performRequest(request: Int): String { delay(1000) // imitate long-running asynchronous work return \"response $request\" } fun main() = runBlocking<Unit> { (1..3).asFlow() // a flow of requests .filter { it != 2 } .map { request -> performRequest(request) } .collect { response -> println(response) } } You can also modify the flow using transform builtin function 1 2 3 4 5 6 (1..3).asFlow() // a flow of requests .transform { request -> emit(\"Making request $request\") emit(performRequest(request)) } .collect { response -> println(response) } Executing in a different CoroutineContext By default, the producer of a flow builder executes in the CoroutineContext of the coroutine that collects from it and you can not emit values from different CoroutineContext . This behavior might be undesirable in some cases. To change the CoroutineContext of a flow, use the intermediate operator flowOn . flowOn changes the CoroutineContext of the upstream flow, meaning the producer and any intermediate operators applied before (or above) flowOn . The downstream flow (the intermediate operators after flowOn along with the consumer) is not affected and executes on the CoroutineContext used to collect from the flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class NewsRepository( private val newsRemoteDataSource: NewsRemoteDataSource, private val userData: UserData, private val defaultDispatcher: CoroutineDispatcher ) { val favoriteLatestNews: Flow<List<ArticleHeadline>> = newsRemoteDataSource.latestNews .map { news -> // Executes on the default dispatcher news.filter { userData.isFavoriteTopic(it) } } .onEach { news -> // Executes on the default dispatcher saveInCache(news) } // flowOn affects the upstream flow \u2191 .flowOn(defaultDispatcher) // the downstream flow \u2193 is not affected .catch { exception -> // Executes in the consumer's context emit(lastCachedNews()) } }","title":"Flow"},{"location":"programming/kotlin/coroutine/flow/#flow","text":"In coroutines, a flow is a type that can emit multiple values sequentially, as opposed to suspend functions that return only a single value. For example, you can use a flow to receive live updates from a database. There are three entities involved in streams of data: A producer produces data that is added to the stream. Thanks to coroutines, flows can also produce data asynchronously. (Optional) Intermediaries can modify each value emitted into the stream or the stream itself. A consumer consumes the values from the stream. Flows are cold streams similar to sequences \u2014 the code inside a flow builder does not run until the flow is collected.","title":"Flow"},{"location":"programming/kotlin/coroutine/flow/#creating-a-flow","text":"To create flows, use the flow builder APIs. The flow builder function creates a new flow where you can manually emit new values into the stream of data using the emit function. 1 2 3 4 5 6 fun simple(): Flow<Int> = flow { // flow builder for (i in 1..3) { delay(100) // pretend we are doing something useful here emit(i) // emit next value } }","title":"Creating a flow"},{"location":"programming/kotlin/coroutine/flow/#collecting-from-a-flow","text":"1 2 3 4 fun main() = runBlocking<Unit> { // Collect the flow simple().collect { value -> println(value) } }","title":"Collecting from a flow"},{"location":"programming/kotlin/coroutine/flow/#modifying-the-stream","text":"Intermediaries can use intermediate operators to modify the stream of data without consuming the value. 1 2 3 4 5 6 7 8 9 10 11 suspend fun performRequest(request: Int): String { delay(1000) // imitate long-running asynchronous work return \"response $request\" } fun main() = runBlocking<Unit> { (1..3).asFlow() // a flow of requests .filter { it != 2 } .map { request -> performRequest(request) } .collect { response -> println(response) } } You can also modify the flow using transform builtin function 1 2 3 4 5 6 (1..3).asFlow() // a flow of requests .transform { request -> emit(\"Making request $request\") emit(performRequest(request)) } .collect { response -> println(response) }","title":"Modifying the stream"},{"location":"programming/kotlin/coroutine/flow/#executing-in-a-different-coroutinecontext","text":"By default, the producer of a flow builder executes in the CoroutineContext of the coroutine that collects from it and you can not emit values from different CoroutineContext . This behavior might be undesirable in some cases. To change the CoroutineContext of a flow, use the intermediate operator flowOn . flowOn changes the CoroutineContext of the upstream flow, meaning the producer and any intermediate operators applied before (or above) flowOn . The downstream flow (the intermediate operators after flowOn along with the consumer) is not affected and executes on the CoroutineContext used to collect from the flow. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class NewsRepository( private val newsRemoteDataSource: NewsRemoteDataSource, private val userData: UserData, private val defaultDispatcher: CoroutineDispatcher ) { val favoriteLatestNews: Flow<List<ArticleHeadline>> = newsRemoteDataSource.latestNews .map { news -> // Executes on the default dispatcher news.filter { userData.isFavoriteTopic(it) } } .onEach { news -> // Executes on the default dispatcher saveInCache(news) } // flowOn affects the upstream flow \u2191 .flowOn(defaultDispatcher) // the downstream flow \u2193 is not affected .catch { exception -> // Executes in the consumer's context emit(lastCachedNews()) } }","title":"Executing in a different CoroutineContext"},{"location":"web/http/","text":"HTTP 1. An overview of HTTP Hypertext Transfer Protocol (HTTP) is an application-layer protocol for transmitting hypermedia documents, such as HTML. It was designed for communication between web browsers and web servers, but it can also be used for other purposes. Basic aspects of HTTP: HTTP is media independent: It means, any type of data can be sent by HTTP as long as both the client and the server know how to handle the data content HTTP is stateless: The server and client are aware of each other only during a current request. Afterwards, both of them forget about each other. Htpp flow When a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps: Open a TCP connection Send an HTTP message (HTTP/1 is human-readable but HTTP/2 is not) Eg: 1 2 3 GET / HTTP/1.1 Host: developer.mozilla.org Accept-Language: fr Read the response sent by the server, such as: 1 2 3 4 5 6 7 8 9 10 HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \"51142bc1-7449-479b075b2891b\" Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html <!DOCTYPE html... (here comes the 29769 bytes of the requested web page) Close or reuse the connection for further requests. HTTP messsage HTTP messages, as defined in HTTP/1.1 and earlier, are human-readable. In HTTP/2, these messages are embedded into a binary structure, a frame, allowing optimizations like compression of headers and multiplexing. HTTP Request Requests consists of the following elements: An HTTP method (GET, POST, PUT, OPTIONS, HEAD...) A path of resource to fetch The version of HTTP protocol HTTP request headers that convey additional information for the servers Optionally, a body, for some methods like POST , which contain the resource sent HTTP Response Responses consist of the following elements: The version of the HTTP protocol A status code A status message, a non-authoritative short description of the status code HTTP response headers Optionally, a body containing the fetched resource. HTTP headers HTTP headers allow the client and the server to pass additional information with the request or the response An HTTP header consists of its case-insensitive name followed by a colon (:), then by its value. Whitespace before the value is ignored. Headers can be grouped according to their contexts: General headers applying to both requests and responses but with no relation to the data eventually transmitted in the body Request headers contain more information about the resource to be fetched, or about the client requesting the resource Response headers hold additional information about the response, like its location or about the server providing it Representation headers contain information about the body of the resource, like its MIME type, or encoding/compression applied Example: Cookie: Contains stored HTTP cookies previously sent by the server with the Set-Cookie header. Set-Cookie: Send cookies from the server to the user-agent. Origin: Indicates where a request originates from Authorization: Contains the credentials to authenticate a user-agent with a server Access-Control-Allow-Origin: Indicates whether the response can be shared ... HTTP status code HTTP status code are like short notes from server to client. They are messsages from the server letting clients konw how things went when it received requests. HTTP status codes are grouped in five classed: Information responses (100- 199) Successful responses (200, 299) Redirect (300-399) Client errors (400-499) Server errors (500-599) 2. HTTP pull In the HTTP pull method, the client sends a request to the server and the server responds to that request (and the connection is closed). The client pulls the data from the server whenever it requires (by creating a new connection). And it keeps doing it over and over to fetch the updated data. This is the default HTTP communication method and is extensively on the Internet for fetching HTTP pages from websites. The disadvantage of the HTTP pull method is that if clients keep on periodically makes the pull request for updated data, but there are no updates at the server hence, every time the client will get the same result, bandwidth will be wasted and the server will be busy too. Also, excessive pulls by the clients have the potential to bring down the server. 3. HTTP push To overcome the problem with HTTP pull, an HTTP push was introduced. In the HTTP push method, the client opens a connection to the server by requesting a server only the first time and after that server keeps on pushing back updated content to the client, whenever there\u2019s any. There are multiple technologies involved in the HTTP push based mechanism such as: Ajax Long polling Web Sockets HTML5 Event Source Message Queues Streaming over HTTP","title":"HTTP"},{"location":"web/http/#http","text":"","title":"HTTP"},{"location":"web/http/#1-an-overview-of-http","text":"Hypertext Transfer Protocol (HTTP) is an application-layer protocol for transmitting hypermedia documents, such as HTML. It was designed for communication between web browsers and web servers, but it can also be used for other purposes. Basic aspects of HTTP: HTTP is media independent: It means, any type of data can be sent by HTTP as long as both the client and the server know how to handle the data content HTTP is stateless: The server and client are aware of each other only during a current request. Afterwards, both of them forget about each other.","title":"1. An overview of HTTP"},{"location":"web/http/#htpp-flow","text":"When a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps: Open a TCP connection Send an HTTP message (HTTP/1 is human-readable but HTTP/2 is not) Eg: 1 2 3 GET / HTTP/1.1 Host: developer.mozilla.org Accept-Language: fr Read the response sent by the server, such as: 1 2 3 4 5 6 7 8 9 10 HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \"51142bc1-7449-479b075b2891b\" Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html <!DOCTYPE html... (here comes the 29769 bytes of the requested web page) Close or reuse the connection for further requests.","title":"Htpp flow"},{"location":"web/http/#http-messsage","text":"HTTP messages, as defined in HTTP/1.1 and earlier, are human-readable. In HTTP/2, these messages are embedded into a binary structure, a frame, allowing optimizations like compression of headers and multiplexing.","title":"HTTP messsage"},{"location":"web/http/#http-request","text":"Requests consists of the following elements: An HTTP method (GET, POST, PUT, OPTIONS, HEAD...) A path of resource to fetch The version of HTTP protocol HTTP request headers that convey additional information for the servers Optionally, a body, for some methods like POST , which contain the resource sent","title":"HTTP Request"},{"location":"web/http/#http-response","text":"Responses consist of the following elements: The version of the HTTP protocol A status code A status message, a non-authoritative short description of the status code HTTP response headers Optionally, a body containing the fetched resource.","title":"HTTP Response"},{"location":"web/http/#http-headers","text":"HTTP headers allow the client and the server to pass additional information with the request or the response An HTTP header consists of its case-insensitive name followed by a colon (:), then by its value. Whitespace before the value is ignored. Headers can be grouped according to their contexts: General headers applying to both requests and responses but with no relation to the data eventually transmitted in the body Request headers contain more information about the resource to be fetched, or about the client requesting the resource Response headers hold additional information about the response, like its location or about the server providing it Representation headers contain information about the body of the resource, like its MIME type, or encoding/compression applied Example: Cookie: Contains stored HTTP cookies previously sent by the server with the Set-Cookie header. Set-Cookie: Send cookies from the server to the user-agent. Origin: Indicates where a request originates from Authorization: Contains the credentials to authenticate a user-agent with a server Access-Control-Allow-Origin: Indicates whether the response can be shared ...","title":"HTTP headers"},{"location":"web/http/#http-status-code","text":"HTTP status code are like short notes from server to client. They are messsages from the server letting clients konw how things went when it received requests. HTTP status codes are grouped in five classed: Information responses (100- 199) Successful responses (200, 299) Redirect (300-399) Client errors (400-499) Server errors (500-599)","title":"HTTP status code"},{"location":"web/http/#2-http-pull","text":"In the HTTP pull method, the client sends a request to the server and the server responds to that request (and the connection is closed). The client pulls the data from the server whenever it requires (by creating a new connection). And it keeps doing it over and over to fetch the updated data. This is the default HTTP communication method and is extensively on the Internet for fetching HTTP pages from websites. The disadvantage of the HTTP pull method is that if clients keep on periodically makes the pull request for updated data, but there are no updates at the server hence, every time the client will get the same result, bandwidth will be wasted and the server will be busy too. Also, excessive pulls by the clients have the potential to bring down the server.","title":"2. HTTP pull"},{"location":"web/http/#3-http-push","text":"To overcome the problem with HTTP pull, an HTTP push was introduced. In the HTTP push method, the client opens a connection to the server by requesting a server only the first time and after that server keeps on pushing back updated content to the client, whenever there\u2019s any. There are multiple technologies involved in the HTTP push based mechanism such as: Ajax Long polling Web Sockets HTML5 Event Source Message Queues Streaming over HTTP","title":"3. HTTP push"},{"location":"web/database/cassandra/","text":"Cassandra Apache Cassandra is a highly-scalable partitioned row store. Rows are organized into tables with a required primary key. Partitioning means that Cassandra can distribute your data across multiple machines Row store means that like relational databases, Cassandra organizes data by rows and columns. 1. Cassandra Architecture Some of the features of Cassandra architecture are as follows: Cassandra is designed such that it has no master or slave nodes. It has a ring-type architecture, that is, its nodes are logically distributed like a ring. Data is automatically distributed across all the nodes. Similar to HDFS, data is replicated across the nodes for redundancy. Data is kept in memory and lazily written to the disk. Hash values of the keys are used to distribute the data among nodes in the cluster. Additional features of Cassandra architecture are: Cassandra architecture supports multiple data centers. Data can be replicated across data centers. You can keep three copies of data in one data center and the fourth copy in a remote data center for remote backup. Data reads prefer a local data center to a remote data center. Node Node is the place where data is stored. It is the basic component of Cassandra. Rack A rack is a group of machines housed in the same physical box. All machines in the rack are connected to the network switch of the rack The rack\u2019s network switch is connected to the cluster. All machines on the rack have a common power supply. It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure. If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down. Datacenter A datacenter is a logical set of racks/ nodes. A common use case is AWS-EAST vs AWS-WEST... Cluster The cluster is the collection of many data centers. 2. Data distribution and replication 2.1 Data Partitions A partition key is converted to a token by a partitioner . The tokens are signed integer values between -2^63 to +2^63-1, and this range is referred to as token range. If we consider there are only 100 tokens used for a Cassandra cluster with three nodes. Each node is assigned approximately 33 tokens like 1 2 3 node1: 0-33 node2: 34-66 node3: 67-99 If there are nodes added or removed, the token range distribution should be shuffled to suit the new topology. This process takes a lot of calculation and configuration change for each cluster operation. If one node is removed, data in removed node is placed on the next neighbor node in clockwise manner. 2.2 Virtual nodes/Vnodes Virtual nodes in a Cassandra cluster are also called vnodes. Vnodes can be defined for each physical node in the cluster. Each node in the ring can hold multiple virtual nodes. The default number of Vnodes owned by a node in Cassandra is 256 , which is set by num_tokens property. When a node is added into a cluster, the token allocation algorithm allocates tokens to the node. The algorithm selects random token values to ensure uniform distribution. In your case you have 6 nodes, each set with 256 token ranges so you have 6*256 token ranges and each psychical node contains 256 token ranges. 2.3 Replication The data in each keyspace is replicated with a replication factor . There is one primary replica of data that resides with the token owner node as explained in the data partitioning section. The remainder of replicas is placed by Cassandra on specific nodes using the replica placement strategy. The total number of replicas for a keyspace across a Cassandra cluster is referred to as the keyspace's replication factor. A replication factor of one means that there is only one copy of each row in the Cassandra cluster. A replication factor of two means there are two copies of each row, where each copy is on a different node. All replicas are equally important; there is no primary or master replica. There are two settings that mainly impact replica placement: First is snitch, which determines the data center, and the rack a Cassandra node belongs to, and it is set at the node level The second setting is the replication strategy. The replication strategy is set at the keyspace level. There are two strategies: SimpleStrategy and NetworkTopologyStrategy. SimpleStrategy: does not consider racks and multiple data centers. It places data replicas on nodes sequentially. NetworkTopologyStrategy: is rack aware and data center aware 2.4 Consistency level The Cassandra consistency level is defined as the minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful. Different consistency levels can be assigned to different Edge keyspaces. You can find all cassandra's consistency level here Write Consistency A client sends a write request to the coordinator. The coordinator forwards the write request (INSERT, UPDATE or DELETE) to all replica nodes whatever write CL you have set. The coordinator waits for n number of replica nodes to respond. n is set by the write CL. The coordinator sends the response back to the client. Read Consistency A client sends a read request to the coordinator. The coordinator forwards the read (SELECT) request to n number of replica nodes. n is set by the read CL. The coordinator waits for n number of replica nodes to respond. The coordinator then merges (finds out most recent copy of written data) the n number of responses to a single response and sends response to the client. 3. Data storage Cassandra processes data at several stages on the write path, starting with the immediate logging of a write and ending in with a write of data to disk: Logging data in the commit log Writing data to the memtable Flushing data from the memtable Storing data on disk in SSTables Commitlogs are an append only log of all mutations local to a Cassandra node. Any data written to Cassandra will first be written to a commit log before being written to a memtable. This provides durability in the case of unexpected shutdown. On startup, any mutations in the commit log will be applied to memtables. Memtables are in-memory structures where Cassandra buffers writes. In general, there is one active memtable per table. Eventually, memtables are flushed onto disk and become immutable SSTables. This can be triggered in several ways: The memory usage of the memtables exceeds the configured threshold (see memtable_cleanup_threshold ) The commit-log approaches its maximum size, and forces memtable flushes in order to allow commitlog segments to be freed SSTables are the immutable data files that Cassandra uses for persisting data on disk. 4. Data Model The Cassandra data model uses the same terms as Google BigTable, for example, column family, column, row, etc. Some of these terms also exist in the relational data model but have different meanings. Keyspace A keyspace is analogous to a schema or database in a relational model. Each Cassandra cluster has a system keyspace to store system-wide metadata. Keyspace contains replication settings that control how data is distributed and replicated in clusters. Syntax: 1 Create keyspace KeyspaceName with replication={'class':strategy name, 'replication_factor': No of replications on different nodes}; Strategy: Simple Strategy: Simple strategy is used when you have just one data center. In this strategy, the first replica is placed on the node selected by the partitioner. Remaining nodes are placed in the clockwise direction in the ring without considering rack or node location. Network Topology Strategy: Network topology strategy is used when you have more than one data centers. In this strategy, you have to provide replication factor for each data center separately. Network topology strategy places replicas in nodes in the clockwise direction in the same data center. This strategy attempts to place replicas in different racks. Replication Factor: Replication factor is the number of replicas of data placed on different nodes. For no failure, 3 is good replication factor. More than two replication factor ensures no single point of failure. Sometimes, the server can be down, or network problem can occur, then other replicas provide service with no failure. Eg: 1 Create keyspace University with replication={'class':SimpleStrategy,'replication_factor': 3}; Column Family A row key in the column family must be unique and be used to identify rows. Although not the same, the column family can be analogous to a table in a relational database. Column families provide greater flexibility by allowing different columns in different rows. Syntax: 1 2 3 4 5 6 7 8 Create table KeyspaceName.TableName ( ColumnName DataType, ColumnName DataType, ColumnName DataType ... Primary key(ColumnName) ) with PropertyName=PropertyValue; Single Primary Key 1 Primary key (ColumnName) In the single primary key, there is only a single column. That column is also called partitioning key. Data is partitioned on the basis of that column. Compound Primary Key 1 Primary key(ColumnName1,ColumnName2 . . .) In above syntax, ColumnName1 is the partitioning key and ColumnName2 is the Clustering key. Data will be partitioned on the basis of ColumnName1 and data will be clustered on the basis of ColumnName2. Clustering is the process that sorts data in the partition. Compound Partitioning key 1 Primary Key((ColumnName1,ColumnName2),ColumnName3...)) In above syntax, ColumnName1 and ColumnName2 are the compound partition key. Data will be partitioned on the basis of both columns ColumnName1 and ColumnName2 and data will be clustered on the basis of the ColumnName3. Cassandra index Cassandra creates indexes on the data during the \u2018create index\u2019 statement execution. After creating an index, Cassandra indexes new data automatically when data is inserted. The index cannot be created on primary key as a primary key is already indexed. Indexes on collections are not supported in Cassandra. Without indexing on the column, Cassandra can\u2019t filter that column unless it is a primary key. That\u2019s why, for filtering columns in Cassandra, indexes needs to be created. Syntax 1 Create index IndexName on KeyspaceName.TableName(ColumnName); Row Each row consists of a row key \u2014 also known as the primary key \u2014 and a set of columns, as shown in the following figure. Each row may have different column names. That is why Cassandra is row-oriented and column-oriented. There are no timestamps for the row. Column A column is the smallest data model element in Cassandra. Although it also exists in a relational database, the column in Cassandra is different. The figure below shows that each column consists of a column name, column value, timestamp, and TTL ( Time-To-Live ). The timestamp is used for conflict resolution by client applications during write operations. Time-To-Live is an optional expiration value that is used to mark columns that are deleted after expiration. Time-To-Live: During data insertion, you have to specify \u2018ttl\u2019 value in seconds. \u2018ttl\u2019 value is the time to live value for the data. After that particular amount of time, data will be automatically removed. For example, specify ttl value 100 seconds during insertion. Data will be automatically deleted after 100 seconds. When data is expired, that expired data is marked with a tombstone. A tombstone exists for a grace period. After data is expired, data is automatically removed after compaction process. Syntax: 1 Insert into KeyspaceName.TableName(ColumnNames) values(ColumnValues) using ttl TimeInseconds; Cassandra Data Model Rules Cassandra does not support joins, group by, OR clause, aggregations, etc. So you have to store your data in such a way that it should be completely retrievable. So these rules must be kept in mind while modelling data in Cassandra. What cassandra does not support: CQL does not support aggregation queries like max, min, avg. CQL does not support group by, having queries. CQL does not support joins. CQL does not support OR queries. CQL does not support wildcard queries. CQL does not support Union, Intersection queries. Table columns cannot be filtered without creating the index. Greater than (>) and less than (<) query is only supported on clustering column. Cassandra query language is not suitable for analytics purposes because it has so many limitations. 5. Data Modeling in Cassandra example 5.1 Facebook Posts Suppose that we are storing Facebook posts of different users in Cassandra. One of the common query patterns will be fetching the top N posts made by a given user. Thus we need to store all data for a particular user on a single partition 1 2 3 4 5 6 CREATE TABLE posts_facebook ( user_id uuid, post_id timeuuid, content text, PRIMARY KEY (user_id, post_id) ) WITH CLUSTERING ORDER BY (post_id DESC); Now, let's write a query to find the top 20 posts for the user Anna: 1 SELECT content FROM posts_facebook WHERE user_id = \"Anna_id\" LIMIT 20 5.2 Gyms Across the Country Suppose that we are storing the details of different partner gyms across the different cities and states of many countries and we would like to fetch the gyms for a given city. Also, let's say we need to return the results having gyms sorted by their opening date. 1 2 3 4 5 6 7 8 9 10 CREATE TABLE gyms_by_city ( country_code text, state text, city text, gym_name text, opening_date timestamp, PRIMARY KEY ( (country_code, state_province, city), (opening_date, gym_name)) WITH CLUSTERING ORDER BY (opening_date ASC, gym_name ASC); Now, let's look at a query that fetches the first ten gyms by their opening date for the city of Phoenix within the U.S. state of Arizona: 1 2 3 SELECT * FROM gyms_by_city WHERE country_code = \"us\" AND state = \"Arizona\" AND city = \"Phoenix\" LIMIT 10 Next, let\u2019s see a query that fetches the ten most recently-opened gyms in the city of Phoenix within the U.S. state of Arizona: 1 2 3 4 SELECT * FROM gyms_by_city WHERE country_code = \"us\" and state = \"Arizona\" and city = \"Phoenix\" ORDER BY opening_date DESC LIMIT 10 Note : As the last query's sort order is opposite of the sort order defined during the table creation, the query will run slower as Cassandra will first fetch the data and then sort it in memory. 5.3 E-commerce Customers and Products Let's say we are running an e-commerce store and that we are storing the Customer and Product information within Cassandra. Let's look at some of the common query patterns around this use case: Get Customer info Get Product info Get all Customers who like a given Product Get all Products a given Customer likes We will start by using separate tables for storing the Customer and Product information. However, we need to introduce a fair amount of denormalization to support the 3rd and 4th queries shown above. We will create two more tables to achieve this \u2013 \u201cCustomer_by_Product\u201d and \u201cProduct_by_Customer\u201c. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 CREATE TABLE Customer ( cust_id text, first_name text, last_name text, registered_on timestamp, PRIMARY KEY (cust_id)); CREATE TABLE Product ( prdt_id text, title text, PRIMARY KEY (prdt_id)); CREATE TABLE Customer_By_Liked_Product ( liked_prdt_id text, liked_on timestamp, title text, cust_id text, first_name text, last_name text, PRIMARY KEY (prdt_id, liked_on)); CREATE TABLE Product_Liked_By_Customer ( cust_id text, first_name text, last_name text, liked_prdt_id text, liked_on timestamp, title text, PRIMARY KEY (cust_id, liked_on)); Query to find the ten Customers who most recently liked the product \u201cPepsi\u201c: 1 SELECT * FROM Customer_By_Liked_Product WHERE title = \"Pepsi\" LIMIT 10 Query that finds the recently-liked products (up to ten) by a customer named \u201cAnna\u201c: 1 SELECT * FROM Product_Liked_By_Customer WHERE first_name = \"Anna\" LIMIT 10","title":"Cassandra"},{"location":"web/database/cassandra/#cassandra","text":"Apache Cassandra is a highly-scalable partitioned row store. Rows are organized into tables with a required primary key. Partitioning means that Cassandra can distribute your data across multiple machines Row store means that like relational databases, Cassandra organizes data by rows and columns.","title":"Cassandra"},{"location":"web/database/cassandra/#1-cassandra-architecture","text":"Some of the features of Cassandra architecture are as follows: Cassandra is designed such that it has no master or slave nodes. It has a ring-type architecture, that is, its nodes are logically distributed like a ring. Data is automatically distributed across all the nodes. Similar to HDFS, data is replicated across the nodes for redundancy. Data is kept in memory and lazily written to the disk. Hash values of the keys are used to distribute the data among nodes in the cluster. Additional features of Cassandra architecture are: Cassandra architecture supports multiple data centers. Data can be replicated across data centers. You can keep three copies of data in one data center and the fourth copy in a remote data center for remote backup. Data reads prefer a local data center to a remote data center.","title":"1. Cassandra Architecture"},{"location":"web/database/cassandra/#node","text":"Node is the place where data is stored. It is the basic component of Cassandra.","title":"Node"},{"location":"web/database/cassandra/#rack","text":"A rack is a group of machines housed in the same physical box. All machines in the rack are connected to the network switch of the rack The rack\u2019s network switch is connected to the cluster. All machines on the rack have a common power supply. It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure. If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down.","title":"Rack"},{"location":"web/database/cassandra/#datacenter","text":"A datacenter is a logical set of racks/ nodes. A common use case is AWS-EAST vs AWS-WEST...","title":"Datacenter"},{"location":"web/database/cassandra/#cluster","text":"The cluster is the collection of many data centers.","title":"Cluster"},{"location":"web/database/cassandra/#2-data-distribution-and-replication","text":"","title":"2. Data distribution and replication"},{"location":"web/database/cassandra/#21-data-partitions","text":"A partition key is converted to a token by a partitioner . The tokens are signed integer values between -2^63 to +2^63-1, and this range is referred to as token range. If we consider there are only 100 tokens used for a Cassandra cluster with three nodes. Each node is assigned approximately 33 tokens like 1 2 3 node1: 0-33 node2: 34-66 node3: 67-99 If there are nodes added or removed, the token range distribution should be shuffled to suit the new topology. This process takes a lot of calculation and configuration change for each cluster operation. If one node is removed, data in removed node is placed on the next neighbor node in clockwise manner.","title":"2.1 Data Partitions"},{"location":"web/database/cassandra/#22-virtual-nodesvnodes","text":"Virtual nodes in a Cassandra cluster are also called vnodes. Vnodes can be defined for each physical node in the cluster. Each node in the ring can hold multiple virtual nodes. The default number of Vnodes owned by a node in Cassandra is 256 , which is set by num_tokens property. When a node is added into a cluster, the token allocation algorithm allocates tokens to the node. The algorithm selects random token values to ensure uniform distribution. In your case you have 6 nodes, each set with 256 token ranges so you have 6*256 token ranges and each psychical node contains 256 token ranges.","title":"2.2 Virtual nodes/Vnodes"},{"location":"web/database/cassandra/#23-replication","text":"The data in each keyspace is replicated with a replication factor . There is one primary replica of data that resides with the token owner node as explained in the data partitioning section. The remainder of replicas is placed by Cassandra on specific nodes using the replica placement strategy. The total number of replicas for a keyspace across a Cassandra cluster is referred to as the keyspace's replication factor. A replication factor of one means that there is only one copy of each row in the Cassandra cluster. A replication factor of two means there are two copies of each row, where each copy is on a different node. All replicas are equally important; there is no primary or master replica. There are two settings that mainly impact replica placement: First is snitch, which determines the data center, and the rack a Cassandra node belongs to, and it is set at the node level The second setting is the replication strategy. The replication strategy is set at the keyspace level. There are two strategies: SimpleStrategy and NetworkTopologyStrategy. SimpleStrategy: does not consider racks and multiple data centers. It places data replicas on nodes sequentially. NetworkTopologyStrategy: is rack aware and data center aware","title":"2.3 Replication"},{"location":"web/database/cassandra/#24-consistency-level","text":"The Cassandra consistency level is defined as the minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful. Different consistency levels can be assigned to different Edge keyspaces. You can find all cassandra's consistency level here Write Consistency A client sends a write request to the coordinator. The coordinator forwards the write request (INSERT, UPDATE or DELETE) to all replica nodes whatever write CL you have set. The coordinator waits for n number of replica nodes to respond. n is set by the write CL. The coordinator sends the response back to the client. Read Consistency A client sends a read request to the coordinator. The coordinator forwards the read (SELECT) request to n number of replica nodes. n is set by the read CL. The coordinator waits for n number of replica nodes to respond. The coordinator then merges (finds out most recent copy of written data) the n number of responses to a single response and sends response to the client.","title":"2.4 Consistency level"},{"location":"web/database/cassandra/#3-data-storage","text":"Cassandra processes data at several stages on the write path, starting with the immediate logging of a write and ending in with a write of data to disk: Logging data in the commit log Writing data to the memtable Flushing data from the memtable Storing data on disk in SSTables Commitlogs are an append only log of all mutations local to a Cassandra node. Any data written to Cassandra will first be written to a commit log before being written to a memtable. This provides durability in the case of unexpected shutdown. On startup, any mutations in the commit log will be applied to memtables. Memtables are in-memory structures where Cassandra buffers writes. In general, there is one active memtable per table. Eventually, memtables are flushed onto disk and become immutable SSTables. This can be triggered in several ways: The memory usage of the memtables exceeds the configured threshold (see memtable_cleanup_threshold ) The commit-log approaches its maximum size, and forces memtable flushes in order to allow commitlog segments to be freed SSTables are the immutable data files that Cassandra uses for persisting data on disk.","title":"3. Data storage"},{"location":"web/database/cassandra/#4-data-model","text":"The Cassandra data model uses the same terms as Google BigTable, for example, column family, column, row, etc. Some of these terms also exist in the relational data model but have different meanings. Keyspace A keyspace is analogous to a schema or database in a relational model. Each Cassandra cluster has a system keyspace to store system-wide metadata. Keyspace contains replication settings that control how data is distributed and replicated in clusters. Syntax: 1 Create keyspace KeyspaceName with replication={'class':strategy name, 'replication_factor': No of replications on different nodes}; Strategy: Simple Strategy: Simple strategy is used when you have just one data center. In this strategy, the first replica is placed on the node selected by the partitioner. Remaining nodes are placed in the clockwise direction in the ring without considering rack or node location. Network Topology Strategy: Network topology strategy is used when you have more than one data centers. In this strategy, you have to provide replication factor for each data center separately. Network topology strategy places replicas in nodes in the clockwise direction in the same data center. This strategy attempts to place replicas in different racks. Replication Factor: Replication factor is the number of replicas of data placed on different nodes. For no failure, 3 is good replication factor. More than two replication factor ensures no single point of failure. Sometimes, the server can be down, or network problem can occur, then other replicas provide service with no failure. Eg: 1 Create keyspace University with replication={'class':SimpleStrategy,'replication_factor': 3}; Column Family A row key in the column family must be unique and be used to identify rows. Although not the same, the column family can be analogous to a table in a relational database. Column families provide greater flexibility by allowing different columns in different rows. Syntax: 1 2 3 4 5 6 7 8 Create table KeyspaceName.TableName ( ColumnName DataType, ColumnName DataType, ColumnName DataType ... Primary key(ColumnName) ) with PropertyName=PropertyValue; Single Primary Key 1 Primary key (ColumnName) In the single primary key, there is only a single column. That column is also called partitioning key. Data is partitioned on the basis of that column. Compound Primary Key 1 Primary key(ColumnName1,ColumnName2 . . .) In above syntax, ColumnName1 is the partitioning key and ColumnName2 is the Clustering key. Data will be partitioned on the basis of ColumnName1 and data will be clustered on the basis of ColumnName2. Clustering is the process that sorts data in the partition. Compound Partitioning key 1 Primary Key((ColumnName1,ColumnName2),ColumnName3...)) In above syntax, ColumnName1 and ColumnName2 are the compound partition key. Data will be partitioned on the basis of both columns ColumnName1 and ColumnName2 and data will be clustered on the basis of the ColumnName3. Cassandra index Cassandra creates indexes on the data during the \u2018create index\u2019 statement execution. After creating an index, Cassandra indexes new data automatically when data is inserted. The index cannot be created on primary key as a primary key is already indexed. Indexes on collections are not supported in Cassandra. Without indexing on the column, Cassandra can\u2019t filter that column unless it is a primary key. That\u2019s why, for filtering columns in Cassandra, indexes needs to be created. Syntax 1 Create index IndexName on KeyspaceName.TableName(ColumnName); Row Each row consists of a row key \u2014 also known as the primary key \u2014 and a set of columns, as shown in the following figure. Each row may have different column names. That is why Cassandra is row-oriented and column-oriented. There are no timestamps for the row. Column A column is the smallest data model element in Cassandra. Although it also exists in a relational database, the column in Cassandra is different. The figure below shows that each column consists of a column name, column value, timestamp, and TTL ( Time-To-Live ). The timestamp is used for conflict resolution by client applications during write operations. Time-To-Live is an optional expiration value that is used to mark columns that are deleted after expiration. Time-To-Live: During data insertion, you have to specify \u2018ttl\u2019 value in seconds. \u2018ttl\u2019 value is the time to live value for the data. After that particular amount of time, data will be automatically removed. For example, specify ttl value 100 seconds during insertion. Data will be automatically deleted after 100 seconds. When data is expired, that expired data is marked with a tombstone. A tombstone exists for a grace period. After data is expired, data is automatically removed after compaction process. Syntax: 1 Insert into KeyspaceName.TableName(ColumnNames) values(ColumnValues) using ttl TimeInseconds; Cassandra Data Model Rules Cassandra does not support joins, group by, OR clause, aggregations, etc. So you have to store your data in such a way that it should be completely retrievable. So these rules must be kept in mind while modelling data in Cassandra. What cassandra does not support: CQL does not support aggregation queries like max, min, avg. CQL does not support group by, having queries. CQL does not support joins. CQL does not support OR queries. CQL does not support wildcard queries. CQL does not support Union, Intersection queries. Table columns cannot be filtered without creating the index. Greater than (>) and less than (<) query is only supported on clustering column. Cassandra query language is not suitable for analytics purposes because it has so many limitations.","title":"4. Data Model"},{"location":"web/database/cassandra/#5-data-modeling-in-cassandra-example","text":"","title":"5. Data Modeling in Cassandra example"},{"location":"web/database/cassandra/#51-facebook-posts","text":"Suppose that we are storing Facebook posts of different users in Cassandra. One of the common query patterns will be fetching the top N posts made by a given user. Thus we need to store all data for a particular user on a single partition 1 2 3 4 5 6 CREATE TABLE posts_facebook ( user_id uuid, post_id timeuuid, content text, PRIMARY KEY (user_id, post_id) ) WITH CLUSTERING ORDER BY (post_id DESC); Now, let's write a query to find the top 20 posts for the user Anna: 1 SELECT content FROM posts_facebook WHERE user_id = \"Anna_id\" LIMIT 20","title":"5.1 Facebook Posts"},{"location":"web/database/cassandra/#52-gyms-across-the-country","text":"Suppose that we are storing the details of different partner gyms across the different cities and states of many countries and we would like to fetch the gyms for a given city. Also, let's say we need to return the results having gyms sorted by their opening date. 1 2 3 4 5 6 7 8 9 10 CREATE TABLE gyms_by_city ( country_code text, state text, city text, gym_name text, opening_date timestamp, PRIMARY KEY ( (country_code, state_province, city), (opening_date, gym_name)) WITH CLUSTERING ORDER BY (opening_date ASC, gym_name ASC); Now, let's look at a query that fetches the first ten gyms by their opening date for the city of Phoenix within the U.S. state of Arizona: 1 2 3 SELECT * FROM gyms_by_city WHERE country_code = \"us\" AND state = \"Arizona\" AND city = \"Phoenix\" LIMIT 10 Next, let\u2019s see a query that fetches the ten most recently-opened gyms in the city of Phoenix within the U.S. state of Arizona: 1 2 3 4 SELECT * FROM gyms_by_city WHERE country_code = \"us\" and state = \"Arizona\" and city = \"Phoenix\" ORDER BY opening_date DESC LIMIT 10 Note : As the last query's sort order is opposite of the sort order defined during the table creation, the query will run slower as Cassandra will first fetch the data and then sort it in memory.","title":"5.2 Gyms Across the Country"},{"location":"web/database/cassandra/#53-e-commerce-customers-and-products","text":"Let's say we are running an e-commerce store and that we are storing the Customer and Product information within Cassandra. Let's look at some of the common query patterns around this use case: Get Customer info Get Product info Get all Customers who like a given Product Get all Products a given Customer likes We will start by using separate tables for storing the Customer and Product information. However, we need to introduce a fair amount of denormalization to support the 3rd and 4th queries shown above. We will create two more tables to achieve this \u2013 \u201cCustomer_by_Product\u201d and \u201cProduct_by_Customer\u201c. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 CREATE TABLE Customer ( cust_id text, first_name text, last_name text, registered_on timestamp, PRIMARY KEY (cust_id)); CREATE TABLE Product ( prdt_id text, title text, PRIMARY KEY (prdt_id)); CREATE TABLE Customer_By_Liked_Product ( liked_prdt_id text, liked_on timestamp, title text, cust_id text, first_name text, last_name text, PRIMARY KEY (prdt_id, liked_on)); CREATE TABLE Product_Liked_By_Customer ( cust_id text, first_name text, last_name text, liked_prdt_id text, liked_on timestamp, title text, PRIMARY KEY (cust_id, liked_on)); Query to find the ten Customers who most recently liked the product \u201cPepsi\u201c: 1 SELECT * FROM Customer_By_Liked_Product WHERE title = \"Pepsi\" LIMIT 10 Query that finds the recently-liked products (up to ten) by a customer named \u201cAnna\u201c: 1 SELECT * FROM Product_Liked_By_Customer WHERE first_name = \"Anna\" LIMIT 10","title":"5.3 E-commerce Customers and Products"},{"location":"web/database/mysql/","text":"MySQL 1. Clustered index and non-clustered index A clustered index is a B-Tree index whose leaf nodes are the actual data blocks on disk. In cluseted index the data are stored physically on the disk in the same order as the clustered index. Therefore, there can be only one clustered index. A non-clustered index is a B-Tree index whose leaf nodes point to the clustered index key. We can have multiple non-clustered indexes per table. Clustered indexes are faster than non-clustered indexes since they don\u2019t involve any extra lookup step. In clustered index we just only traverse the tree once however in non-clustered index we need to do it twice. The first one for getting clustered index key in non-clustered B-tree and the second one for getting actual data from clustered B-tree. When you define a primary key for an InnoDB table, MySQL uses the primary key as the clustered index. If you do not have a primary key for a table, MySQL will search for the first UNIQUE index where all the key columns are NOT NULL and use this UNIQUE index as the clustered index. In case the InnoDB table has no primary key or suitable UNIQUE index, MySQL internally generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column that contains the row ID values. 2. MySQL InnoDB vs MyISAM InnnoDB MyISAM Row level locking Table level locking Supports foreign key Does not support relationship constraints Transactional (Rollback, commit) Non-transactional ACID compliant Not ACID compliant Row data stored in pages as per primary key order No particular order for data stored","title":"MySQL"},{"location":"web/database/mysql/#mysql","text":"","title":"MySQL"},{"location":"web/database/mysql/#1-clustered-index-and-non-clustered-index","text":"A clustered index is a B-Tree index whose leaf nodes are the actual data blocks on disk. In cluseted index the data are stored physically on the disk in the same order as the clustered index. Therefore, there can be only one clustered index. A non-clustered index is a B-Tree index whose leaf nodes point to the clustered index key. We can have multiple non-clustered indexes per table. Clustered indexes are faster than non-clustered indexes since they don\u2019t involve any extra lookup step. In clustered index we just only traverse the tree once however in non-clustered index we need to do it twice. The first one for getting clustered index key in non-clustered B-tree and the second one for getting actual data from clustered B-tree. When you define a primary key for an InnoDB table, MySQL uses the primary key as the clustered index. If you do not have a primary key for a table, MySQL will search for the first UNIQUE index where all the key columns are NOT NULL and use this UNIQUE index as the clustered index. In case the InnoDB table has no primary key or suitable UNIQUE index, MySQL internally generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column that contains the row ID values.","title":"1. Clustered index and non-clustered index"},{"location":"web/database/mysql/#2-mysql-innodb-vs-myisam","text":"InnnoDB MyISAM Row level locking Table level locking Supports foreign key Does not support relationship constraints Transactional (Rollback, commit) Non-transactional ACID compliant Not ACID compliant Row data stored in pages as per primary key order No particular order for data stored","title":"2. MySQL InnoDB vs MyISAM"},{"location":"web/database/nosql/","text":"NoSQL NoSQL DB is a database used to manage huge sets of unstructured data, where the data is not stored in tabular relations like relational databases. 1. BASE properties The relational databases strongly follow the ACID properties while the NoSQL databases follow BASE principles. In comparison with the CAP Theorem , BASE chooses availability over consistency. Basic Availability: The system guarantees availability. There will be a response to any request (can be failure too). Soft state: The data stored in the system may change because of the eventual consistency model (even without an input, the system state may change). Eventual consistency: The system will eventually become consistent once it stops receiving input.","title":"NoSQL"},{"location":"web/database/nosql/#nosql","text":"NoSQL DB is a database used to manage huge sets of unstructured data, where the data is not stored in tabular relations like relational databases.","title":"NoSQL"},{"location":"web/database/nosql/#1-base-properties","text":"The relational databases strongly follow the ACID properties while the NoSQL databases follow BASE principles. In comparison with the CAP Theorem , BASE chooses availability over consistency. Basic Availability: The system guarantees availability. There will be a response to any request (can be failure too). Soft state: The data stored in the system may change because of the eventual consistency model (even without an input, the system state may change). Eventual consistency: The system will eventually become consistent once it stops receiving input.","title":"1. BASE properties"},{"location":"web/database/sql_cheat_sheet/","text":"SQL cheat sheet","title":"SQL cheat sheet"},{"location":"web/database/sql_cheat_sheet/#sql-cheat-sheet","text":"","title":"SQL cheat sheet"},{"location":"web/database/transactions/","text":"Transactions 1. ACID properties ACID is a set of properties of database transaction intended to guarantee data validity despite errors. Atomicity Transactions are often composed of multiple stataments. Atomicity guarantees that each transaction is treated as a single unit, which either succeeds completely or fails completely. If any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged. Consistency The data is in consistent state before and after transaction. Eg: We have a transaction to transfer money from account A to account B. The total amount before and after the transaction must be the same. Isolation If the multiple transactions are running concurrently, they should not be affected by each other. Durability Changes that have been committed to the database should remain even in the case of software and system failure. 2. Read phenomena Dirty read A dirty read is the situation when a transaction reads a uncommitted data from other transactions. Non-repeatable read Non repeatable read occurs when a transaction reads same row twice, and get a different value each time. For example, suppose transaction T1 reads data. Due to concurrency, another transaction T2 updates the same data and commit, Now if transaction T1 rereads the same data, it will retrieve a different value. Phantom read Phantom read occurs when two same range queries are executed, but the number of rows retrieved by the two, are different. For example, suppose transaction T1 retrieves a set of rows that satisfy some search criteria. Now, Transaction T2 generates some new rows that match the search criteria for transaction T1. If transaction T1 re-executes the statement that reads the rows, it gets a different set of rows this time. 3. Isolation levels Isolation Levels Dirty Reads Non-repeatable Reads Phantom Reads Read Uncommitted \u2714 \u2714 \u2714 Read Committed \u2718 \u2714 \u2714 Repeatable Reads \u2718 \u2718 \u2714 Serializable \u2718 \u2718 \u2718 In Repeatable Reads isolation or higher if we are updating row data at the same time another concurrent transaction is also updating the same row then we are getting an error ERROR: could not serialize access due to concurrent update (In MySQL we need Serializable isolation level).","title":"Transactions"},{"location":"web/database/transactions/#transactions","text":"","title":"Transactions"},{"location":"web/database/transactions/#1-acid-properties","text":"ACID is a set of properties of database transaction intended to guarantee data validity despite errors.","title":"1. ACID properties"},{"location":"web/database/transactions/#atomicity","text":"Transactions are often composed of multiple stataments. Atomicity guarantees that each transaction is treated as a single unit, which either succeeds completely or fails completely. If any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged.","title":"Atomicity"},{"location":"web/database/transactions/#consistency","text":"The data is in consistent state before and after transaction. Eg: We have a transaction to transfer money from account A to account B. The total amount before and after the transaction must be the same.","title":"Consistency"},{"location":"web/database/transactions/#isolation","text":"If the multiple transactions are running concurrently, they should not be affected by each other.","title":"Isolation"},{"location":"web/database/transactions/#durability","text":"Changes that have been committed to the database should remain even in the case of software and system failure.","title":"Durability"},{"location":"web/database/transactions/#2-read-phenomena","text":"","title":"2. Read phenomena"},{"location":"web/database/transactions/#dirty-read","text":"A dirty read is the situation when a transaction reads a uncommitted data from other transactions.","title":"Dirty read"},{"location":"web/database/transactions/#non-repeatable-read","text":"Non repeatable read occurs when a transaction reads same row twice, and get a different value each time. For example, suppose transaction T1 reads data. Due to concurrency, another transaction T2 updates the same data and commit, Now if transaction T1 rereads the same data, it will retrieve a different value.","title":"Non-repeatable read"},{"location":"web/database/transactions/#phantom-read","text":"Phantom read occurs when two same range queries are executed, but the number of rows retrieved by the two, are different. For example, suppose transaction T1 retrieves a set of rows that satisfy some search criteria. Now, Transaction T2 generates some new rows that match the search criteria for transaction T1. If transaction T1 re-executes the statement that reads the rows, it gets a different set of rows this time.","title":"Phantom read"},{"location":"web/database/transactions/#3-isolation-levels","text":"Isolation Levels Dirty Reads Non-repeatable Reads Phantom Reads Read Uncommitted \u2714 \u2714 \u2714 Read Committed \u2718 \u2714 \u2714 Repeatable Reads \u2718 \u2718 \u2714 Serializable \u2718 \u2718 \u2718 In Repeatable Reads isolation or higher if we are updating row data at the same time another concurrent transaction is also updating the same row then we are getting an error ERROR: could not serialize access due to concurrent update (In MySQL we need Serializable isolation level).","title":"3. Isolation levels"},{"location":"web/networking/tcp/","text":"TCP 1. TCP 3-way handshake Step 1: The client sends a message with the SYN flag = 1 + random sequence number (SEQ = x) to the server. Step 2: After receiving the client's synchronization request, the server sends a message with the SYN flag = 1, ACK flag = 1, Ack number = Client's sequence number + 1 (x + 1) and server random sequence number (SEQ = y). Step 3: After receiving the SYN from the server, the client sends a message to server with ACK flag = 1, Ack number = Server sequence number + 1 (y + 1), SEQ = x + 1 The first two handshakes SYN packets cannot carry data. They are using to establish the sequence number for both client and server. The third handshake to make sure that the client already received the SYN-ACK message from the server and it can carry data. 2. TCP waving 4 times Step 1: The client sends to the server a segment with the FIN flag = 1. Step 2: After receiving the client's FIN segment, the server immediately send ACK message to the client. Step 3: The server sends a segment with the FIN flag = 1 to the client. Step 4: After receiving the server's FIN segment, the client immediately send ACK message to the server and enters TIME_WAIT state. After the wait, the connection formally closes and all resources on the client side (including port numbers and buffer data) are released. TCP connection is full-duplex. There are two channels (read and write), and we must close these channels separately. After the step 2 the server can still send data to the client (connection is in haft-open state). The TIME_WAIT state lets the client resend the final acknowledgment in case the ACK is lost.","title":"TCP"},{"location":"web/networking/tcp/#tcp","text":"","title":"TCP"},{"location":"web/networking/tcp/#1-tcp-3-way-handshake","text":"Step 1: The client sends a message with the SYN flag = 1 + random sequence number (SEQ = x) to the server. Step 2: After receiving the client's synchronization request, the server sends a message with the SYN flag = 1, ACK flag = 1, Ack number = Client's sequence number + 1 (x + 1) and server random sequence number (SEQ = y). Step 3: After receiving the SYN from the server, the client sends a message to server with ACK flag = 1, Ack number = Server sequence number + 1 (y + 1), SEQ = x + 1 The first two handshakes SYN packets cannot carry data. They are using to establish the sequence number for both client and server. The third handshake to make sure that the client already received the SYN-ACK message from the server and it can carry data.","title":"1. TCP 3-way handshake"},{"location":"web/networking/tcp/#2-tcp-waving-4-times","text":"Step 1: The client sends to the server a segment with the FIN flag = 1. Step 2: After receiving the client's FIN segment, the server immediately send ACK message to the client. Step 3: The server sends a segment with the FIN flag = 1 to the client. Step 4: After receiving the server's FIN segment, the client immediately send ACK message to the server and enters TIME_WAIT state. After the wait, the connection formally closes and all resources on the client side (including port numbers and buffer data) are released. TCP connection is full-duplex. There are two channels (read and write), and we must close these channels separately. After the step 2 the server can still send data to the client (connection is in haft-open state). The TIME_WAIT state lets the client resend the final acknowledgment in case the ACK is lost.","title":"2. TCP waving 4 times"},{"location":"web/security/cors/","text":"CORS Cross-origin resource sharing (CORS) is a browser mechanism which enables controlled access to resources located outside of a given domain. Same-origin policy (SOP) The same-origin policy is a web browser security mechanism that aims to prevent websites from attacking each other. The same-origin policy restricts scripts on one origin from accessing data from another origin. An origin consists of a URI scheme, domain and port number. Relaxation of the same-origin policy The same-origin policy is very restrictive and consequently various approaches have been devised to circumvent the constraints. Many websites interact with subdomains or third-party sites in a way that requires full cross-origin access. A controlled relaxation of the same-origin policy is possible using cross-origin resource sharing (CORS). The cross-origin resource sharing protocol uses a suite of HTTP headers that define trusted web origins and associated properties such as whether authenticated access is permitted. These are combined in a header exchange between a browser and the cross-origin web site that it is trying to access. How does CORS work? Simple requests A simple request is one that meets all the following conditions: One of the allowed methods: GET, HEAD, POST A CORS safe-listed header is used When using the Content-Type header, only the following values are allowed: application/x-www-form-urlencoded , multipart/form-data , or text/plain No event listeners are registered on any XMLHttpRequestUpload object No ReadableStream object is used in the request Step 1: Client (browser) request When the browser is making a cross-origin request, the browser adds an Origin header with the current origin (scheme, host, and port). Step 2: Server response On the server side, when a server sees this header, and wants to allow access, it needs to add an Access-Control-Allow-Origin header to the response specifying the requesting origin (or * to allow any origin.) Step 3: Browser receives response When the browser sees this response with an appropriate Access-Control-Allow-Origin header, the browser allows the response data to be shared with the client site. Preflighted requests Unlike simple requests, for \"preflighted\" requests the browser first sends an HTTP request using the OPTIONS method to the resource on the other origin, in order to determine if the actual request is safe to send. Such cross-origin requests are preflighted since they may have implications for user data. For example, a client might be asking a server if it would allow a DELETE request, before sending a DELETE request, by using a preflight request: 1 2 3 4 OPTIONS /resource/foo Access-Control-Request-Method: DELETE Access-Control-Request-Headers: origin, x-requested-with Origin: https://foo.bar.org If the server allows it, then it will respond to the preflight request with an Access-Control-Allow-Methods response header, which lists DELETE : 1 2 3 4 5 HTTP/1.1 204 No Content Connection: keep-alive Access-Control-Allow-Origin: https://foo.bar.org Access-Control-Allow-Methods: POST, GET, OPTIONS, DELETE Access-Control-Max-Age: 86400 The preflight response can be optionally cached for the requests created in the same URL using Access-Control-Max-Age header like in the above example. CORS headers Request headers Header Name Example Value Description Used in preflight requests Used in CORS requests Origin https://www.test.com Combination of protocol, domain, and port of the browser tab opened YES YES Access-Control-Request-Method POST For the preflight request, specifies what method the original CORS request will use YES NO Access-Control-Request-Headers Authorization, X-PING For the preflight request, a comma separated list specifying what headers the original CORS request will send YES NO Response Headers Header Name Example Value Description Used in preflight requests Used in CORS requests Access-Control-Allow-Origin https://www.test.com Item3.1 YES YES Access-Control-Allow-Credentials true Item3.2 YES YES Access-Control-Expose-Headers Date, X-Device-Id Column1 NO YES Access-Control-Max-Age 600 Item3.1 YES NO Access-Control-Allow-Methods GET, POST, PUT, DELETE Item3.2 YES NO Access-Control-Allow-Headers Authorization, X-PING Column1 YES NO Common Pitfalls Using * operator for Access-Control-Allow-Origin. CORS is a relaxation of same-origin policy while attempting to remain secure. Using * disables most security rules of CORS. There are use cases where wildcard is OK such as an open API that integrates into many 3rd party websites. Returning multiple domains for Access-Control-Allow-Origin. Unfortunately, the spec does not allow Access-Control-Allow-Origin: https://mydomain.com, https://www.mydomain.com. The server can only respond with one domain or *, but you can leverage the Origin request header. Using wildcard selection like *.mydomain.com. This is not part of the CORS spec, wildcard can only be used to imply all domains are allowed. Not including protocol or non-standard ports. Access-Control-Allow-Origin: mydomain.com is not valid since the protocol is not included. In a similar way, you will have trouble with Access-Control-Allow-Origin: http://localhost unless the server is actually running on a standard HTTP port like :80. Not including Origin in the Vary response header Most CORS frameworks do this automatically, you must specify to clients that server responses will differ based on the request origin. Not specifying the Access-Control-Expose-Headers If a required header is not included, the CORS request will still pass, but response headers not whitelisted will be hidden from the browser tab. The default response headers always exposed for CORS requests are: Cache-Control Content-Language Content-Type Expires Last-Modified Pragma Using wildcard when Access-Control-Allow-Credentials is set to true This is a tricky case that catches many people. If response has Access-Control-Allow-Credentials: true , then the wildcard operator cannot be used on any of the response headers like Access-Control-Allow-Origin .","title":"CORS"},{"location":"web/security/cors/#cors","text":"Cross-origin resource sharing (CORS) is a browser mechanism which enables controlled access to resources located outside of a given domain.","title":"CORS"},{"location":"web/security/cors/#same-origin-policy-sop","text":"The same-origin policy is a web browser security mechanism that aims to prevent websites from attacking each other. The same-origin policy restricts scripts on one origin from accessing data from another origin. An origin consists of a URI scheme, domain and port number.","title":"Same-origin policy (SOP)"},{"location":"web/security/cors/#relaxation-of-the-same-origin-policy","text":"The same-origin policy is very restrictive and consequently various approaches have been devised to circumvent the constraints. Many websites interact with subdomains or third-party sites in a way that requires full cross-origin access. A controlled relaxation of the same-origin policy is possible using cross-origin resource sharing (CORS). The cross-origin resource sharing protocol uses a suite of HTTP headers that define trusted web origins and associated properties such as whether authenticated access is permitted. These are combined in a header exchange between a browser and the cross-origin web site that it is trying to access.","title":"Relaxation of the same-origin policy"},{"location":"web/security/cors/#how-does-cors-work","text":"","title":"How does CORS work?"},{"location":"web/security/cors/#simple-requests","text":"A simple request is one that meets all the following conditions: One of the allowed methods: GET, HEAD, POST A CORS safe-listed header is used When using the Content-Type header, only the following values are allowed: application/x-www-form-urlencoded , multipart/form-data , or text/plain No event listeners are registered on any XMLHttpRequestUpload object No ReadableStream object is used in the request Step 1: Client (browser) request When the browser is making a cross-origin request, the browser adds an Origin header with the current origin (scheme, host, and port). Step 2: Server response On the server side, when a server sees this header, and wants to allow access, it needs to add an Access-Control-Allow-Origin header to the response specifying the requesting origin (or * to allow any origin.) Step 3: Browser receives response When the browser sees this response with an appropriate Access-Control-Allow-Origin header, the browser allows the response data to be shared with the client site.","title":"Simple requests"},{"location":"web/security/cors/#preflighted-requests","text":"Unlike simple requests, for \"preflighted\" requests the browser first sends an HTTP request using the OPTIONS method to the resource on the other origin, in order to determine if the actual request is safe to send. Such cross-origin requests are preflighted since they may have implications for user data. For example, a client might be asking a server if it would allow a DELETE request, before sending a DELETE request, by using a preflight request: 1 2 3 4 OPTIONS /resource/foo Access-Control-Request-Method: DELETE Access-Control-Request-Headers: origin, x-requested-with Origin: https://foo.bar.org If the server allows it, then it will respond to the preflight request with an Access-Control-Allow-Methods response header, which lists DELETE : 1 2 3 4 5 HTTP/1.1 204 No Content Connection: keep-alive Access-Control-Allow-Origin: https://foo.bar.org Access-Control-Allow-Methods: POST, GET, OPTIONS, DELETE Access-Control-Max-Age: 86400 The preflight response can be optionally cached for the requests created in the same URL using Access-Control-Max-Age header like in the above example.","title":"Preflighted requests"},{"location":"web/security/cors/#cors-headers","text":"Request headers Header Name Example Value Description Used in preflight requests Used in CORS requests Origin https://www.test.com Combination of protocol, domain, and port of the browser tab opened YES YES Access-Control-Request-Method POST For the preflight request, specifies what method the original CORS request will use YES NO Access-Control-Request-Headers Authorization, X-PING For the preflight request, a comma separated list specifying what headers the original CORS request will send YES NO Response Headers Header Name Example Value Description Used in preflight requests Used in CORS requests Access-Control-Allow-Origin https://www.test.com Item3.1 YES YES Access-Control-Allow-Credentials true Item3.2 YES YES Access-Control-Expose-Headers Date, X-Device-Id Column1 NO YES Access-Control-Max-Age 600 Item3.1 YES NO Access-Control-Allow-Methods GET, POST, PUT, DELETE Item3.2 YES NO Access-Control-Allow-Headers Authorization, X-PING Column1 YES NO","title":"CORS headers"},{"location":"web/security/cors/#common-pitfalls","text":"Using * operator for Access-Control-Allow-Origin. CORS is a relaxation of same-origin policy while attempting to remain secure. Using * disables most security rules of CORS. There are use cases where wildcard is OK such as an open API that integrates into many 3rd party websites. Returning multiple domains for Access-Control-Allow-Origin. Unfortunately, the spec does not allow Access-Control-Allow-Origin: https://mydomain.com, https://www.mydomain.com. The server can only respond with one domain or *, but you can leverage the Origin request header. Using wildcard selection like *.mydomain.com. This is not part of the CORS spec, wildcard can only be used to imply all domains are allowed. Not including protocol or non-standard ports. Access-Control-Allow-Origin: mydomain.com is not valid since the protocol is not included. In a similar way, you will have trouble with Access-Control-Allow-Origin: http://localhost unless the server is actually running on a standard HTTP port like :80. Not including Origin in the Vary response header Most CORS frameworks do this automatically, you must specify to clients that server responses will differ based on the request origin. Not specifying the Access-Control-Expose-Headers If a required header is not included, the CORS request will still pass, but response headers not whitelisted will be hidden from the browser tab. The default response headers always exposed for CORS requests are: Cache-Control Content-Language Content-Type Expires Last-Modified Pragma Using wildcard when Access-Control-Allow-Credentials is set to true This is a tricky case that catches many people. If response has Access-Control-Allow-Credentials: true , then the wildcard operator cannot be used on any of the response headers like Access-Control-Allow-Origin .","title":"Common Pitfalls"},{"location":"web/security/jwt/","text":"JWT 1. What is JWT? JWT, or JSON Web Token, is an open standard used to share security information between two parties \u2014 a client and a server. Each JWT contains encoded JSON objects, including a set of claims. JWTs are signed using a cryptographic algorithm to ensure that the claims cannot be altered after the token is issued. 2. What is the JSON Web Token structure? A JSON Web Token consists of 3 parts separated by a period. header.payload.signature 2.1 Header JWT header consists of token type and algorithm used for signing and encoding. Algorithms can be HMAC, SHA256, RSA, HS256 or RS256. Eg: 1 2 3 4 { \"typ\" : \"JWT\" , \"alg\" : \"HS256\" } Then, this JSON is Base64Url encoded to form the first part of the JWT. 2.2 Payload Payload consists of the session data called as claims. Below are some of the the standard claims that we can use, Issuer(iss) Subject (sub) Audience (aud) Expiration time (exp) Issued at (iat) Eg: 1 2 3 4 { \"sub\" : \"user10001\" , \"iat\" : 1569302116 } Custom claims can also be included in the claim set. When using custom claim sets, Do not put large data in claim sets. Claim sets meant to be compact. Do not put sensitive informations since, JWT can be decoded easily. 1 2 3 4 5 6 { \"sub\" : \"user10001\" , \"iat\" : 1569302116 , \"role\" : \"admin\" , \"user_id\" : \"user10001\" } The payload is then Base64Url encoded to form the second part of the JSON Web Token. 2.3 Signature To create the signature part you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that. For example if you want to use the HMAC SHA256 algorithm, the signature will be created in the following way: 1 2 3 4 HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) The signature is used to verify the message wasn't changed along the way, and, in the case of tokens signed with a private key, it can also verify that the sender of the JWT is who it says it is. 3. How do JSON Web Tokens work?","title":"JWT"},{"location":"web/security/jwt/#jwt","text":"","title":"JWT"},{"location":"web/security/jwt/#1-what-is-jwt","text":"JWT, or JSON Web Token, is an open standard used to share security information between two parties \u2014 a client and a server. Each JWT contains encoded JSON objects, including a set of claims. JWTs are signed using a cryptographic algorithm to ensure that the claims cannot be altered after the token is issued.","title":"1. What is JWT?"},{"location":"web/security/jwt/#2-what-is-the-json-web-token-structure","text":"A JSON Web Token consists of 3 parts separated by a period. header.payload.signature","title":"2. What is the JSON Web Token structure?"},{"location":"web/security/jwt/#21-header","text":"JWT header consists of token type and algorithm used for signing and encoding. Algorithms can be HMAC, SHA256, RSA, HS256 or RS256. Eg: 1 2 3 4 { \"typ\" : \"JWT\" , \"alg\" : \"HS256\" } Then, this JSON is Base64Url encoded to form the first part of the JWT.","title":"2.1 Header"},{"location":"web/security/jwt/#22-payload","text":"Payload consists of the session data called as claims. Below are some of the the standard claims that we can use, Issuer(iss) Subject (sub) Audience (aud) Expiration time (exp) Issued at (iat) Eg: 1 2 3 4 { \"sub\" : \"user10001\" , \"iat\" : 1569302116 } Custom claims can also be included in the claim set. When using custom claim sets, Do not put large data in claim sets. Claim sets meant to be compact. Do not put sensitive informations since, JWT can be decoded easily. 1 2 3 4 5 6 { \"sub\" : \"user10001\" , \"iat\" : 1569302116 , \"role\" : \"admin\" , \"user_id\" : \"user10001\" } The payload is then Base64Url encoded to form the second part of the JSON Web Token.","title":"2.2 Payload"},{"location":"web/security/jwt/#23-signature","text":"To create the signature part you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that. For example if you want to use the HMAC SHA256 algorithm, the signature will be created in the following way: 1 2 3 4 HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret) The signature is used to verify the message wasn't changed along the way, and, in the case of tokens signed with a private key, it can also verify that the sender of the JWT is who it says it is.","title":"2.3 Signature"},{"location":"web/security/jwt/#3-how-do-json-web-tokens-work","text":"","title":"3. How do JSON Web Tokens work?"},{"location":"web/security/oauth2/","text":"OAuth2 1. Authentication vs Authorization Authentication Authentication is the process of proving your own identity to third party service. So when we are trying to log in to Facebook or Google, we are required to first enter the email and password to verify our identity. This is what Authentication is. Authorization Authorization is the process of giving someone permission to do something or have something. It is done after successful Authentication. 2. OAuth2 OAuth 2.0 is the industry-standard protocol for authorization and anyone can implement it. OAuth 2.0 terminology: Resource Owner : The user who owns the data that the client application wants to access. Client: The application that wants access to the user\u2019s data. Authorization Server: The authorization server authorizes the client to access a user\u2019s data by granting permission from the user. Resource Server: The system that holds the data that the client wants to access. In some cases, the resource server and authorization server are the same. Access token: An access token is the only key that the client can use to access the granted data by the user on the resource server. 3. OAuth2 flow Authorization Code Flow The client begins the authorization sequence by redirecting the user to the authorization server with response_type set to code this tells the authorization server to respond with an authorization code. 1 2 3 4 5 https://accounts.google.com/o/oauth2/v2/auth? response_type=code& client_id=your_client_id& scope=profile%20contacts& redirect_uri=https%3A//oauth2.example.com/code The result of this request is an authorization code, which the client can exchange for an access token. The authorization code looks like this: 1 4/W7q7P51a-iMsCeLvIaQc6bYrgtp9 Why exchange code for a token? An access token is a secret piece of information that we don\u2019t want someone to access. If the client requests an access token directly and stores it in the browser, it can be stolen because browsers are not fully secure. Anyone can see the page source or potentially use dev tools to acquire the access token. To avoid exposure of the access token in the browser, the front end channel of the client gets the application code from the authorization server, then it sends the application code to the client\u2019s back end channel. Now to exchange this application code for an access token, a thing called client_secret is needed. The client_secret is only known by the client\u2019s back-end channel The request might look something like this: 1 2 3 4 5 6 7 8 POST /token HTTP/1.1 Host: oauth2.googleapis.com Content-Type: application/x-www-form-urlencoded code=4/W7q7P51a-iMsCeLvIaQc6bYrgtp9& client_id=your_client_id& client_secret=your_client_secret_only_known_by_server& redirect_uri=https%3A//oauth2.example.com/code Implicit flow The OAuth 2.0 implicit flow is used when you don\u2019t have a back end channel and your website is a static site that uses only the browser. The client redirects the browser to the authorization server URI to start the authorization flow with response_type set to token Implicit flow is considered less secure because the browser is responsible for managing the access token, so it could potentially be stolen. Still, it\u2019s widely used for single-page applications. 4. OpenID Connect OpenID Connect is an identity layer on top of the OAuth 2.0 protocol. It extends OAuth 2.0 to standardize a way for authentication. OpenID is about verifying a person's identity (authentication). OAuth is about accessing a person's stuff (authorization). OpenID Connect does both.","title":"OAuth2"},{"location":"web/security/oauth2/#oauth2","text":"","title":"OAuth2"},{"location":"web/security/oauth2/#1-authentication-vs-authorization","text":"","title":"1. Authentication vs Authorization"},{"location":"web/security/oauth2/#authentication","text":"Authentication is the process of proving your own identity to third party service. So when we are trying to log in to Facebook or Google, we are required to first enter the email and password to verify our identity. This is what Authentication is.","title":"Authentication"},{"location":"web/security/oauth2/#authorization","text":"Authorization is the process of giving someone permission to do something or have something. It is done after successful Authentication.","title":"Authorization"},{"location":"web/security/oauth2/#2-oauth2","text":"OAuth 2.0 is the industry-standard protocol for authorization and anyone can implement it. OAuth 2.0 terminology: Resource Owner : The user who owns the data that the client application wants to access. Client: The application that wants access to the user\u2019s data. Authorization Server: The authorization server authorizes the client to access a user\u2019s data by granting permission from the user. Resource Server: The system that holds the data that the client wants to access. In some cases, the resource server and authorization server are the same. Access token: An access token is the only key that the client can use to access the granted data by the user on the resource server.","title":"2. OAuth2"},{"location":"web/security/oauth2/#3-oauth2-flow","text":"","title":"3. OAuth2 flow"},{"location":"web/security/oauth2/#authorization-code-flow","text":"The client begins the authorization sequence by redirecting the user to the authorization server with response_type set to code this tells the authorization server to respond with an authorization code. 1 2 3 4 5 https://accounts.google.com/o/oauth2/v2/auth? response_type=code& client_id=your_client_id& scope=profile%20contacts& redirect_uri=https%3A//oauth2.example.com/code The result of this request is an authorization code, which the client can exchange for an access token. The authorization code looks like this: 1 4/W7q7P51a-iMsCeLvIaQc6bYrgtp9 Why exchange code for a token? An access token is a secret piece of information that we don\u2019t want someone to access. If the client requests an access token directly and stores it in the browser, it can be stolen because browsers are not fully secure. Anyone can see the page source or potentially use dev tools to acquire the access token. To avoid exposure of the access token in the browser, the front end channel of the client gets the application code from the authorization server, then it sends the application code to the client\u2019s back end channel. Now to exchange this application code for an access token, a thing called client_secret is needed. The client_secret is only known by the client\u2019s back-end channel The request might look something like this: 1 2 3 4 5 6 7 8 POST /token HTTP/1.1 Host: oauth2.googleapis.com Content-Type: application/x-www-form-urlencoded code=4/W7q7P51a-iMsCeLvIaQc6bYrgtp9& client_id=your_client_id& client_secret=your_client_secret_only_known_by_server& redirect_uri=https%3A//oauth2.example.com/code","title":"Authorization Code Flow"},{"location":"web/security/oauth2/#implicit-flow","text":"The OAuth 2.0 implicit flow is used when you don\u2019t have a back end channel and your website is a static site that uses only the browser. The client redirects the browser to the authorization server URI to start the authorization flow with response_type set to token Implicit flow is considered less secure because the browser is responsible for managing the access token, so it could potentially be stolen. Still, it\u2019s widely used for single-page applications.","title":"Implicit flow"},{"location":"web/security/oauth2/#4-openid-connect","text":"OpenID Connect is an identity layer on top of the OAuth 2.0 protocol. It extends OAuth 2.0 to standardize a way for authentication. OpenID is about verifying a person's identity (authentication). OAuth is about accessing a person's stuff (authorization). OpenID Connect does both.","title":"4. OpenID Connect"},{"location":"web/webrtc/connecting/","text":"Connecting 1. Peer to peer connection WebRTC does\u2019t use a client/server model, it establishes peer-to-peer (P2P) connections. 2. How does it work 2.1 Networking real-world constraints Most of the time the other WebRTC Agent will not even be in the same network. A typical call is usually between two WebRTC Agents in different networks with no direct connectivity. For hosts in the same network it is very easy to connect. However, a host using Router B has no way to directly access anything behind Router A . How would you tell the different between 191.168.0.1 behind Router A and the same IP behind Router B ? They are private IPs. A host using Router B could send traffic directly to Router A , but the request would end there. How does Router A know which host it should forward the message to? 2.2 NAT Mapping IP version 4 addresses are only 32 bit(4 byte) long, which provides 4.29 billion(2 to the power of 32 = 4,294,967,296) unique IP addresses. 4.29 billion address space not enough for give public IP address for all available hosts. To solve this issue NAT devices introduced. These devices would be responsible for maintaining a table mapping of local IP(private IP) and port tuples to one or more globally unique IP(public IP) and port tuples. By using this technology firewalls and routers allow multiple devices on a LAN with private IP addresses to share a single public IP address There are different types of NATs, but some of them allocates a public IP address and a port for UDP flows (what we need). When you want to create a P2P connection with a peer, a first challenge is therefore to discover what kind of NATs you are behind, and if they exist, to get an IP address and a port you can give to your contact. 2.3 STUN Session Traversal Utilities for NAT (STUN) protocol enables a device to discover its public IP address. STUN relies on a simple observation: when you talk to a server on the internet from a NATed client, the server sees the public ip:port that your NAT device created for you, not your LAN ip:port . So, the server can tell you what ip:port it saw. That way, you know what traffic from your LAN ip:port looks like on the internet, you can tell your peers about that mapping, and now they know where to send packets. 2.4 NAT implementations NATs are not all implemented the same way and may differ in how they allow packets to go through. Some NAT implementations, like the One-to-one NATs, will allow a P2P connection to be established. Some, like the symmetric ones, don\u2019t. One-to-one NAT (or Full Cone NAT) In this implementation, once an internal IP address/ port duo is mapped to the external IP adress/ port duo, all packages arriving to the external address/ port, no matter where they are coming from, will be sent through to the original internal one. Symmetric NAT A symmetric NAT is one where all requests from the same internal IP address and port, to a specific destination IP address and port, are mapped to the same external IP address and port. If the same host sends a packet with the same source address and port, but to a different destination, a different mapping is used. Furthermore, only the external host that receives a packet can send a UDP packet back to the internal host. 2.5 TURN TURN (Traversal Using Relays around NAT) is the solution when direct connectivity isn\u2019t possible. It could be because a symmetric NAT is in use! WebRTC agent bypass the Symmetric NAT restriction by opening a connection with a TURN server and relaying all information through that server. You would create a connection with a TURN server and tell all peers to send packets to the server which will then be forwarded to you. This obviously comes with some overhead so it is only used if there are no other alternatives. 2.6 ICE ICE (Interactive Connectivity Establishment) is how WebRTC connects two Agents. ICE is a protocol for establishing connectivity. It determines all the possible routes between the two peers and then ensures you stay connected. These routes are known as Candidate Pairs , which is a pairing of a local and remote transport address. This is where STUN and TURN come into play with ICE. These addresses can be your local IP Address plus a port , NAT mapping , or Relayed Transport Address (TURN). Each side gathers all the addresses they want to use, exchanges them, and then attempts to connect! An ICE Agent is either Controlling or Controlled. The Controlling Agent is the one that decides the selected Candidate Pair. Usually, the peer sending the offer is the controlling side. Each side must have a user fragment and a password . These two values must be exchanged before connectivity checks can even begin. The user fragment is sent in plain text and is useful for demuxing multiple ICE Sessions. The password is used to generate a MESSAGE-INTEGRITY attribute. At the end of each STUN packet, there is an attribute that is a hash of the entire packet using the password as a key. This is used to authenticate the packet and ensure it hasn\u2019t been tampered with.","title":"Connecting"},{"location":"web/webrtc/connecting/#connecting","text":"","title":"Connecting"},{"location":"web/webrtc/connecting/#1-peer-to-peer-connection","text":"WebRTC does\u2019t use a client/server model, it establishes peer-to-peer (P2P) connections.","title":"1. Peer to peer connection"},{"location":"web/webrtc/connecting/#2-how-does-it-work","text":"","title":"2. How does it work"},{"location":"web/webrtc/connecting/#21-networking-real-world-constraints","text":"Most of the time the other WebRTC Agent will not even be in the same network. A typical call is usually between two WebRTC Agents in different networks with no direct connectivity. For hosts in the same network it is very easy to connect. However, a host using Router B has no way to directly access anything behind Router A . How would you tell the different between 191.168.0.1 behind Router A and the same IP behind Router B ? They are private IPs. A host using Router B could send traffic directly to Router A , but the request would end there. How does Router A know which host it should forward the message to?","title":"2.1 Networking real-world constraints"},{"location":"web/webrtc/connecting/#22-nat-mapping","text":"IP version 4 addresses are only 32 bit(4 byte) long, which provides 4.29 billion(2 to the power of 32 = 4,294,967,296) unique IP addresses. 4.29 billion address space not enough for give public IP address for all available hosts. To solve this issue NAT devices introduced. These devices would be responsible for maintaining a table mapping of local IP(private IP) and port tuples to one or more globally unique IP(public IP) and port tuples. By using this technology firewalls and routers allow multiple devices on a LAN with private IP addresses to share a single public IP address There are different types of NATs, but some of them allocates a public IP address and a port for UDP flows (what we need). When you want to create a P2P connection with a peer, a first challenge is therefore to discover what kind of NATs you are behind, and if they exist, to get an IP address and a port you can give to your contact.","title":"2.2 NAT Mapping"},{"location":"web/webrtc/connecting/#23-stun","text":"Session Traversal Utilities for NAT (STUN) protocol enables a device to discover its public IP address. STUN relies on a simple observation: when you talk to a server on the internet from a NATed client, the server sees the public ip:port that your NAT device created for you, not your LAN ip:port . So, the server can tell you what ip:port it saw. That way, you know what traffic from your LAN ip:port looks like on the internet, you can tell your peers about that mapping, and now they know where to send packets.","title":"2.3 STUN"},{"location":"web/webrtc/connecting/#24-nat-implementations","text":"NATs are not all implemented the same way and may differ in how they allow packets to go through. Some NAT implementations, like the One-to-one NATs, will allow a P2P connection to be established. Some, like the symmetric ones, don\u2019t. One-to-one NAT (or Full Cone NAT) In this implementation, once an internal IP address/ port duo is mapped to the external IP adress/ port duo, all packages arriving to the external address/ port, no matter where they are coming from, will be sent through to the original internal one. Symmetric NAT A symmetric NAT is one where all requests from the same internal IP address and port, to a specific destination IP address and port, are mapped to the same external IP address and port. If the same host sends a packet with the same source address and port, but to a different destination, a different mapping is used. Furthermore, only the external host that receives a packet can send a UDP packet back to the internal host.","title":"2.4 NAT implementations"},{"location":"web/webrtc/connecting/#25-turn","text":"TURN (Traversal Using Relays around NAT) is the solution when direct connectivity isn\u2019t possible. It could be because a symmetric NAT is in use! WebRTC agent bypass the Symmetric NAT restriction by opening a connection with a TURN server and relaying all information through that server. You would create a connection with a TURN server and tell all peers to send packets to the server which will then be forwarded to you. This obviously comes with some overhead so it is only used if there are no other alternatives.","title":"2.5 TURN"},{"location":"web/webrtc/connecting/#26-ice","text":"ICE (Interactive Connectivity Establishment) is how WebRTC connects two Agents. ICE is a protocol for establishing connectivity. It determines all the possible routes between the two peers and then ensures you stay connected. These routes are known as Candidate Pairs , which is a pairing of a local and remote transport address. This is where STUN and TURN come into play with ICE. These addresses can be your local IP Address plus a port , NAT mapping , or Relayed Transport Address (TURN). Each side gathers all the addresses they want to use, exchanges them, and then attempts to connect! An ICE Agent is either Controlling or Controlled. The Controlling Agent is the one that decides the selected Candidate Pair. Usually, the peer sending the offer is the controlling side. Each side must have a user fragment and a password . These two values must be exchanged before connectivity checks can even begin. The user fragment is sent in plain text and is useful for demuxing multiple ICE Sessions. The password is used to generate a MESSAGE-INTEGRITY attribute. At the end of each STUN packet, there is an attribute that is a hash of the entire packet using the password as a key. This is used to authenticate the packet and ensure it hasn\u2019t been tampered with.","title":"2.6 ICE"},{"location":"web/webrtc/example/","text":"Example Starting a call 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 function invite ( evt ) { if ( myPeerConnection ) { alert ( \"You can't start a call because you already have one open!\" ); } else { createPeerConnection (); navigator . mediaDevices . getUserMedia ({ audio : true , // We want an audio track video : true // ...and we want a video track }) . then ( function ( localStream ) { document . getElementById ( \"local_video\" ). srcObject = localStream ; localStream . getTracks (). forEach ( track => myPeerConnection . addTrack ( track , localStream )); }) . catch ( handleGetUserMediaError ); } } function handleGetUserMediaError ( e ) { switch ( e . name ) { case \"NotFoundError\" : alert ( \"Unable to open your call because no camera and/or microphone\" + \"were found.\" ); break ; case \"SecurityError\" : case \"PermissionDeniedError\" : // Do nothing; this is the same as the user canceling the call. break ; default : alert ( \"Error opening your camera and/or microphone: \" + e . message ); break ; } closeVideoCall (); } In the above example, we create peer connection via createPeerConnection method. Once the RTCPeerConnection has been created, we request access to the user's camera and microphone by calling MediaDevices.getUserMedia() which is exposed to us through the MediaDevices.getUserMedia property. We attach the incoming stream to the local preview <video> element by setting the element's srcObject property. We then iterate over the tracks in the stream, calling addTrack() to add each track to the RTCPeerConnection . Even though the connection is not fully established yet, you can begin sending data when you feel it's appropriate to do so. Media received before the ICE negotiation is completed may be used to help ICE decide upon the best connectivity approach to take, thus aiding in the negotiation process. Note that for native apps, such as a phone application, you should not begin sending until the connection has been accepted at both ends, at a minimum, to avoid inadvertently sending video and/or audio data when the user isn't prepared for it. Creating the peer connection 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function createPeerConnection () { myPeerConnection = new RTCPeerConnection ({ iceServers : [ // Information about ICE servers - Use your own! { urls : \"stun:stun.stunprotocol.org\" } ] }); myPeerConnection . onicecandidate = handleICECandidateEvent ; myPeerConnection . ontrack = handleTrackEvent ; myPeerConnection . onnegotiationneeded = handleNegotiationNeededEvent ; myPeerConnection . onremovetrack = handleRemoveTrackEvent ; myPeerConnection . oniceconnectionstatechange = handleICEConnectionStateChangeEvent ; myPeerConnection . onicegatheringstatechange = handleICEGatheringStateChangeEvent ; myPeerConnection . onsignalingstatechange = handleSignalingStateChangeEvent ; } The createPeerConnection() function is used by both the caller and the callee to construct their RTCPeerConnection objects, their respective ends of the WebRTC connection. When using the RTCPeerConnection() constructor, we will specify an RTCConfiguration-compliant object providing configuration parameters for the connection. This is an array of objects describing STUN and/or TURN servers for the ICE layer to use when attempting to establish a route between the caller and the callee. After creating the RTCPeerConnection, we set up handlers for the events that matter to us. The first three of these event handlers are required; you have to handle them to do anything involving streamed media with WebRTC. Starting negotiation Once the caller has created its RTCPeerConnection , created a media stream, and added its tracks to the connection, the browser will deliver a negotiationneeded event to the RTCPeerConnection to indicate that it's ready to begin negotiation with the other peer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 function handleNegotiationNeededEvent () { myPeerConnection . createOffer (). then ( function ( offer ) { return myPeerConnection . setLocalDescription ( offer ); }) . then ( function () { sendToServer ({ name : myUsername , target : targetUsername , type : \"video-offer\" , sdp : myPeerConnection . localDescription }); }) . catch ( reportError ); } To start the negotiation process, we need to create and send an SDP offer to the peer we want to connect to. This offer includes a list of supported configurations for the connection, including information about the media stream we've added to the connection locally, and any ICE candidates gathered by the ICE layer already. We create this offer by calling myPeerConnection.createOffer() . When createOffer() succeeds, we pass the created offer information into myPeerConnection.setLocalDescription() , which configures the connection and media configuration state for the caller's end of the connection. We know the description is valid, and has been set, this is when we send our offer to the other peer by creating a new \"video-offer\" message containing the local description, then sending it through our signaling server to the callee. Handling the invitation When the offer arrives, the callee's handleVideoOfferMsg() function is called with the \"video-offer\" message that was received 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function handleVideoOfferMsg ( msg ) { var localStream = null ; targetUsername = msg . name ; createPeerConnection (); var desc = new RTCSessionDescription ( msg . sdp ); myPeerConnection . setRemoteDescription ( desc ). then ( function () { return navigator . mediaDevices . getUserMedia ( mediaConstraints ); }) . then ( function ( stream ) { localStream = stream ; document . getElementById ( \"local_video\" ). srcObject = localStream ; localStream . getTracks (). forEach ( track => myPeerConnection . addTrack ( track , localStream )); }) . then ( function () { return myPeerConnection . createAnswer (); }) . then ( function ( answer ) { return myPeerConnection . setLocalDescription ( answer ); }) . then ( function () { var msg = { name : myUsername , target : targetUsername , type : \"video-answer\" , sdp : myPeerConnection . localDescription }; sendToServer ( msg ); }) . catch ( handleGetUserMediaError ); } This code is very similar to what we did in the invite() function. It starts by creating and configuring an RTCPeerConnection using our createPeerConnection() function. Then it takes the SDP offer from the received \"video-offer\" message and uses it to create a new RTCSessionDescription object representing the caller's session description. That session description is then passed into myPeerConnection.setRemoteDescription(). This establishes the received offer as the description of the remote (caller's) end of the connection. Once the answer has been created using myPeerConnection.createAnswer(), the description of the local end of the connection is set to the answer's SDP by calling myPeerConnection.setLocalDescription(), then the answer is transmitted through the signaling server to the caller to let them know what the answer is. Sending ICE candidates The ICE negotiation process involves each peer sending candidates to the other, repeatedly, until it runs out of potential ways it can support the RTCPeerConnection 's media transport needs. Once setLocalDescription() 's fulfillment handler has run, the ICE agent begins sending icecandidate events to the RTCPeerConnection one for each potential configuration it discovers. 1 2 3 4 5 6 7 8 9 10 11 12 function handleICECandidateEvent ( event ) { if ( ! e . candidate ) { // All candidates have been sent return } sendToServer ({ type : \"new-ice-candidate\" , target : targetUsername , candidate : event . candidate }); } Receiving ICE candidates 1 2 3 4 5 6 function handleNewICECandidateMsg ( msg ) { var candidate = new RTCIceCandidate ( msg . candidate ); myPeerConnection . addIceCandidate ( candidate ) . catch ( reportError ); } This function constructs an RTCIceCandidate object by passing the received SDP into its constructor, then delivers the candidate to the ICE layer by passing it into myPeerConnection.addIceCandidate() . Receiving new streams When new tracks are added to the RTCPeerConnection \u2014 either by calling its addTrack() method or because of renegotiation of the stream's format\u2014a track event is set to the RTCPeerConnection for each track added to the connection. 1 2 3 4 function handleTrackEvent ( event ) { document . getElementById ( \"received_video\" ). srcObject = event . streams [ 0 ]; document . getElementById ( \"hangup-button\" ). disabled = false ; } The incoming stream is attached to the \"received_video\" <video> element. Handling the removal of tracks Your code receives a removetrack event when the remote peer removes a track from the connection by calling RTCPeerConnection.removeTrack(). 1 2 3 4 5 6 7 8 function handleRemoveTrackEvent ( event ) { var stream = document . getElementById ( \"received_video\" ). srcObject ; var trackList = stream . getTracks (); if ( trackList . length == 0 ) { closeVideoCall (); } } Ending the call When the user clicks the \"Hang Up\" button to end the call, the hangUpCall() function is called: 1 2 3 4 5 6 7 8 function hangUpCall () { closeVideoCall (); sendToServer ({ name : myUsername , target : targetUsername , type : \"hang-up\" }); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 function closeVideoCall () { var remoteVideo = document . getElementById ( \"received_video\" ); var localVideo = document . getElementById ( \"local_video\" ); if ( myPeerConnection ) { myPeerConnection . ontrack = null ; myPeerConnection . onremovetrack = null ; myPeerConnection . onremovestream = null ; myPeerConnection . onicecandidate = null ; myPeerConnection . oniceconnectionstatechange = null ; myPeerConnection . onsignalingstatechange = null ; myPeerConnection . onicegatheringstatechange = null ; myPeerConnection . onnegotiationneeded = null ; if ( remoteVideo . srcObject ) { remoteVideo . srcObject . getTracks (). forEach ( track => track . stop ()); } if ( localVideo . srcObject ) { localVideo . srcObject . getTracks (). forEach ( track => track . stop ()); } myPeerConnection . close (); myPeerConnection = null ; } remoteVideo . removeAttribute ( \"src\" ); remoteVideo . removeAttribute ( \"srcObject\" ); localVideo . removeAttribute ( \"src\" ); remoteVideo . removeAttribute ( \"srcObject\" ); document . getElementById ( \"hangup-button\" ). disabled = true ; targetUsername = null ; } Dealing with state changes ICE connection state iceconnectionstatechange events are sent to the RTCPeerConnection by the ICE layer when the connection state changes (such as when the call is terminated from the other end). 1 2 3 4 5 6 7 8 function handleICEConnectionStateChangeEvent ( event ) { switch ( myPeerConnection . iceConnectionState ) { case \"closed\" : case \"failed\" : closeVideoCall (); break ; } } ICE signaling state 1 2 3 4 5 6 7 function handleSignalingStateChangeEvent ( event ) { switch ( myPeerConnection . signalingState ) { case \"closed\" : closeVideoCall (); break ; } }; ICE gathering state icegatheringstatechange events are used to let you know when the ICE candidate gathering process state changes 1 2 3 4 function handleICEGatheringStateChangeEvent ( event ) { // Our sample just logs information to console here, // but you can do whatever you need. }","title":"Example"},{"location":"web/webrtc/example/#example","text":"","title":"Example"},{"location":"web/webrtc/example/#starting-a-call","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 function invite ( evt ) { if ( myPeerConnection ) { alert ( \"You can't start a call because you already have one open!\" ); } else { createPeerConnection (); navigator . mediaDevices . getUserMedia ({ audio : true , // We want an audio track video : true // ...and we want a video track }) . then ( function ( localStream ) { document . getElementById ( \"local_video\" ). srcObject = localStream ; localStream . getTracks (). forEach ( track => myPeerConnection . addTrack ( track , localStream )); }) . catch ( handleGetUserMediaError ); } } function handleGetUserMediaError ( e ) { switch ( e . name ) { case \"NotFoundError\" : alert ( \"Unable to open your call because no camera and/or microphone\" + \"were found.\" ); break ; case \"SecurityError\" : case \"PermissionDeniedError\" : // Do nothing; this is the same as the user canceling the call. break ; default : alert ( \"Error opening your camera and/or microphone: \" + e . message ); break ; } closeVideoCall (); } In the above example, we create peer connection via createPeerConnection method. Once the RTCPeerConnection has been created, we request access to the user's camera and microphone by calling MediaDevices.getUserMedia() which is exposed to us through the MediaDevices.getUserMedia property. We attach the incoming stream to the local preview <video> element by setting the element's srcObject property. We then iterate over the tracks in the stream, calling addTrack() to add each track to the RTCPeerConnection . Even though the connection is not fully established yet, you can begin sending data when you feel it's appropriate to do so. Media received before the ICE negotiation is completed may be used to help ICE decide upon the best connectivity approach to take, thus aiding in the negotiation process. Note that for native apps, such as a phone application, you should not begin sending until the connection has been accepted at both ends, at a minimum, to avoid inadvertently sending video and/or audio data when the user isn't prepared for it.","title":"Starting a call"},{"location":"web/webrtc/example/#creating-the-peer-connection","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function createPeerConnection () { myPeerConnection = new RTCPeerConnection ({ iceServers : [ // Information about ICE servers - Use your own! { urls : \"stun:stun.stunprotocol.org\" } ] }); myPeerConnection . onicecandidate = handleICECandidateEvent ; myPeerConnection . ontrack = handleTrackEvent ; myPeerConnection . onnegotiationneeded = handleNegotiationNeededEvent ; myPeerConnection . onremovetrack = handleRemoveTrackEvent ; myPeerConnection . oniceconnectionstatechange = handleICEConnectionStateChangeEvent ; myPeerConnection . onicegatheringstatechange = handleICEGatheringStateChangeEvent ; myPeerConnection . onsignalingstatechange = handleSignalingStateChangeEvent ; } The createPeerConnection() function is used by both the caller and the callee to construct their RTCPeerConnection objects, their respective ends of the WebRTC connection. When using the RTCPeerConnection() constructor, we will specify an RTCConfiguration-compliant object providing configuration parameters for the connection. This is an array of objects describing STUN and/or TURN servers for the ICE layer to use when attempting to establish a route between the caller and the callee. After creating the RTCPeerConnection, we set up handlers for the events that matter to us. The first three of these event handlers are required; you have to handle them to do anything involving streamed media with WebRTC.","title":"Creating the peer connection"},{"location":"web/webrtc/example/#starting-negotiation","text":"Once the caller has created its RTCPeerConnection , created a media stream, and added its tracks to the connection, the browser will deliver a negotiationneeded event to the RTCPeerConnection to indicate that it's ready to begin negotiation with the other peer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 function handleNegotiationNeededEvent () { myPeerConnection . createOffer (). then ( function ( offer ) { return myPeerConnection . setLocalDescription ( offer ); }) . then ( function () { sendToServer ({ name : myUsername , target : targetUsername , type : \"video-offer\" , sdp : myPeerConnection . localDescription }); }) . catch ( reportError ); } To start the negotiation process, we need to create and send an SDP offer to the peer we want to connect to. This offer includes a list of supported configurations for the connection, including information about the media stream we've added to the connection locally, and any ICE candidates gathered by the ICE layer already. We create this offer by calling myPeerConnection.createOffer() . When createOffer() succeeds, we pass the created offer information into myPeerConnection.setLocalDescription() , which configures the connection and media configuration state for the caller's end of the connection. We know the description is valid, and has been set, this is when we send our offer to the other peer by creating a new \"video-offer\" message containing the local description, then sending it through our signaling server to the callee.","title":"Starting negotiation"},{"location":"web/webrtc/example/#handling-the-invitation","text":"When the offer arrives, the callee's handleVideoOfferMsg() function is called with the \"video-offer\" message that was received 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function handleVideoOfferMsg ( msg ) { var localStream = null ; targetUsername = msg . name ; createPeerConnection (); var desc = new RTCSessionDescription ( msg . sdp ); myPeerConnection . setRemoteDescription ( desc ). then ( function () { return navigator . mediaDevices . getUserMedia ( mediaConstraints ); }) . then ( function ( stream ) { localStream = stream ; document . getElementById ( \"local_video\" ). srcObject = localStream ; localStream . getTracks (). forEach ( track => myPeerConnection . addTrack ( track , localStream )); }) . then ( function () { return myPeerConnection . createAnswer (); }) . then ( function ( answer ) { return myPeerConnection . setLocalDescription ( answer ); }) . then ( function () { var msg = { name : myUsername , target : targetUsername , type : \"video-answer\" , sdp : myPeerConnection . localDescription }; sendToServer ( msg ); }) . catch ( handleGetUserMediaError ); } This code is very similar to what we did in the invite() function. It starts by creating and configuring an RTCPeerConnection using our createPeerConnection() function. Then it takes the SDP offer from the received \"video-offer\" message and uses it to create a new RTCSessionDescription object representing the caller's session description. That session description is then passed into myPeerConnection.setRemoteDescription(). This establishes the received offer as the description of the remote (caller's) end of the connection. Once the answer has been created using myPeerConnection.createAnswer(), the description of the local end of the connection is set to the answer's SDP by calling myPeerConnection.setLocalDescription(), then the answer is transmitted through the signaling server to the caller to let them know what the answer is.","title":"Handling the invitation"},{"location":"web/webrtc/example/#sending-ice-candidates","text":"The ICE negotiation process involves each peer sending candidates to the other, repeatedly, until it runs out of potential ways it can support the RTCPeerConnection 's media transport needs. Once setLocalDescription() 's fulfillment handler has run, the ICE agent begins sending icecandidate events to the RTCPeerConnection one for each potential configuration it discovers. 1 2 3 4 5 6 7 8 9 10 11 12 function handleICECandidateEvent ( event ) { if ( ! e . candidate ) { // All candidates have been sent return } sendToServer ({ type : \"new-ice-candidate\" , target : targetUsername , candidate : event . candidate }); }","title":"Sending ICE candidates"},{"location":"web/webrtc/example/#receiving-ice-candidates","text":"1 2 3 4 5 6 function handleNewICECandidateMsg ( msg ) { var candidate = new RTCIceCandidate ( msg . candidate ); myPeerConnection . addIceCandidate ( candidate ) . catch ( reportError ); } This function constructs an RTCIceCandidate object by passing the received SDP into its constructor, then delivers the candidate to the ICE layer by passing it into myPeerConnection.addIceCandidate() .","title":"Receiving ICE candidates"},{"location":"web/webrtc/example/#receiving-new-streams","text":"When new tracks are added to the RTCPeerConnection \u2014 either by calling its addTrack() method or because of renegotiation of the stream's format\u2014a track event is set to the RTCPeerConnection for each track added to the connection. 1 2 3 4 function handleTrackEvent ( event ) { document . getElementById ( \"received_video\" ). srcObject = event . streams [ 0 ]; document . getElementById ( \"hangup-button\" ). disabled = false ; } The incoming stream is attached to the \"received_video\" <video> element.","title":"Receiving new streams"},{"location":"web/webrtc/example/#handling-the-removal-of-tracks","text":"Your code receives a removetrack event when the remote peer removes a track from the connection by calling RTCPeerConnection.removeTrack(). 1 2 3 4 5 6 7 8 function handleRemoveTrackEvent ( event ) { var stream = document . getElementById ( \"received_video\" ). srcObject ; var trackList = stream . getTracks (); if ( trackList . length == 0 ) { closeVideoCall (); } }","title":"Handling the removal of tracks"},{"location":"web/webrtc/example/#ending-the-call","text":"When the user clicks the \"Hang Up\" button to end the call, the hangUpCall() function is called: 1 2 3 4 5 6 7 8 function hangUpCall () { closeVideoCall (); sendToServer ({ name : myUsername , target : targetUsername , type : \"hang-up\" }); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 function closeVideoCall () { var remoteVideo = document . getElementById ( \"received_video\" ); var localVideo = document . getElementById ( \"local_video\" ); if ( myPeerConnection ) { myPeerConnection . ontrack = null ; myPeerConnection . onremovetrack = null ; myPeerConnection . onremovestream = null ; myPeerConnection . onicecandidate = null ; myPeerConnection . oniceconnectionstatechange = null ; myPeerConnection . onsignalingstatechange = null ; myPeerConnection . onicegatheringstatechange = null ; myPeerConnection . onnegotiationneeded = null ; if ( remoteVideo . srcObject ) { remoteVideo . srcObject . getTracks (). forEach ( track => track . stop ()); } if ( localVideo . srcObject ) { localVideo . srcObject . getTracks (). forEach ( track => track . stop ()); } myPeerConnection . close (); myPeerConnection = null ; } remoteVideo . removeAttribute ( \"src\" ); remoteVideo . removeAttribute ( \"srcObject\" ); localVideo . removeAttribute ( \"src\" ); remoteVideo . removeAttribute ( \"srcObject\" ); document . getElementById ( \"hangup-button\" ). disabled = true ; targetUsername = null ; }","title":"Ending the call"},{"location":"web/webrtc/example/#dealing-with-state-changes","text":"","title":"Dealing with state changes"},{"location":"web/webrtc/example/#ice-connection-state","text":"iceconnectionstatechange events are sent to the RTCPeerConnection by the ICE layer when the connection state changes (such as when the call is terminated from the other end). 1 2 3 4 5 6 7 8 function handleICEConnectionStateChangeEvent ( event ) { switch ( myPeerConnection . iceConnectionState ) { case \"closed\" : case \"failed\" : closeVideoCall (); break ; } }","title":"ICE connection state"},{"location":"web/webrtc/example/#ice-signaling-state","text":"1 2 3 4 5 6 7 function handleSignalingStateChangeEvent ( event ) { switch ( myPeerConnection . signalingState ) { case \"closed\" : closeVideoCall (); break ; } };","title":"ICE signaling state"},{"location":"web/webrtc/example/#ice-gathering-state","text":"icegatheringstatechange events are used to let you know when the ICE candidate gathering process state changes 1 2 3 4 function handleICEGatheringStateChangeEvent ( event ) { // Our sample just logs information to console here, // but you can do whatever you need. }","title":"ICE gathering state"},{"location":"web/webrtc/media_communication/","text":"Media communication WebRTC uses RTP and RTCP for media communication. 1. RTP The Real-time Transport Protocol(RTP) is a network protocol used to deliver streaming audio and video media over the internet. 2. RTCP RTCP stands for Real-time Transport Control Protocol. RTCP works hand in hand with RTP. RTP does the delivery of the actual data, whereas RTCP is used to send control packets to participants in a call. The primary function is to provide feedback on the quality of service being provided by RTP. Every RTCP packet has the following structure: 1 2 3 4 5 6 7 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |V=2|P| RC | PT | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Version (V) Version is always 2 Padding (P) Padding is a bool that controls if the payload has padding. The last byte of the payload contains a count of how many padding bytes was added Reception Report Count (RC) The number of reports in this packet. A single RTCP packet can contain multiple events. Packet Type (PT) Unique Identifier for what type of RTCP Packet this is. 192 Full INTRA-frame Request (FIR) 193 Negative ACKnowledgements (NACK) 200 Sender Report 201 Receiver Report 205 Generic RTP Feedback 206 Payload Specific Feedback Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI) Both FIR and PLI messages serve a similar purpose. These messages request a full key frame from the sender PLI is used when partial frames were given to the decoder, but it was unable to decode them. This could happen because you had lots of packet loss, or maybe the decoder crashed. FIR shall not be used when packets or frames are lost. That is PLIs job. FIR requests a key frame for reasons other than packet loss - for example when a new member enters a video conference. They need a full key frame to start decoding video stream, the decoder will be discarding frames until key frame arrives. Negative Acknowledgment A NACK requests that a sender re-transmits a single RTP packet. This is usually caused by an RTP packet getting lost, but could also happen because it is late. Sender and Receiver Reports These reports are used to send statistics between agents. This communicates the amount of packets actually received and jitter. The reports can be used for diagnostics and congestion control.","title":"Media communication"},{"location":"web/webrtc/media_communication/#media-communication","text":"WebRTC uses RTP and RTCP for media communication.","title":"Media communication"},{"location":"web/webrtc/media_communication/#1-rtp","text":"The Real-time Transport Protocol(RTP) is a network protocol used to deliver streaming audio and video media over the internet.","title":"1. RTP"},{"location":"web/webrtc/media_communication/#2-rtcp","text":"RTCP stands for Real-time Transport Control Protocol. RTCP works hand in hand with RTP. RTP does the delivery of the actual data, whereas RTCP is used to send control packets to participants in a call. The primary function is to provide feedback on the quality of service being provided by RTP. Every RTCP packet has the following structure: 1 2 3 4 5 6 7 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |V=2|P| RC | PT | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Payload | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Version (V) Version is always 2 Padding (P) Padding is a bool that controls if the payload has padding. The last byte of the payload contains a count of how many padding bytes was added Reception Report Count (RC) The number of reports in this packet. A single RTCP packet can contain multiple events. Packet Type (PT) Unique Identifier for what type of RTCP Packet this is. 192 Full INTRA-frame Request (FIR) 193 Negative ACKnowledgements (NACK) 200 Sender Report 201 Receiver Report 205 Generic RTP Feedback 206 Payload Specific Feedback Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI) Both FIR and PLI messages serve a similar purpose. These messages request a full key frame from the sender PLI is used when partial frames were given to the decoder, but it was unable to decode them. This could happen because you had lots of packet loss, or maybe the decoder crashed. FIR shall not be used when packets or frames are lost. That is PLIs job. FIR requests a key frame for reasons other than packet loss - for example when a new member enters a video conference. They need a full key frame to start decoding video stream, the decoder will be discarding frames until key frame arrives. Negative Acknowledgment A NACK requests that a sender re-transmits a single RTP packet. This is usually caused by an RTP packet getting lost, but could also happen because it is late. Sender and Receiver Reports These reports are used to send statistics between agents. This communicates the amount of packets actually received and jitter. The reports can be used for diagnostics and congestion control.","title":"2. RTCP"},{"location":"web/webrtc/securing/","text":"Securing 1. DTLS DTLS(Datagram Transport Layer Security) is a variant of the TLS protocol that runs over UDP instead of TCP. In WebRTC it is used as a way to generate symmetric encryption keys for use in SRTP as well as encyption of the datachannel. The DTLS handshake takes place after ICE has found a path through the NAT maze. DTLS handshake ClientHello ClientHello is the initial message sent by the client. It contains a list of attributes that tell the server the ciphers and features the client supports. HelloVerifyRequest HelloVerifyRequest is sent by the server to the client. It is to make sure that the client intended to send the request. The Client then re-sends the ClientHello, but with a token provided in the HelloVerifyRequest. ServerHello ServerHello is the response by the server for the configuration of this session. It contains what cipher will be used when this session is over. It also contains the server random data. Certificate Certificate contains the certificate for the Client or Server. After the handshake is over we will make sure this certificate when hashed matches the fingerprint in the SessionDescription . ServerKeyExchange/ClientKeyExchange These messages are used to transmit the public key. On startup, the client and server both generate a keypair. After the handshake these values will be used to generate the Pre-Master Secret. CertificateRequest A CertificateRequest is sent by the server notifying the client that it wants a certificate. ServerHelloDone ServerHelloDone notifies the client that the server is done with the handshake. CertificateVerify CertificateVerify is how the sender proves that it has the private key sent in the Certificate message. ChangeCipherSpec ChangeCipherSpec informs the receiver that everything sent after this message will be encrypted. Finished Finished is encrypted and contains a hash of all messages. This is to assert that the handshake was not tampered with. 2. SRTP SRTP is a protocol designed specifically for encrypting RTP packets. To start an SRTP session you specify your keys and cipher. Unlike DTLS it has no handshake mechanism. All the configuration and keys were generated during the DTLS handshake.","title":"Securing"},{"location":"web/webrtc/securing/#securing","text":"","title":"Securing"},{"location":"web/webrtc/securing/#1-dtls","text":"DTLS(Datagram Transport Layer Security) is a variant of the TLS protocol that runs over UDP instead of TCP. In WebRTC it is used as a way to generate symmetric encryption keys for use in SRTP as well as encyption of the datachannel. The DTLS handshake takes place after ICE has found a path through the NAT maze.","title":"1. DTLS"},{"location":"web/webrtc/securing/#dtls-handshake","text":"ClientHello ClientHello is the initial message sent by the client. It contains a list of attributes that tell the server the ciphers and features the client supports. HelloVerifyRequest HelloVerifyRequest is sent by the server to the client. It is to make sure that the client intended to send the request. The Client then re-sends the ClientHello, but with a token provided in the HelloVerifyRequest. ServerHello ServerHello is the response by the server for the configuration of this session. It contains what cipher will be used when this session is over. It also contains the server random data. Certificate Certificate contains the certificate for the Client or Server. After the handshake is over we will make sure this certificate when hashed matches the fingerprint in the SessionDescription . ServerKeyExchange/ClientKeyExchange These messages are used to transmit the public key. On startup, the client and server both generate a keypair. After the handshake these values will be used to generate the Pre-Master Secret. CertificateRequest A CertificateRequest is sent by the server notifying the client that it wants a certificate. ServerHelloDone ServerHelloDone notifies the client that the server is done with the handshake. CertificateVerify CertificateVerify is how the sender proves that it has the private key sent in the Certificate message. ChangeCipherSpec ChangeCipherSpec informs the receiver that everything sent after this message will be encrypted. Finished Finished is encrypted and contains a hash of all messages. This is to assert that the handshake was not tampered with.","title":"DTLS handshake"},{"location":"web/webrtc/securing/#2-srtp","text":"SRTP is a protocol designed specifically for encrypting RTP packets. To start an SRTP session you specify your keys and cipher. Unlike DTLS it has no handshake mechanism. All the configuration and keys were generated during the DTLS handshake.","title":"2. SRTP"},{"location":"web/webrtc/signaling/","text":"Signaling When a WebRTC Agent starts it has no idea who it is going to communicate with and what they are going to communicate about. Signaling is used to bootstrap the call so that two WebRTC agents can start communicating. Signaling messages are just text. The WebRTC agents don\u2019t care how they are transported. They are commonly shared via Websockets, but that is not a requirement. 1. How does WebRTC signaling work? WebRTC uses an existing protocol called the Session Description Protocol. Via this protocol, the two WebRTC Agents will share all the state required to establish a connection. 2. What is the Session Description Protocol (SDP) The SDP is a key/value protocol wiht a newline after each value. A SDP contains zero or more Media Descriptions. A Media Description usually maps to a single stream of media. So if you wanted to describe a call with three video streams and two audio tracks you would have five Media Descriptions. 2.2. How to read the SDP Every line in a SDP will start with a single character, this is your key. The SDP defines all they keys that are valid. You can ONLY use letters for keys as defined in the protocol. Eg: 1 2 a=my-sdp-value a=second-value SDP keys is using in WebRTC: v version, should be equal to 0 o origin, contains a unique ID useful for renegotiations s session name, should be equal to - t timing, should be equal to 0 0 m media description ( m=<media> <port> <proto> <fmt> ... ) a attribute, a free text field c connection data, shoudl be equal to IN IP4 0.0.0.0 Media Descriptions in SDP: A Media Description definition contains list of formats. These formats map to RTP Payload Types. The actual codec is then defined by an Attribute with the value rtpmap in the Media Description. Eg: 1 2 3 4 5 6 7 8 9 v=0 o=- 0 0 IN IP4 127.0.0.1 s=- c=IN IP4 127.0.0.1 t=0 0 m=audio 4000 RTP/AVP 111 a=rtpmap:111 OPUS/48000/2 m=video 4002 RTP/AVP 96 a=rtpmap:96 VP8/90000 In the above example, we have 2 Media Descriptions: Audio format in port 4000 , proto RTP/AVP , fmt 111 . The attribute maps the Payload Type 111 to Opus Video format in port 4002 , proto RTP/AVP , fmt 96 . The attribute maps the Payload Type 96 to VP8 Note : In the above example v , o , s , c , t are defined, but they do not affect the WebRTC session. 3. How SDP and WebRTC work together 3.1 What are offers and answers WebRTC uses an offer/answer model. All this means is that one WebRTC Agent makes an \u201cOffer\u201d to start a call, and the other WebRTC Agents \u201cAnswers\u201d if it is willing to accept what has been offered. This gives the answerer a chance to reject unsupported codecs in the Media Descriptions. This is how two peers can understand what formats they are willing to exchange. 3.2 Transceivers are for sending and receiving Transceivers is a WebRTC specific concept that you will see in the API. What it is doing is exposing the \u201cMedia Description\u201d to the JavaScript API. Each Media Description becomes a Transceiver. Every time you create a Transceiver a new Media Description is added to the local Session Description. Each Media Description in WebRTC will have a direction attribute. There are four valid values: send recv sendrecv inactive 3.3 SDP Values used by WebRTC group:BUNDLE Bundling is an act of running multiple types of traffic over one connection. Some WebRTC implementations use a dedicated connection per media stream. Bundling should be preferred. fingerprint:sha-256 This is a hash of the certificate a peer is using for DTLS. After the DTLS handshake is completed, you compare this to the actual certificate to confirm you are communicating with whom you expect. setup: This controls the DTLS Agent behavior. This determines if it runs as a client or server after ICE has connected. The possible values are: setup:active - Run as DTLS Client setup:passive - Run as DTLS Server setup:actpass - Ask the other WebRTC Agent to choose ice-ufrag This is the user fragment value for the ICE Agent. Used for the authentication of ICE Traffic. ice-pwd This is the password for the ICE Agent. Used for authentication of ICE Traffic. rtpmap This value is used to map a specific codec to an RTP Payload Type. fmtp Defines additional values for one Payload Type. This is useful to communicate a specific video profile or encoder setting. candidate This is an ICE Candidate that comes from the ICE Agent. This is one possible address that the WebRTC Agent is available on ssrc A Synchronization Source (SSRC) defines a single media stream track. label is the ID for this individual stream. mslabel is the ID for a container that can have multiple streams inside it. 3.4 Example of a WebRTC Session Description 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 v=0 o=- 3546004397921447048 1596742744 IN IP4 0.0.0.0 s=- t=0 0 a=fingerprint:sha-256 0F:74:31:25:CB:A2:13:EC:28:6F:6D:2C:61:FF:5D:C2:BC:B9:DB:3D:98:14:8D:1A:BB:EA:33:0C:A4:60:A8:8E a=group:BUNDLE 0 1 m=audio 9 UDP/TLS/RTP/SAVPF 111 c=IN IP4 0.0.0.0 a=setup:active a=mid:0 a=ice-ufrag:CsxzEWmoKpJyscFj a=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd a=rtcp-mux a=rtcp-rsize a=rtpmap:111 opus/48000/2 a=fmtp:111 minptime=10;useinbandfec=1 a=ssrc:350842737 cname:yvKPspsHcYcwGFTw a=ssrc:350842737 msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV a=ssrc:350842737 mslabel:yvKPspsHcYcwGFTw a=ssrc:350842737 label:DfQnKjQQuwceLFdV a=msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV a=sendrecv a=candidate:foundation 1 udp 2130706431 192.168.1.1 53165 typ host generation 0 a=candidate:foundation 2 udp 2130706431 192.168.1.1 53165 typ host generation 0 a=candidate:foundation 1 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0 a=candidate:foundation 2 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0 a=end-of-candidates m=video 9 UDP/TLS/RTP/SAVPF 96 c=IN IP4 0.0.0.0 a=setup:active a=mid:1 a=ice-ufrag:CsxzEWmoKpJyscFj a=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd a=rtcp-mux a=rtcp-rsize a=rtpmap:96 VP8/90000 a=ssrc:2180035812 cname:XHbOTNRFnLtesHwJ a=ssrc:2180035812 msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW a=ssrc:2180035812 mslabel:XHbOTNRFnLtesHwJ a=ssrc:2180035812 label:JgtwEhBWNEiOnhuW a=msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW a=sendrecv This is what we know from the above message: We have two media sections, one audio and one video. Both of them are sendrecv transceivers. We are getting two streams, and we can send two back. We have ICE Candidates and Authentication details, so we can attempt to connect. We have a certificate fingerprint, so we can have a secure call.","title":"Signaling"},{"location":"web/webrtc/signaling/#signaling","text":"When a WebRTC Agent starts it has no idea who it is going to communicate with and what they are going to communicate about. Signaling is used to bootstrap the call so that two WebRTC agents can start communicating. Signaling messages are just text. The WebRTC agents don\u2019t care how they are transported. They are commonly shared via Websockets, but that is not a requirement.","title":"Signaling"},{"location":"web/webrtc/signaling/#1-how-does-webrtc-signaling-work","text":"WebRTC uses an existing protocol called the Session Description Protocol. Via this protocol, the two WebRTC Agents will share all the state required to establish a connection.","title":"1. How does WebRTC signaling work?"},{"location":"web/webrtc/signaling/#2-what-is-the-session-description-protocol-sdp","text":"The SDP is a key/value protocol wiht a newline after each value. A SDP contains zero or more Media Descriptions. A Media Description usually maps to a single stream of media. So if you wanted to describe a call with three video streams and two audio tracks you would have five Media Descriptions.","title":"2. What is the Session Description Protocol (SDP)"},{"location":"web/webrtc/signaling/#22-how-to-read-the-sdp","text":"Every line in a SDP will start with a single character, this is your key. The SDP defines all they keys that are valid. You can ONLY use letters for keys as defined in the protocol. Eg: 1 2 a=my-sdp-value a=second-value SDP keys is using in WebRTC: v version, should be equal to 0 o origin, contains a unique ID useful for renegotiations s session name, should be equal to - t timing, should be equal to 0 0 m media description ( m=<media> <port> <proto> <fmt> ... ) a attribute, a free text field c connection data, shoudl be equal to IN IP4 0.0.0.0 Media Descriptions in SDP: A Media Description definition contains list of formats. These formats map to RTP Payload Types. The actual codec is then defined by an Attribute with the value rtpmap in the Media Description. Eg: 1 2 3 4 5 6 7 8 9 v=0 o=- 0 0 IN IP4 127.0.0.1 s=- c=IN IP4 127.0.0.1 t=0 0 m=audio 4000 RTP/AVP 111 a=rtpmap:111 OPUS/48000/2 m=video 4002 RTP/AVP 96 a=rtpmap:96 VP8/90000 In the above example, we have 2 Media Descriptions: Audio format in port 4000 , proto RTP/AVP , fmt 111 . The attribute maps the Payload Type 111 to Opus Video format in port 4002 , proto RTP/AVP , fmt 96 . The attribute maps the Payload Type 96 to VP8 Note : In the above example v , o , s , c , t are defined, but they do not affect the WebRTC session.","title":"2.2. How to read the SDP"},{"location":"web/webrtc/signaling/#3-how-sdp-and-webrtc-work-together","text":"","title":"3. How SDP and WebRTC work together"},{"location":"web/webrtc/signaling/#31-what-are-offers-and-answers","text":"WebRTC uses an offer/answer model. All this means is that one WebRTC Agent makes an \u201cOffer\u201d to start a call, and the other WebRTC Agents \u201cAnswers\u201d if it is willing to accept what has been offered. This gives the answerer a chance to reject unsupported codecs in the Media Descriptions. This is how two peers can understand what formats they are willing to exchange.","title":"3.1 What are offers and answers"},{"location":"web/webrtc/signaling/#32-transceivers-are-for-sending-and-receiving","text":"Transceivers is a WebRTC specific concept that you will see in the API. What it is doing is exposing the \u201cMedia Description\u201d to the JavaScript API. Each Media Description becomes a Transceiver. Every time you create a Transceiver a new Media Description is added to the local Session Description. Each Media Description in WebRTC will have a direction attribute. There are four valid values: send recv sendrecv inactive","title":"3.2 Transceivers are for sending and receiving"},{"location":"web/webrtc/signaling/#33-sdp-values-used-by-webrtc","text":"group:BUNDLE Bundling is an act of running multiple types of traffic over one connection. Some WebRTC implementations use a dedicated connection per media stream. Bundling should be preferred. fingerprint:sha-256 This is a hash of the certificate a peer is using for DTLS. After the DTLS handshake is completed, you compare this to the actual certificate to confirm you are communicating with whom you expect. setup: This controls the DTLS Agent behavior. This determines if it runs as a client or server after ICE has connected. The possible values are: setup:active - Run as DTLS Client setup:passive - Run as DTLS Server setup:actpass - Ask the other WebRTC Agent to choose ice-ufrag This is the user fragment value for the ICE Agent. Used for the authentication of ICE Traffic. ice-pwd This is the password for the ICE Agent. Used for authentication of ICE Traffic. rtpmap This value is used to map a specific codec to an RTP Payload Type. fmtp Defines additional values for one Payload Type. This is useful to communicate a specific video profile or encoder setting. candidate This is an ICE Candidate that comes from the ICE Agent. This is one possible address that the WebRTC Agent is available on ssrc A Synchronization Source (SSRC) defines a single media stream track. label is the ID for this individual stream. mslabel is the ID for a container that can have multiple streams inside it.","title":"3.3 SDP Values used by WebRTC"},{"location":"web/webrtc/signaling/#34-example-of-a-webrtc-session-description","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 v=0 o=- 3546004397921447048 1596742744 IN IP4 0.0.0.0 s=- t=0 0 a=fingerprint:sha-256 0F:74:31:25:CB:A2:13:EC:28:6F:6D:2C:61:FF:5D:C2:BC:B9:DB:3D:98:14:8D:1A:BB:EA:33:0C:A4:60:A8:8E a=group:BUNDLE 0 1 m=audio 9 UDP/TLS/RTP/SAVPF 111 c=IN IP4 0.0.0.0 a=setup:active a=mid:0 a=ice-ufrag:CsxzEWmoKpJyscFj a=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd a=rtcp-mux a=rtcp-rsize a=rtpmap:111 opus/48000/2 a=fmtp:111 minptime=10;useinbandfec=1 a=ssrc:350842737 cname:yvKPspsHcYcwGFTw a=ssrc:350842737 msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV a=ssrc:350842737 mslabel:yvKPspsHcYcwGFTw a=ssrc:350842737 label:DfQnKjQQuwceLFdV a=msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV a=sendrecv a=candidate:foundation 1 udp 2130706431 192.168.1.1 53165 typ host generation 0 a=candidate:foundation 2 udp 2130706431 192.168.1.1 53165 typ host generation 0 a=candidate:foundation 1 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0 a=candidate:foundation 2 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0 a=end-of-candidates m=video 9 UDP/TLS/RTP/SAVPF 96 c=IN IP4 0.0.0.0 a=setup:active a=mid:1 a=ice-ufrag:CsxzEWmoKpJyscFj a=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd a=rtcp-mux a=rtcp-rsize a=rtpmap:96 VP8/90000 a=ssrc:2180035812 cname:XHbOTNRFnLtesHwJ a=ssrc:2180035812 msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW a=ssrc:2180035812 mslabel:XHbOTNRFnLtesHwJ a=ssrc:2180035812 label:JgtwEhBWNEiOnhuW a=msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW a=sendrecv This is what we know from the above message: We have two media sections, one audio and one video. Both of them are sendrecv transceivers. We are getting two streams, and we can send two back. We have ICE Candidates and Authentication details, so we can attempt to connect. We have a certificate fingerprint, so we can have a secure call.","title":"3.4 Example of a WebRTC Session Description"},{"location":"web/webrtc/webrtc/","text":"WebRTC In WebRTC we have 4 steps: Signaling Connecting Securing Communicating These four steps happen sequentially. The prior step must be 100% successful for the subsequent one to even begin. WebRTC Topologies One-To-One One-to-One is the first connection type you will use with WebRTC. You connect two WebRTC Agents directly and they can send bi-directional media and data. Full Mesh Full mesh is the answer if you want to build a conference call or a multiplayer game. In this topology each user establishes a connection with every other user directly In a Full Mesh topology each user is connected directly. That means you have to encode and upload video independently for each member of the call. The network conditions between each connection will be different, so you can\u2019t reuse the same video. Hybrid Mesh Hybrid Mesh is an alternative to Full Mesh that can alleviate some of the Full Mesh\u2019s issues. In a Hybrid Mesh connections aren\u2019t established between every user. Instead, media is relayed through peers in the network. This means that the creator of the media doesn\u2019t have to use as much bandwidth to distribute media. This does have some downsides. In this set up, the original creator of the media has no idea who its video is being sent too, and if it arrived successfully. You also will have an increase in latency with every hop in your Hybrid Mesh network. MCU The MCU architecture assumes that each conference participant sends his or her stream to the MCU. The MCU decodes each received stream, rescales it, composes a new stream from all received streams, encodes it, and sends a single to all other participants. The main disadvantage of MCU is its cost, as It decodes and re-encodes streams to compose the final stream, hence requires significant computing power by the MCU. Selective Forwarding Unit Selective Forwarding Units (SFUs) are the most popular modern approach. In the SFU architecture, every participant sends his or her media stream to a centralized server (SFU) and receives streams from all other participants via the same central server. The architecture allows the call participant to send multiple media streams to the SFU, where the SFU may decide which of the media streams should be forwarded to the other call participants. Unlike in the MCU architecture, the SFU does not need to decode and re-encode received streams, but simply acts as a forwarder of streams between call participants. The device endpoints need to be more intelligent and have more computing power than in the MCU architecture.","title":"WebRTC"},{"location":"web/webrtc/webrtc/#webrtc","text":"In WebRTC we have 4 steps: Signaling Connecting Securing Communicating These four steps happen sequentially. The prior step must be 100% successful for the subsequent one to even begin.","title":"WebRTC"},{"location":"web/webrtc/webrtc/#webrtc-topologies","text":"","title":"WebRTC Topologies"},{"location":"web/webrtc/webrtc/#one-to-one","text":"One-to-One is the first connection type you will use with WebRTC. You connect two WebRTC Agents directly and they can send bi-directional media and data.","title":"One-To-One"},{"location":"web/webrtc/webrtc/#full-mesh","text":"Full mesh is the answer if you want to build a conference call or a multiplayer game. In this topology each user establishes a connection with every other user directly In a Full Mesh topology each user is connected directly. That means you have to encode and upload video independently for each member of the call. The network conditions between each connection will be different, so you can\u2019t reuse the same video.","title":"Full Mesh"},{"location":"web/webrtc/webrtc/#hybrid-mesh","text":"Hybrid Mesh is an alternative to Full Mesh that can alleviate some of the Full Mesh\u2019s issues. In a Hybrid Mesh connections aren\u2019t established between every user. Instead, media is relayed through peers in the network. This means that the creator of the media doesn\u2019t have to use as much bandwidth to distribute media. This does have some downsides. In this set up, the original creator of the media has no idea who its video is being sent too, and if it arrived successfully. You also will have an increase in latency with every hop in your Hybrid Mesh network.","title":"Hybrid Mesh"},{"location":"web/webrtc/webrtc/#mcu","text":"The MCU architecture assumes that each conference participant sends his or her stream to the MCU. The MCU decodes each received stream, rescales it, composes a new stream from all received streams, encodes it, and sends a single to all other participants. The main disadvantage of MCU is its cost, as It decodes and re-encodes streams to compose the final stream, hence requires significant computing power by the MCU.","title":"MCU"},{"location":"web/webrtc/webrtc/#selective-forwarding-unit","text":"Selective Forwarding Units (SFUs) are the most popular modern approach. In the SFU architecture, every participant sends his or her media stream to a centralized server (SFU) and receives streams from all other participants via the same central server. The architecture allows the call participant to send multiple media streams to the SFU, where the SFU may decide which of the media streams should be forwarded to the other call participants. Unlike in the MCU architecture, the SFU does not need to decode and re-encode received streams, but simply acts as a forwarder of streams between call participants. The device endpoints need to be more intelligent and have more computing power than in the MCU architecture.","title":"Selective Forwarding Unit"}]}